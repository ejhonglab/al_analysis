#!/usr/bin/env python3
"""
Wrappers to facilitate running `olfsysm` MB models, mainly:
- `model_mb_responses`: highest-level function. takes multiple-fly ORN dF/F data and
  runs multiple parameterizations of the model (as well as saving plots and other
  outputs to subdirs).

- `fit_and_plot_mb_model`: takes mean ORN data (already scaled into units of spike rate
  deltas), and runs one parameterization of the model (making multiple `fit_mb_model`
  calls only when multiple seeds are needed), saving plots/etc in a created directory

- `fit_mb_model`: the loosest wrapper around `olfsysm`. returns model outputs, but
  typically does not make any plots (though there is some debug code for that).
  when needed, it:
  - fills glomeruli to intersection of those in hemibrain and Task et al. 2022
  - imputes mean Hallem SFR

See also docstring for `connectome_wPNKC` below.
"""

import itertools
from os.path import getmtime
from pathlib import Path
from pprint import pprint, pformat
import re
import sys
import shutil
from tempfile import NamedTemporaryFile
import time
import traceback
from typing import Optional, Tuple, Dict, Any

import numpy as np
import pandas as pd
import xarray as xr
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib.axes import Axes
from matplotlib.ticker import MaxNLocator
from matplotlib.colors import to_rgba
import matplotlib as mpl
from scipy.stats import spearmanr, pearsonr, f_oneway
from sklearn.preprocessing import maxabs_scale as sk_maxabs_scale
from sklearn.preprocessing import minmax_scale as sk_minmax_scale
import statsmodels.api as sm
import seaborn as sns
import colorcet as cc
from tqdm import tqdm
from termcolor import cprint
# for type hinting
from statsmodels.regression.linear_model import RegressionResultsWrapper

from hong2p import olf, util, viz
from hong2p.olf import solvent_str
from hong2p.viz import dff_latex, no_constrained_layout
from hong2p.util import num_notnull, num_null, pd_allclose, format_date, date_fmt_str
from hong2p.types import Pathlike
import olfsysm as osm
from drosolf import orns

# NOTE: can't import from al_analysis w/o causing circular import issues, so need to
# factor shared stuff to al_util
from al_util import (savefig, abbrev_hallem_odor_index, sort_odors, panel2name_order,
    diag_panel_str, warn, should_ignore_existing, to_csv, to_pickle, read_pickle,
    produces_output, makedirs, corr_triangular, invert_corr_triangular, n_choose_2,
    diverging_cmap, diverging_cmap_kwargs, bootstrap_seed, mean_of_fly_corrs, plot_corr,
    plot_responses_and_corr, rotate_xticklabels, cluster_rois, odor_is_megamat,
    megamat_odor_names, remy_data_dir, remy_binary_response_dir, remy_megamat_sparsity,
    remy_date_col, remy_fly_cols, remy_fly_id, remy_fly_binary_response_fname,
    load_remy_fly_binary_responses, load_remy_megamat_kc_binary_responses,
    n_final_megamat_kc_flies, MultipleSavesPerRunException
)
import al_util


# e.g. before calculating correlations across model KC populations.
#
# Remy generally DOES drop "bad" cells, which are largely silent cells, but that isn't
# controlled by this flag. my analysis of her data generally also drops the same cells
# she does.
drop_silent_model_kcs = True

def read_series_csv(csv: Pathlike, **kwargs) -> pd.Series:
    df = pd.read_csv(csv, header=None, index_col=0, **kwargs)
    assert df.shape[1] == 1
    ser = df.iloc[:, 0].copy()

    # these should both be autogenerated, and not actually in CSV
    assert ser.index.name == 0
    assert ser.name == 1
    ser.index.name = None
    ser.name = None

    return ser


# TODO refactor to share drop_multiglomerular_receptors default w/ fit_mb_model
# TODO TODO check there aren't some cases where drop_maximal_extent_rois=True (in ROI ->
# mask generation code) is leaving '+' in some ROI names (maybe if they aren't at end in
# input, e.g. if there's something like 'DM5+?' (and handle / change that code if so)
def handle_multiglomerular_receptors(df: pd.DataFrame,
    drop_multiglomerular_receptors: bool = True) -> pd.DataFrame:

    if not drop_multiglomerular_receptors:
        raise NotImplementedError

    assert df.index.name == 'glomerulus'
    # glomeruli should only contain '+' delimiter (e.g. 'DM3+DM5') if the
    # measurement was originally from a receptor that is expressed in each
    # of the glomeruli in the string.
    multiglomerular_receptors = df.index.str.contains('+', regex=False)
    if multiglomerular_receptors.any():
        # TODO warn? at least is some verbose flag is True?
        df = df.loc[~multiglomerular_receptors, :].copy()

    return df


# TODO add 'hemibrain-matt' here, and replace _use_matt_wPNKC w/ that?
# NOTE: FAFB = FlyWire
connectome_options = {'hemibrain', 'fafb-left', 'fafb-right'}

_connectome2glomset = dict()
# TODO compare to what ann had been using (does she have her own fully formed connectome
# based PN->KC matrix, or is it all random draws with certain connectome inspired
# probabilities?)
# TODO TODO add parameter for thresholding PN-KC pairs above the existing cutoffs of >=4
# (which was enforced by neuprint query that produced the hemibrain data we have, or we
# do manually here for the fafb datasets) (-> try to get total # of claws closer to
# reported ~5.[2-6?] mean claws in Davi Bock paper, through varying both this and
# weight_divisor. currently weight_divisor=20 brings us way above their reported mean #
# of claws, given threshold of >4 we are stuck w/ in hemibrain case. maybe Bock # of
# claws is partially just b/c that brain is abnormal [i.e. many more KCs, etc]?)
def connectome_wPNKC(connectome: str = 'hemibrain',
    weight_divisor: Optional[float] = None, plot_dir: Optional[Path] = None,
    _use_matt_wPNKC=False, _drop_glom_with_plus: bool = True) -> pd.DataFrame:
    """
    Args:
        connectome: which connectome of hemibrain/fafb-left/fafb-right to use.
            should be one of `connectome_options`.

        weight_divisor: if None, one model claw is added for each PN-KC pair exceeding
            minimum total number of unitary synapses (currently de facto >=4 for
            hemibrain, b/c of query that pulled hemibrain data, and also hardcoded to
            that in this function for FAFB datasets).

            If float, each PN-KC pair gets `ceil(total_synapses / weight_divisor)`
            claws.

            Note that glomeruli have different numbers of cognate PNs, and that only in
            the `weight_divisor=<float>` case can there be multiple claws assigned to
            one particular PN (for a given KC).

        plot_dir: if passed, saves plots histograms of: 1) PN-KC connectome weights, and
            2) #-claws-per-model-KC (#2 depends on `weight_divisor`)

    Returns dataframe of shape (#-KCs [in selected connectome], #-glomeruli).
    """
    assert connectome in connectome_options
    if _use_matt_wPNKC:
        assert connectome == 'hemibrain'

    # TODO refactor to share w/ calling code (also defined there...) (/cache?)?
    glomerulus2receptors = orns.task_glomerulus2receptors()
    #
    task_gloms = set(glomerulus2receptors.keys())
    del glomerulus2receptors

    glomerulus_renames = {'VC3l': 'VC3', 'VC3m': 'VC3'}
    assert all(x in task_gloms for x in glomerulus_renames.values())
    # wouldn't necessarily need to be true, if we were shuffling names around, but we
    # currently aren't...
    assert not any(x in task_gloms for x in glomerulus_renames.keys())

    glomerulus_col = 'glomerulus'

    def _underscore_part(ser, i=0):
        return ser.str.split('_').apply(lambda x: x[i])

    def _first_underscore_part(ser):
        return _underscore_part(ser, i=0)

    def _add_glomerulus_col_from_hemibrain_type(df: pd.DataFrame, pn_type_col: str,
        kc_id_col: str, *, check_no_multi_underscores: bool = False) -> pd.DataFrame:

        assert glomerulus_col not in df.columns

        assert not df[kc_id_col].isna().any()
        n_kcs_initial = df[kc_id_col].nunique()
        def _kc_drop_message(n_kcs_dropped):
            return (f'(also means that {n_kcs_dropped}/{n_kcs_initial} KCs only '
                'connected to these "glomeruli" dropped)'
            )

        assert not df[pn_type_col].isna().any()
        # askprat: are there specific PN types (RHS after '_') that i should
        # categorically be dropping? (prob not. what are prefixes anyway? all lineage
        # info, or anything closer to what i actually care about?).
        # Prat: no. keep all.
        #
        # a.type should all be roughly of form: <glomerulus-str>_<PN-group>, where
        # PN-group are distributed as follows (w/ connectome='hemibrain' data):
        # adPN       6927
        # lPN        2367
        # ilPN        296
        # lvPN        262
        # l2PN1       250
        # l2PN        137
        # adPNm4       85
        # ivPN         76
        # il2PN        49
        # lPNm11D      42
        # vPN          23
        # lvPN2        10
        # l2PNm16       7
        # adPNm5        5
        # lPNm13        2
        # adPNm7        1
        # lvPN1         1
        try:
            assert (df[pn_type_col].str.count('_') == 1).all()

        except AssertionError:
            # can do this for hemibrain input, but not fafb (b/c this type column in
            # fafb comes from aligning to hemibrain types, which are better annotated,
            # and i believe this multiple-underscore cases are all from a small number
            # of instances where this alignment process is ambiguous
            if check_no_multi_underscores:
                raise

            assert (df[pn_type_col].str.count('_') >= 1).all()

            # askprat: are any of below MG PNs, or something i want to include? what
            # are these weird glomeruli names? trailing +?
            # Prat: their alignment produced duplicates. not MG PNs. he thinks 'M' is to
            # indicate multiglomerular (or at least that it goes to multiple areas,
            # perhaps including things other than glomeruli), but not easy to get info
            # on which they are, without more work. he was saying if we really cared, we
            # could use the previously defined glomerular boundaries to count and figure
            # out which.
            #
            # connectome='hemibrain' has no rows w/ multiple '_'
            #
            # connectome='fafb-left'
            # M_adPNm4,M_adPNm5                120
            # VP1m+VP2_lvPN1,VP1m+VP2_lvPN2     23
            # M_lPNm12,M_lPNm13                 20
            # M_ilPNm90,M_ilPN8t91               4
            #
            # connectome='fafb-right'
            # M_adPNm4,M_adPNm5                86
            # M_lPNm12,M_lPNm13                12
            # VP1m+VP2_lvPN1,VP1m+VP2_lvPN2    12
            # M_ilPNm90,M_ilPN8t91              5
            multi_rows = df[pn_type_col].str.count('_') >= 2
            n_multi_rows = multi_rows.sum()
            assert n_multi_rows > 0

            df_no_multi = df[~multi_rows].copy()
            n_kcs_dropped = n_kcs_initial - df_no_multi[kc_id_col].nunique()

            warn(f'{connectome=} dropping {n_multi_rows} rows w/ multiple underscores '
                'in PN type:\n' +
                df[pn_type_col][multi_rows].value_counts().to_string() + '\n' +
                _kc_drop_message(n_kcs_dropped) + '\n'
            )
            df = df_no_multi

        if _drop_glom_with_plus:
            has_plus = df[pn_type_col].str.contains('+', regex=False)
            if has_plus.any():
                # askprat: what is meaning when it finishes w/ '+', w/o that
                # being a separator between two glomerulus names?
                # Prat: (re: VP3+ does go somewhere else, but either "not dense enough
                # (in other place(s) it goes to)" or not going to somewhere in hemibrain
                # volume, but that could still be in AL...)
                #
                # what is 'Z'? (Prat: Z=SEZ. it also goes there.)
                #
                # connectome='hemibrain'
                # VP1d+VP4_l2PN1    250
                # VP3+VP1l_ivPN      76
                # VP1m+VP5_ilPN      64
                # VP5+Z_adPN         17
                # VP3+_vPN           17
                # VP1m+VP2_lvPN2     10
                # VP1m+VP2_lvPN1      1
                #
                # connectome='fafb-left'
                # VP1d+VP4_l2PN1                   285
                # VP3+VP1l_ivPN                    117
                # VP1m+VP5_ilPN                     92
                # VP5+Z_adPN                        47
                # VP1m+VP2_lvPN1,VP1m+VP2_lvPN2     23
                # VP3+_vPN                          20
                # VP1m+_lvPN                         6
                # VP1l+VP3_ilPN                      2
                # VP2+_adPN                          1
                #
                # connectome='fafb-right'
                # VP1d+VP4_l2PN1                   303
                # VP3+VP1l_ivPN                    112
                # VP1m+VP5_ilPN                    106
                # VP5+Z_adPN                        42
                # VP3+_vPN                          20
                # VP1m+VP2_lvPN1,VP1m+VP2_lvPN2     12
                # VP2+_adPN                          5
                # VP1m+_lvPN                         4
                # VP1l+VP3_ilPN                      3
                df_no_plus = df[~has_plus].copy()
                n_kcs_dropped = n_kcs_initial - df_no_plus[kc_id_col].nunique()
                warn(f'{connectome=} dropping {has_plus.sum()} rows w/ "+" in PN type:'
                    '\n' + df[pn_type_col][has_plus].value_counts().to_string() + '\n' +
                    _kc_drop_message(n_kcs_dropped) + '\n'
                )
                df = df_no_plus
            else:
                warn(f"not dropping glomeruli with '+' in their name, because "
                    f'{_drop_glom_with_plus=}. this preserves old hemibrain model '
                    'behavior exactly, but should probably be removed moving forward.'
                )

        # TODO also print count of unique PN bodyids within each glom_strs value?
        # (here might not be the place anymore, if i even still care about this...)
        glom_strs = _first_underscore_part(df[pn_type_col])

        # only doing if _drop_glom_with_plus, so i don't have to also add that flag to
        # dict key (cause it will change set of glomeruli, e.g. w/ hemibrain)
        if _drop_glom_with_plus:
            # (ran after end of first loop over model_kw_list, in model_mb_responses)
            # ipdb> {k: len(v) for k, v in _connectome2glomset.items()}
            # {'fafb-left': 57, 'fafb-right': 56, 'hemibrain': 56}
            # ipdb> sh = _connectome2glomset['hemibrain']
            # ipdb> sl = _connectome2glomset['fafb-left']
            # ipdb> sr = _connectome2glomset['fafb-right']
            #
            # ipdb> sl - sr
            # {'MZ'}
            # ipdb> sr - sl
            # set()
            #
            # ipdb> sr == sh
            # True
            glom_str_set = set(glom_strs)
            if connectome not in _connectome2glomset:
                _connectome2glomset[connectome] = glom_str_set
            else:
                assert _connectome2glomset[connectome] == glom_str_set

        # glom_strs.value_counts() (w/ connectome='hemibrain' data):
        # DP1m         481
        # DM1          377
        # DC1          342
        # DL1          327
        # DM2          312
        # VM5d         310
        # DP1l         309
        # VC3l         301
        # DM6          297
        # DA1          290
        # DM4          283
        # VA2          280
        # VL2p         270
        # VC3m         255
        # VP1d+VP4     250
        # VA4          237
        # VA7m         231
        # DC2          231
        # VA3          228
        # DC3          226
        # VA6          217
        # VC4          207
        # DM3          207
        # DM5          205
        # VL2a         203
        # DL2d         197
        # V            195
        # D            179
        # VA5          168
        # VC2          159
        # VM3          154
        # VM2          150
        # DA2          147
        # VC5          142
        # M            142
        # VP1m         137
        # DL2v         137
        # DL5          136
        # VM5v         133
        # VA1d         131
        # VA7l         129
        # VC1          125
        # VM7d         109
        # VM7v          99
        # VM4           98
        # VA1v          93
        # DA4l          83
        # VM1           78
        # VP3+VP1l      76
        # VP2           71
        # VP1m+VP5      64
        # DC4           55
        # VP1d          49
        # DL4           38
        # DL3           38
        # VL1           37
        # DA3           35
        # DA4m          29
        # VP3+          17
        # VP5+Z         17
        # VP1m+VP2      11
        # VP4            6
        df[glomerulus_col] = glom_strs

        return df


    if connectome == 'hemibrain':
        if _use_matt_wPNKC:
            matt_data_dir = Path('data/from_matt/hemibrain')

            # TODO which was that other CSV (that maybe derived these?) that was full
            # PN->KC connectome matrix?
            #
            # NOTE: gkc-halfmat[-wide].csv have 22 columns for glomeruli (/receptors).
            # This should be the number excluding 2a and 33b.
            gkc_wide = pd.read_csv(matt_data_dir / 'halfmat/gkc-halfmat-wide.csv')

            # TODO TODO see if above include more than hallem glomeruli (and find
            # scripts that generated these -> figure out how to regen w/ more than
            # hallem glomeruli)
            # TODO TODO process gkc_wide to have consistent glomerulus/receptor labels
            # where possible (consistent w/ what i hope to also return in random
            # connectivity cases, etc) (presumbly if it's already a subset, should be
            # possible for all of that subset?)

            # All other columns are glomerulus names.
            assert gkc_wide.columns[0] == 'bodyid'

            wPNKC = gkc_wide.set_index('bodyid', verify_integrity=True)
            wPNKC.columns.name = 'glomerulus'
            assert wPNKC.columns.isin(task_gloms).all()

            # TODO TODO where are values >1 coming from in here:
            # ipdb> mdf.w.value_counts()
            # 1    9218
            # 2     359
            # 3      15
            # 4       1
            mdf = pd.read_csv(matt_data_dir / 'glom-kc-cxns.csv')

            mdf.glom = mdf.glom.replace(glomerulus_renames)

            # NOTE: if we do this, mdf_wide.max() is only >1 for VC3 (and it's 2 there,
            # from merging VC3l and VC3m)
            #mdf.loc[mdf.w > 1, 'w'] = 1

            # TODO are all >1 weights below coming from 'w' values that are already >1
            # before this sum? set all to 1, recompute, and see? (seems so?)
            # TODO replace groupby->pivot w/ pivot_table (aggfunc='count'/'sum')?
            # seemed possible w/ pratyush input (but 'weight' input there was max 1...)
            # TODO factor out similar pivoting (w/ pivot_table) below -> share w/ here?
            mcounts = mdf.groupby(['glom', 'bodyid']).sum('w').reset_index()
            mdf_wide = mcounts.pivot(columns='glom', index='bodyid', values='w').fillna(
                0).astype(int)
            # TODO uncomment
            #del mcounts

            # TODO implement? delete?
            if weight_divisor is not None:
                import ipdb; ipdb.set_trace()
            #

            # TODO try to remove need for orns.orns + handle_multiglomerular_receptors
            # in here?
            # TODO refactor to share w/ code calling connectome_wPNKC? or get from a
            # module level const in drosolf (maybe add one)?
            hallem_orn_deltas = orns.orns(add_sfr=False, drop_sfr=False,
                columns='glomerulus').T

            hallem_glomeruli = handle_multiglomerular_receptors(hallem_orn_deltas,
                drop_multiglomerular_receptors=True
            ).index
            del hallem_orn_deltas

            mdf_wide = mdf_wide[[x for x in hallem_glomeruli if x != 'DA4m']].copy()
            del hallem_glomeruli

            mdf_wide = mdf_wide[mdf_wide.sum(axis='columns') > 0].copy()

            # TODO move creation of mdf_wide + checking against wPNKC to model_test.py /
            # similar
            assert wPNKC.columns.equals(mdf_wide.columns)
            assert set(mdf_wide.index) == set(wPNKC.index)
            mdf_wide = mdf_wide.loc[wPNKC.index].copy()
            assert mdf_wide.equals(wPNKC)
            del mdf_wide

            # from matt-hemibrain/docs/data-loading.html
            # pn_gloms <- read_csv("data/misc/pn-major-gloms.csv")
            # pn_kc_cxns <- read_csv("data/cxns/pn-kc-cxns.csv")
            # glom_kc_cxns <- pn_kc_cxns %>%
            #   filter(weight >= 3) %>%
            #   inner_join(pn_gloms, by=c("bodyid_pre" = "bodyid")) %>%
            #   group_by(major_glom, bodyid_post) %>%
            #   summarize(w = n(), .groups = "drop") %>%
            #   rename(bodyid = bodyid_post, glom = major_glom)
            # write_csv(glom_kc_cxns, "data/cxns/glom-kc-cxns.csv")

            # inspecting some of the files from above:
            # tom@atlas:~/src/matt/matt-hemibrain/data/misc$ head pn-major-gloms.csv
            # bodyid,major_glom
            # 294792184,DC1
            # 480927537,DC1
            # 541632990,DC1
            # 542311358,DC2
            # 542634818,DM1
            # ...
            # tom@atlas:~/src/matt/matt-hemibrain/data$ head cxns/pn-kc-cxns.csv
            # bodyid_pre,bodyid_post,weight,weight_hp
            # 542634818,487489028,17,9
            # 542634818,548885313,1,0
            # 542634818,549222167,1,1
            # 542634818,5813021736,6,4
            # ...
            # NOTE: pn-kc-cxns.csv above should also be what matt uses to generate
            # distribution of # claws per KC (in matt-hemibrain/docs/mb-claws.html)

            n_kcs = len(wPNKC)
        else:
            data_path = Path('data/PNtoKC_connections_raw.xlsx')

            # (looks like it was c.weight > 3 actually)
            #
            # This should be from Pratyush, generated on v1.2.1, via something like:
            # MATCH (a:Neuron)-[c.ConnectsTo]->(b:Neuron)
            # WHERE a.Instance CONTAINS "PN"
            # AND b.Instance CONTAINS "KC"
            # AND c.weight > 5
            # RETURN a.bodyId, a.Instance, a.type, b.bodyId, b.Instance, b.type, c.weight
            #
            # TODO move this one to appropriate subdir in data/from_prat -> update path
            # here
            df = pd.read_excel(data_path)

            pn_id_col = 'a.bodyId'
            kc_id_col = 'b.bodyId'
            weight_col = 'c.weight'

            hemibrain_pn_type = 'a.type'

            # TODO move this call into `not _use_matt_wPNKC` case below (to share w/
            # connectome='fafb-[left|right]' cases below)?
            df = _add_glomerulus_col_from_hemibrain_type(df, hemibrain_pn_type,
                kc_id_col, check_no_multi_underscores=True
            )
    else:
        fafb_dir = Path('data/from_pratyush/2024-09-13')

        if connectome == 'fafb-left':
            data_path = fafb_dir / 'FlyWire_PNKC_Left.csv'
        else:
            assert connectome == 'fafb-right'
            data_path = fafb_dir / 'FlyWire_PNKC_Right.csv'

        df = pd.read_csv(data_path)

        cols_with_nan = df.isna().any()
        assert set(cols_with_nan[cols_with_nan].index) == {
            'source_cell_type', 'target_cell_type'
        }

        assert (df.source_cell_class == 'ALPN').all()
        assert (df.target_cell_class == 'Kenyon_Cell').all()

        if connectome == 'fafb-left':
            assert (df.source_side == 'left').all()

            # askprat: drop those w/ target side == 'right'?
            # Prat: eh, any of these options could work. up to me.
            # (prob doesn't matter much anyway, looking at which glomeruli it is...)
            #
            # TODO or just use them too? could turn to be roughly equiv to using values
            # from other csv (appending two w/ same 'target_side' together)?
            # TODO TODO or load both csvs and add both together, based on target
            # side (prob doesn't matter hugely w/ how many fewer connections there are)?
            # TODO is there anything else special about these PNs that cross midline?
            # (maybe we'd exclude already anyway, for some other reason?)
            #
            # ipdb> df.target_side.value_counts()
            # left     13774
            # right      275
            #
            # Prat: below mostly/all the bilateral PNs he was excited about before, that
            # i had helped him image
            #
            # ipdb> df.loc[df.target_side == 'right', 'source_hemibrain_type'].value_counts()
            # V_ilPN                  87
            # VP3+VP1l_ivPN           65
            # VP1m+VP5_ilPN           45
            # VP1d_il2PN              42
            # VL1_ilPN                29
            # M_ilPNm90,M_ilPN8t91     3
            # VP1l+VP3_ilPN            2
            # M_smPNm1                 1
            # M_smPN6t2                1
        else:
            assert (df.source_side == 'right').all()
            # ipdb> df.target_side.value_counts()
            # right    13485
            # left       314
            #
            # ipdb> df.loc[df.target_side == 'left', 'source_hemibrain_type'].value_counts()
            # V_ilPN                  145
            # VP1m+VP5_ilPN            60
            # VP3+VP1l_ivPN            48
            # VP1d_il2PN               30
            # VL1_ilPN                 25
            # M_ilPNm90,M_ilPN8t91      3
            # M_smPN6t2                 2
            # VP1l+VP3_ilPN             1

        pn_id_col = 'source'
        kc_id_col = 'target'
        weight_col = 'weight'

        hemibrain_pn_type = 'source_hemibrain_type'
        df = _add_glomerulus_col_from_hemibrain_type(df, hemibrain_pn_type, kc_id_col)

        # this should be the same as the min_weight from hemibrain, where in that case
        # prat's query is what filtered stuff w/ smaller weight
        df = df[df[weight_col] >= 4].copy()

        # TODO delete
        # fafb_types = df.source_cell_type.dropna()
        # # true for at least fafb-left
        # assert (fafb_types.str.count('_') == 1).all()
        # fafb_gloms = _first_underscore_part(fafb_types)
        # odf = df.dropna(subset=['source_cell_type'])
        # assert odf.index.equals(fafb_gloms.index)
        # print(pd.concat([fafb_gloms, odf.glomerulus], axis=1).drop_duplicates())
        #
        # (askprat) want to change handling of any of these? have been
        # merging VC3l and VC3m into "VC3" (w/ old hemibrain stuff, at least). should
        # glomerulus_renames reflect this?
        # Prat: just completely ignore the source_cell_type values. almost certainly not
        # meaningful corrections made by the flywire people.
        #
        # why does task not split them? same receptor or something?
        # (task doesn't refer to VC3l/m, but does list diff receptors/etc for VC3 and
        # VC5. they also split VM6 into VM6v/m/l [all w/ at least mostly same
        # receptors]. they might also be saying that the "canonical" VM6 was VM6v?)
        #
        # (same combinations for both left/right)
        #       source_cell_type glomerulus
        # 630                VC3       VC3l
        # 9868               VC5       VC3m
        # 10678              VM6        VC5
        #
        # ipdb> 'VM6' in set(df.glomerulus)
        # False

        # askprat: do anything w/ 'target_hemibrain_type'? e.g. 'KCab-s', 'KCg-m',
        # etc (prob not, at least not categorically filtering out any of them)
        # Prat: final part after dash is from clustering on connectome. no reason to
        # exclude any of this.
        #
        # connectome='fafb-left'
        # ipdb> df.target_hemibrain_type.value_counts()
        # KCg-m         6463
        # KCab-m        1893
        # KCab-s        1874
        # KCab-c        1221
        # KCa'b'-m       765
        # KCa'b'-ap2     553
        # KCa'b'-ap1     421
        # KCg-d           66
        # KCab-p          44
        # KCg-s2           6
        # KCg-s3           4
        # KCg-s1           2
        #
        # connectome='fafb-right'
        # ipdb> df.target_hemibrain_type.value_counts()
        # KCg-m         6555
        # KCab-s        1800
        # KCab-m        1542
        # KCab-c        1357
        # KCa'b'-m       745
        # KCa'b'-ap2     613
        # KCa'b'-ap1     346
        # KCg-d           73
        # KCab-p          51
        # KCg-s2           5
        # KCg-s3           1
        # KCg-s1           1


    # TODO replace `glomerulus` w/ `glomerulus_col` below? or revert to hardcode above?

    if not _use_matt_wPNKC:
        # TODO also get working in _use_matt_wPNKC case?
        assert len(df[[pn_id_col, kc_id_col]].drop_duplicates()) == len(df)

        # askprat: so does this mean prat has already excluded multiglomeruli PNs
        # (intentionally or not), or are they all contained w/in stuff dropped above?
        #
        # Prat: doesn't think it's likely any of his queries would have missed MG PNs
        # (and as other comments say 'M' in PN type str probably means multiglomerular,
        # or at least includes them)
        #
        # seems True for both hemibrain and fafb (at least as long as we are dropping
        # stuff w/ multiple '_' or '+' in PN types above...)
        assert (
            len(df[[pn_id_col, 'glomerulus']].drop_duplicates()) ==
            df[pn_id_col].nunique()
        )

        assert not df[kc_id_col].isna().any()
        n_kcs = df[kc_id_col].nunique()

        if plot_dir is not None:
            assert not df[weight_col].isna().any()
            min_weight = df[weight_col].min()
            assert min_weight > 0
            # was true b/c Prat's query in hemibrain case, and b/c subsetting above in
            # fafb cases
            assert min_weight == 4

            # TODO TODO also move this plot after we drop non-task glomeruli? (prob
            # doesn't matter) (actually would keep bins more sane in
            # _drop_glom_with_plus, b/c data should match _drop_glom_with_plus=True case
            # after dropping non-task glomeruli)
            fig, ax = plt.subplots()
            # TODO also put input filename formatted mtime in a title line? prob not...
            sns.histplot(df[weight_col], discrete=True, ax=ax)
            ax.set_title(f'{connectome} PN->KC weights\n{min_weight=}\n{data_path.name}'
                f'\n{n_kcs=}'
            )
            savefig(fig, plot_dir, f'wPNKC_hist_{connectome}')

        to_rename = df.glomerulus.isin(glomerulus_renames)
        if to_rename.any():
            old_names = df.glomerulus[to_rename]
            warn(f'{connectome=} renaming glomeruli as {glomerulus_renames}:\n' +
                old_names.value_counts().to_string() + '\n'
            )

        glom_set = set(df.glomerulus)
        missing_old_names = set(glomerulus_renames.keys()) - glom_set
        assert len(missing_old_names) == 0
        # TODO print which if this assertion ever fails

        # TODO actually check it doesn't matter whether i do this before vs after pivot?
        # (or at least, that i intend for current behavior)
        df.glomerulus = df.glomerulus.replace(glomerulus_renames)

        if weight_divisor is None:
            # TODO refactor pivoting to share across branches of this conditional, and
            # with above processing of matt's CSVs
            wPNKC = pd.pivot_table(df, values=weight_col, index=kc_id_col,
                columns='glomerulus', aggfunc='count').fillna(0).astype(int)

            # TODO delete?
            df_bin = df.copy()
            assert (df[weight_col] > 0).all()
            df_bin[weight_col] = (df_bin[weight_col] > 0).astype(int)
            assert (df_bin[weight_col] == 1).all()
            wb = pd.pivot_table(df_bin, values=weight_col, index=kc_id_col,
                columns='glomerulus', aggfunc='sum').fillna(0).astype(int)
            assert wb.equals(wPNKC)
            del df_bin, wb
            #
        else:
            assert weight_divisor > 0

            using_count = pd.pivot_table(df, values=weight_col, index=kc_id_col,
                columns='glomerulus', aggfunc='count').fillna(0).astype(int)

            df[weight_col] = np.ceil(df[weight_col] / weight_divisor)

            wPNKC = pd.pivot_table(df, values=weight_col, index=kc_id_col,
                columns='glomerulus', aggfunc='sum').fillna(0).astype(int)

            assert (wPNKC >= using_count).all().all()
            # if weight_divisor is too large, this could fail
            # TODO maybe check it does (i.e. that wPNKC.equals(using_count), for high
            # enough weight_divisor)?
            assert (wPNKC > using_count).any().any()
            del using_count

    # sanity check
    assert n_kcs > 1000

    # TODO only do for 'hemibrain'? use more generic term ('kc_id'?)?
    wPNKC.index.name = 'bodyid'

    non_task_gloms = set(wPNKC.columns) - task_gloms
    if len(non_task_gloms) > 0:
        # connectome='hemibrain'|'fafb-right'
        # ['M']
        #
        # connectome='fafb-left'
        # ['M', 'MZ']
        warn(f'dropping glomeruli in {connectome=} but NOT task: '
            f'{sorted(non_task_gloms)}'
        )

    non_connectome_gloms = task_gloms - set(wPNKC.columns)
    if len(non_connectome_gloms) > 0:
        # connectome='hemibrain'|'fafb-left'|'fafb-right'
        # ['VM6l', 'VM6m', 'VM6v', 'VP1l', 'VP3', 'VP5']
        warn(f'glomeruli in Task but NOT {connectome=}: {sorted(non_connectome_gloms)}')

    def _get_kcs_without_input(wPNKC):
        kcs_without_input = (wPNKC == 0).T.all()
        n_kcs_without_input = kcs_without_input.sum()
        return kcs_without_input, n_kcs_without_input

    # these KC ids are already gone from df before pivot-ing into wPNKC
    # (dropped by _add_glom...)
    _, n_without_input_before = _get_kcs_without_input(wPNKC)
    assert n_without_input_before == 0

    wPNKC = wPNKC[task_gloms & set(wPNKC.columns)].copy()

    kcs_without_input, n_kcs_without_input = _get_kcs_without_input(wPNKC)
    if n_kcs_without_input > 0:
        warn(f'{n_kcs_without_input}/{len(wPNKC)} KCs without input, after dropping '
            f'non-Task glomeruli:\n{sorted(kcs_without_input[kcs_without_input].index)}'
        )

    assert not (wPNKC == 0).all().any(), 'had Task glomeruli providing no input to KCs'

    if plot_dir is not None:
        # NOTE: mean of this w/ connectome='hemibrain' is 5.44 (NOT n_claws=7 used
        # by uniform)
        n_inputs_per_kc = wPNKC.T.sum()

        fig, ax = plt.subplots()
        sns.histplot(n_inputs_per_kc, discrete=True, ax=ax)

        ax.set_xlabel('inputs per KC\n(after processing connectome weights)')

        # relevant for picking appropriate n_claws for uniform/hemidraw cases, or for
        # picking weight_divisor that produces closest avg to the n_claws=7 we had
        # already been using
        avg_n_inputs_per_kc = n_inputs_per_kc.mean()

        ax.set_title(f'total inputs per KC\n{connectome=}\n{weight_divisor=}\n{n_kcs=}'
            f'\nmean inputs per KC: {avg_n_inputs_per_kc:.2f}'
        )
        # TODO why these look so different for fafb inputs (vs hemibrain)? for similar
        # avg_n_inputs_per_kc (adjusting fafb weight_divisor to roughly match the value
        # for hemibrain w/ weight_divisor=20), we get much more of the right lobe in the
        # fafb plots. are there less (PN, KC) pairs for some reason (what should be the
        # only lobe w/ weight_divisor=None, or the left lobe in others)?
        savefig(fig, plot_dir, f'wPNKC-summed-within-KC_hist_{connectome}')

        # TODO also plot sums within glomeruli?

        # TODO also plot (hierarchichally clustered) wPNKC (w/ plot+colorscale as in
        # natmix_data/analysis.py?)?

    assert len(wPNKC) == n_kcs

    # TODO see if i can also sort output in uniform case (b/c if not, might suggest
    # there is other order-of-glomeruli dependence in fit_mb_model THAT THERE SHOULD NOT
    # BE)
    # TODO i can not seem to recreate uniform output (by sorting wPNKC post-hoc), but
    # i'm not sure that's actually a problem. maybe input *should* always just be in a
    # particular order, and shouldn't necessarily matter that it's this one...
    wPNKC = wPNKC.sort_index(axis='columns')

    return wPNKC


# TODO doc expected input format (i.e. what are rows / cols)
def drop_silent_model_cells(responses: pd.DataFrame) -> pd.DataFrame:
    # .any() checks for any non-zero, so should also work for spike counts
    # (or 1.0/0.0, instead of True/False)
    nonsilent_cells = responses.T.any()

    # TODO maybe restore as warning, or behind a checks flag or something.
    # (also works if index.name == 'model_kc')
    # (commented cause was failing b/c index name was diff in natmix_data/analysis.py
    # script, though i could have changed that...)
    #assert 'model_kc' in nonsilent_cells.index.names

    return responses.loc[nonsilent_cells].copy()


def _get_silent_cell_suffix(responses_including_silent, responses_without_silent=None
    ) -> str:
    if responses_without_silent is None:
        responses_without_silent = drop_silent_model_cells(responses_including_silent)

    assert responses_without_silent.shape[1] == responses_including_silent.shape[1]
    n_total_cells = len(responses_including_silent)
    n_silent_cells = n_total_cells - len(responses_without_silent)

    # TODO could relax to assert >=, if ever fails
    # (same as asserting len(responses_without_silent) > 0)
    assert n_total_cells > n_silent_cells

    # titles these will be appended to should already end w/ '\n'
    title_suffix = f'(dropped {n_silent_cells}/{n_total_cells} silent cells)'
    # TODO also return n_[total|silent]_cells? only if useful in some existing calling
    # code
    return title_suffix


# TODO delete all these? or re-organize? want to minimize how much mb_model stuff
# assumes a certain output folder structure
#
# TODO also use for wPNKC(s)? anything else?
data_outputs_root = Path('data')
hallem_csv_root = data_outputs_root / 'preprocessed_hallem'
hallem_delta_csv = hallem_csv_root / 'hallem_orn_deltas.csv'
hallem_sfr_csv = hallem_csv_root / 'hallem_sfr.csv'
#

# TODO delete Optional in RHS of return Tuple after implementing in other cases
# TODO if orn_deltas is passed, should we assume we should tune on hallem? or assume we
# should tune on that input?
# TODO rename drop_receptors_not_in_hallem -> glomeruli
# TODO some kind of enum instead of str for pn2kc_connections?
# TODO accept sparsities argument (or scalar avg probably?), for target when tuning
# TODO delete _use_matt_wPNKC after resolving differences wrt Prat's? maybe won't be
# possible though, and still want to be able to reproduce matt's stuff...
# TODO doc that sim_odors is Optional[Set[str]]
# TODO actually, probably can delete sim_odors now? why even have it? to tune on a diff
# set than to return responses for?
# TODO check new default for tune_on_hallem didn't break any of my existing calling code
def fit_mb_model(orn_deltas: Optional[pd.DataFrame] = None, sim_odors=None, *,
    tune_on_hallem: bool = False, pn2kc_connections: str = 'hemibrain',
    weight_divisor: Optional[float] = None, n_claws: Optional[int] = None,
    drop_multiglomerular_receptors: bool = True,
    drop_receptors_not_in_hallem: bool = False, seed: int = 12345,
    target_sparsity: Optional[float] = None,
    target_sparsity_factor_pre_APL: Optional[float] = None,
    _use_matt_wPNKC=False, _drop_glom_with_plus=True,
    _add_back_methanoic_acid_mistake=False,
    fixed_thr: Optional[float] = None,
    # TODO update float type if i support vector wAPLKC/wKCAPL (for connectome inputs)
    wAPLKC: Optional[float] = None, wKCAPL: Optional[float] = None,
    print_olfsysm_log: Optional[bool] = None, plot_dir: Optional[Path] = None,
    make_plots: bool = True, title: str = '',
    drop_silent_cells_before_analyses: bool = drop_silent_model_kcs,
    repro_preprint_s1d: bool = False,
    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Dict[str, Any]]:
    # TODO doc point of sim_odors. do we need to pass them in (not typically, no)?
    # (even when neither tuning nor running on any hallem data?)
    # TODO does matt's code support float wPNKC? can i just directly normalize wPNKC and
    # pass that in, rather than dealing w/ weight_divisor=<float>?
    # TODO TODO have responses [/spike_counts] returned w/ row indices using same
    # connectome cell IDs are wPNKC row index ('bodyid' as called in hemibrain data prat
    # assembled) (or have wPNKC also use same sequential int index)
    """
    Args:
        orn_deltas: dataframe of shape (# glomeruli, # odors). values should be in units
            of change in spike deltas (`model_mb_responses` will do this scaling for
            you). Input should only contain one value per (glomerulus, odor) pair, so
            take a mean across flies (if applicable) before passing.

            Odor (column) index should also not have multiple repeats of the same odor.
            Odor strings (in at least one index level) should be in format like:
            '<odor-name-abbreviation> @ <log10-conc>', e.g. 'eb @ -3'.
            Odor index may also have a 'panel' level, to group odors by experiment,
            though output `responses` and `spike_counts` currently do not preserve
            this level [could implement].

            Only glomeruli in intersection of connectome (hemibrain, unless otherwise
            specified) and Task et al 2022 glomeruli will be included in model.
            Additional input glomeruli will be dropped, and any of these glomeruli
            missing from input will have their spike-change-deltas imputed as 0.
            Glomeruli will use SFRs reported in Hallem, when cognate OR is in Hallem
            dataset, or mean Hallem SFR otherwise.

        target_sparsity: target mean response rate (across tuned odors, which is all
            odors by default). By default, model is only tuned until mean response
            rate is within +/- 10% (sp_acc) of target. If passed, `fixed_thr` and
            `wAPLKC` should not be.

        fixed_thr: added to each KCs spontaneous firing rate (SFR) to produce the spike
            threshold. `olfsysm` has options to not add this, but not currently exposed
            by this wrapper. expected that only either this and `wAPLKC` OR
            `target_sparsity` are specified.

        wAPLKC: weight from APL to every KC. `wKCAPL` defined from this, unless both are
            passed.

        pn2kc_connections_options: for connectome (non-RNG) options, passed to
            `connectome_wPNKC`

        weight_divisor: passed to `connectome_wPNKC` (only relevant for connectome
            `pn2kc_connections` options)

        _drop_glom_with_plus: passed to `connectome_wPNKC`

        seed: if using one of `pn2kc_connections` options that uses RNG to generate the
            PN->KC weight matrix (e.g. 'uniform', 'hemidraw', NOT 'hemibrain'), this
            seeds that RNG. The model (as configured by default, and as presented by
            this wrapper) is otherwise deterministic. NOTE: seeded by default.

        n_claws: for relevant `pn2kc_connections` (e.g. 'uniform', 'hemidraw', NOT
            'hemibrain') this sets that number of PNs each KC draws (i.e. # of claws for
            each KC)

        drop_silent_cells_before_analyses: only relevant if `make_plots=True`

        title: (internal use only) only used if plot_dir passed. used as prefix for
            titles of plots.

        repro_preprint_s1d: (internal use only) whether to add fake odors + return data
            to allow reproduction of preprint figure S1D (showing model response rates
            to fake CO2, fake ms, and real eb)


    Returns:
        responses: `spike_counts`, but binarized so any amount of spikes = True, else
            False.

        spike_counts: dataframe of shape (# KCs, # odors). If input had 'panel' level in
            odor (column) index, this currently does not preserve that level.
            If `repro_preprint_s1d=True` (which is NOT the default), extra
            (synthetic) odors for that plot will be included in output.

        wPNKC: dataframe of shape (# KCs, # glomeruli).

            Currently, I've only tested Matt's code with integer values, which can be
            interpreted as # of claws between PNs (of each glomerulus) to each KC.

        param_dict: dict containing tuned parameters (e.g. 'fixed_thr', 'wAPLKC'),
            certain model intermediates (e.g. 'kc_spont_in'), and certain parameters
            relevant for reproducibility (e.g. 'sp_acc', 'max_iters').
    """
    # TODO maybe make it so sim_odors is ignored if orn_deltas is passed in?
    # or err [/ assert same odors as orn_deltas]? would then need to conditionally pass
    # in calls in here...

    pn2kc_connections_options = {'uniform', 'caron', 'hemidraw'}
    pn2kc_connections_options.update(connectome_options)

    if pn2kc_connections not in pn2kc_connections_options:
        raise ValueError(f'{pn2kc_connections=} not in {pn2kc_connections_options}')

    if pn2kc_connections == 'caron':
        # TODO support? may need for comparisons to ann's model (but hopefully there's a
        # determinstic version of her model, like our 'hemibrain', if i really care
        # about that? not sure i could get same RNG wPNKC in Matt's code vs hers...)?
        raise NotImplementedError

    # TODO rename? there is a fixed number of claws, just that we can set them w/
    # n_claws for these models, as opposed to wPNKC determining it (from whatever
    # connectome) in other cases.
    variable_n_claw_options = {'uniform', 'caron', 'hemidraw'}
    variable_n_claws = False
    if pn2kc_connections not in variable_n_claw_options:
        if n_claws is not None:
            raise ValueError(f'n_claws only supported for {variable_n_claw_options}')
    else:
        # TODO also default to averaging over at least a few seeds in all these cases?
        # how much do things actually tend to vary, seed to seed?
        variable_n_claws = True
        if n_claws is None:
            # NOTE: it seems to default to 6 in olfsysm.cpp
            raise ValueError('n_claws must be passed an int if pn2kc_connections in '
                f'{variable_n_claw_options}'
            )

    # TODO rename hallem_input to only_run_on_hallem (or something better)?
    hallem_input = False
    if orn_deltas is None:
        hallem_input = True
        # TODO just load orn_deltas here?
    else:
        # TODO delete
        orn_deltas = orn_deltas.copy()
        #

        # TODO switch to requiring 'glomerulus' (and in the one test that passes hallem
        # as input explicitly, process to convert to glomeruli before calling this fn)?
        valid_orn_index_names = ('receptor', 'glomerulus')
        if orn_deltas.index.name not in valid_orn_index_names:
            raise ValueError(f"{orn_deltas.index.name=} not in {valid_orn_index_names}")

        if orn_deltas.index.name == 'receptor':
            # TODO delete? (/ use to explain what is happening in case where
            # verbose=True and we are dropping stuff below)
            receptors = orn_deltas.index.copy()
            #

            glomeruli = [
                orns.find_glomeruli(r, verbose=False) for r in orn_deltas.index
            ]
            assert not any('+' in g for gs in glomeruli for g in gs)
            glomeruli = ['+'.join(gs) for gs in glomeruli]

            orn_deltas.index = glomeruli
            orn_deltas.index.name = 'glomerulus'

            # Should drop any input glomeruli w/ '+' in name (e.g. 'DM3+DM5')
            orn_deltas = handle_multiglomerular_receptors(orn_deltas,
                drop_multiglomerular_receptors=drop_multiglomerular_receptors
            )

        # TODO if orn_deltas.index.name == 'glomerulus', assert all input are in
        # task/connectome glomerulus names
        # TODO same check on hallem glomeruli names too (below)?

    mp = osm.ModelParams()

    # TODO TODO what was matt using this for in narrow-odors-jupyter/modeling.ipynb
    #
    # Betty seemed to think this should always be True?
    # TODO was this actualy always True for matt's other stuff (including what's in
    # preprint? does it matter?)
    # Doesn't seem to affect any of the comparisons to Matt's outputs, whether this is
    # True or not (though I'm not clear on why it wouldn't do something, looking at the
    # code...)
    mp.kc.ignore_ffapl = True

    if fixed_thr is not None:
        assert target_sparsity is None
        assert target_sparsity_factor_pre_APL is None
        assert wAPLKC is not None, 'for now, assuming both passed if either is'

        # TODO move these values / notes to model_test.py, or wherever sensitivity
        # analysis gets moved to.
        #fixed_thr = 145.973
        # these both break exact similarity to matt's hemimat outputs, as expected:
        #fixed_thr = 140
        #fixed_thr = 147
        mp.kc.fixed_thr = fixed_thr

        # TODO TODO add comment explaining what this is
        mp.kc.add_fixed_thr_to_spont = True

        # actually do need this. may or may not need thr_type='fixed' too
        mp.kc.use_fixed_thr = True
        mp.kc.thr_type = 'fixed'
    else:
        mp.kc.thr_type = 'uniform'

    # TODO delete
    # just to try to confirm actual sparsity after setting thresh is ~2 * sp_target.
    # may not work.
    # TODO TODO make a test that does this, and then checks sparsity is within tolerance
    # (or even tigher, almost exactly?) after target_sparsity_factor_pre_APL
    #print('HARDCODING TUNE_APL_WEIGHTS=FALSE')
    #mp.kc.tune_apl_weights = False
    #
    #pre_APL_sparsity_should_be = target_sparsity * target_sparsity_factor_pre_APL
    #

    # TODO after getting model to accept hardcoded wAPLKC and wKCAPL, only do this if
    # not hardcoding those (+ fixed thr)
    if target_sparsity is not None:
        mp.kc.sp_target = target_sparsity

    # target_sparsity_factor_pre_APL=2 would preserve old default behavior, where KC
    # threshold set to achieve 2 * sp_target, then APL tuned to bring down to sp_target
    if target_sparsity_factor_pre_APL is not None:
        # since APL should only be able to decrease response rate from where we set it
        # by picking KC spike threshold
        assert target_sparsity_factor_pre_APL >= 1

        if target_sparsity is not None:
            sp_target = target_sparsity
        else:
            # should be the olfsysm default (get from mp.kc?)
            sp_target = .1
        assert sp_target * target_sparsity_factor_pre_APL <= 1.0
        del sp_target

        mp.kc.sp_factor_pre_APL = target_sparsity_factor_pre_APL

        # TODO TODO test that if this is 1.0, then APL is kept off (or will one
        # iteration of tuning loop still happen + change things?), or at least doesn't
        # change responses/spike_counts
        # TODO TODO what's max value of this (min that forces threshold to do nothing,
        # with only APL bringing activity down? or does that not max sense?) maybe i
        # should change olfsysm to use something with a more sensible max (so i can go
        # between two extremes of all-threshold vs all-APL more easily)?
        # (probably just `1 / target_sparsity`?)

    # TODO assert that this csv is equiv to orn_deltas / orns.orns data?
    hc_data_csv = str(Path('~/src/olfsysm/hc_data.csv').expanduser())
    # TODO TODO add comment explaining what this is doing
    osm.load_hc_data(mp, hc_data_csv)
    del hc_data_csv

    hallem_orn_deltas = orns.orns(add_sfr=False, drop_sfr=False, columns='glomerulus').T
    # Should drop 'DM3+DM5'
    hallem_orn_deltas = handle_multiglomerular_receptors(hallem_orn_deltas,
        drop_multiglomerular_receptors=drop_multiglomerular_receptors
    )

    # how to handle this for stuff not in hallem? (currently imputing mean Hallem sfr)
    sfr_col = 'spontaneous firing rate'
    sfr = hallem_orn_deltas[sfr_col]
    assert hallem_orn_deltas.columns[-1] == sfr_col
    hallem_orn_deltas = hallem_orn_deltas.iloc[:, :-1].copy()
    n_hallem_odors = hallem_orn_deltas.shape[1]
    assert n_hallem_odors == 110

    # TODO refactor
    hallem_orn_deltas = abbrev_hallem_odor_index(hallem_orn_deltas, axis='columns')

    # TODO delete
    # TODO any code i'm still using break if hallem_input=True and sim_odors is not
    # passed in? not currently passing in sim_odors anymore...
    # (only 1 call in model_test.py explicitly passes in, and only to check against
    # calls that don't)
    '''
    if hallem_input:
        assert sim_odors is None
        #print('see comment above')
        #import ipdb; ipdb.set_trace()
    '''
    #

    # TODO delete? still want to support?
    # TODO add comment explaining purpose of this block
    if hallem_input and sim_odors is not None:
        sim_odors_names2concs = dict()
        for odor_str in sim_odors:
            name = olf.parse_odor_name(odor_str)
            log10_conc = olf.parse_log10_conc(odor_str)

            # If input has any odor at multiple concentrations, this will fail...
            assert name not in sim_odors_names2concs
            sim_odors_names2concs[name] = log10_conc

        assert len(sim_odors_names2concs) == len(sim_odors)

        # These should have any abbreviations applied, but should currently all be the
        # main data (excluding lower concentration ramps + fruits), and not include
        # concentration (via suffixes like '@ -3')
        hallem_odors = hallem_orn_deltas.columns

        # TODO would need to relax this if i ever add lower conc data to hallem input
        assert all(olf.parse_log10_conc(x) is None for x in hallem_odors)

        # TODO TODO replace w/ hope_hallem_minus2_is_our_minus3 code used elsewhere
        # (refactoring to share), rather than overcomplicating here?
        #
        # TODO TODO warn about any fuzzy conc matching (maybe later, only if
        # hallem_input=True?)
        # (easier if i split this into ~2 steps?)
        hallem_sim_odors = [n for n in hallem_odors
            if n in sim_odors_names2concs and -3 <= sim_odors_names2concs[n] < -1
        ]
        # this may not be all i want to check
        assert len(hallem_sim_odors) == len(sim_odors)

        # since we are appending ' @ -2' to hallem_orn_deltas.columns below
        hallem_sim_odors = [f'{n} @ -2' for n in hallem_sim_odors]

    # TODO factor to drosolf.orns?
    assert hallem_orn_deltas.columns.name == 'odor'
    hallem_orn_deltas.columns += ' @ -2'

    # so that glomerulus order in Hallem CSVs will match eventual wPNKC output (which
    # has glomeruli sorted)
    #
    # making a copy to sort by glomeruli, since that would break an assertion later
    # (comparing against mp.orn internal data), if I sorted source variables.
    hallem_orn_deltas_for_csv = hallem_orn_deltas.sort_index(axis='index')
    sfr_for_csv = sfr.sort_index()

    if hallem_delta_csv.exists():
        assert hallem_sfr_csv.exists()
        # TODO or just save to root, but only do so if not already there? and load and
        # check against that otherwise? maybe save to ./data

        # TODO could just load first time we reach this (per run of script)...
        deltas_from_csv = pd.read_csv(hallem_delta_csv, index_col='glomerulus')
        sfr_from_csv = pd.read_csv(hallem_sfr_csv, index_col='glomerulus')

        deltas_from_csv.columns.name = 'odor'

        assert sfr_from_csv.shape[1] == 1
        sfr_from_csv = sfr_from_csv.iloc[:, 0].copy()

        assert sfr_for_csv.equals(sfr_from_csv)
        # changing abbreviations of some odors broke this previously
        # (hence why i replaced it w/ the two assertions below. now ignoring odor
        # columns)
        #assert hallem_orn_deltas_for_csv.equals(deltas_from_csv)
        assert np.array_equal(hallem_orn_deltas_for_csv, deltas_from_csv)
        assert hallem_orn_deltas_for_csv.index.equals(deltas_from_csv.index)
    else:
        if data_outputs_root.is_dir():
            # (subdirectory of data_outputs_root)
            hallem_csv_root.mkdir(exist_ok=True)

            # TODO assert columns of the two match here (so i don't need to check from
            # loaded versions, and so i can only check one against wPNKC, not both)
            to_csv(hallem_orn_deltas_for_csv, hallem_delta_csv)
            to_csv(sfr_for_csv, hallem_sfr_csv)

            # TODO delete? unused
            #deltas_from_csv = hallem_orn_deltas_for_csv.copy()
            #sfr_from_csv = sfr_for_csv.copy()
            #

        # TODO warn if data_outputs_root does not exist

    del hallem_orn_deltas_for_csv, sfr_for_csv

    if hallem_input:
        orn_deltas = hallem_orn_deltas.copy()

        if _add_back_methanoic_acid_mistake:
            warn('intentionally mangling Hallem methanoic acid responses, to recreate '
                'old bug in Ann/Matt modeling analysis! do not use for any new '
                'results!'
            )
            orn_deltas['methanoic acid @ -2'] = [
                -2,-14,31,0,33,-8,-6,-9,8,-1,-20,3,25,2,5,12,-8,-9,14,7,0,4,14
            ]

    # TODO (delete?) implement means of getting threshold from hallem input + hallem
    # glomeruli only -> somehow applying that threshold [+APL inh?] globally (and
    # running subsequent stuff w/ all glomeruli, including non-hallem ones) (even
    # possible?)
    # TODO (delete?) now that i can just hardcode the 2 params, can i make plots where i
    # "tune" on hallem and then apply those params to the model using my data as input,
    # w/ all glomeruli (or does it still not make sense to use the same global params,
    # w/ new PNs w/ presumably new spontaneous input now there? think it might not make
    # sense...)
    # TODO delete
    '''
    if not hallem_input and tune_on_hallem:
        # (think i always have this True when tune_on_hallem=True, at the moment, but if
        # i can do what i'm asking in comment above, could try letting this be False
        # while tune_on_hallem=True, for input that has more glomeruli than in Hallem)
        print(f'{drop_receptors_not_in_hallem=}')
        import ipdb; ipdb.set_trace()
    '''
    #

    connectome = (
        # NOTE: this means that if pn2kc_connections == 'hemidraw', it will use marginal
        # probabilities from 'hemibrain' connectome. no current support for using either
        # fafb data source for that.
        pn2kc_connections if pn2kc_connections in connectome_options else 'hemibrain'
    )
    # TODO check that nothing else depends on order of columns (glomeruli) in these
    wPNKC = connectome_wPNKC(connectome=connectome, weight_divisor=weight_divisor,
        # disabling plot_dir here b/c models that are run w/ multiple seeds (handled in
        # code that calls this fn, not within here), would end up trying to make the
        # same plots for each seed (which would trigger savefig assertion that we aren't
        # writing to same path more than once)
        plot_dir=plot_dir if pn2kc_connections in connectome_options else None,
        _use_matt_wPNKC=_use_matt_wPNKC,
        _drop_glom_with_plus=_drop_glom_with_plus,
    )
    glomerulus_index = wPNKC.columns

    if not hallem_input:
        zero_filling = (~ glomerulus_index.isin(orn_deltas.index))
        if zero_filling.any():
            msg = ('zero filling spike deltas for glomeruli not in data: '
                f'{sorted(glomerulus_index[zero_filling])}'
            )
            warn(msg)

        # TODO TODO (?) if i add 'uniform' draw path, make sure zero filling is keeping
        # glomeruli that would implicitly be dropped in hemibrain (/ hemidraw / caron)
        # cases (as we don't need wPNKC info in 'uniform' case, as all glomeruli are
        # sampled equally, without using any explicit connectivity / distribution)
        # (don't warn there either)
        #
        # Any stuff w/ '+' in name (e.g. 'DM3+DM5' in Hallem) should already have been
        # dropped.
        input_glomeruli = set(orn_deltas.index)
        glomeruli_missing_in_wPNKC = input_glomeruli - set(glomerulus_index)
        if len(glomeruli_missing_in_wPNKC) > 0:
            # TODO assert False? seems we could do that at least for megamat data...
            warn('dropping glomeruli not in wPNKC (while zero-filling): '
                f'{glomeruli_missing_in_wPNKC}'
            )

        if tune_on_hallem:
            # TODO make sure we aren't writing wPNKC in this case (and maybe not other
            # hallem CSVs? they are probably fine either way...)
            hallem_not_in_wPNKC = set(hallem_orn_deltas.index) - set(glomerulus_index)
            assert len(hallem_not_in_wPNKC) == 0 or hallem_not_in_wPNKC == {'DA4m'}, \
                f'unexpected {hallem_not_in_wPNKC=}'

            if len(hallem_not_in_wPNKC) > 0:
                warn(f'dropping glomeruli not in wPNKC {hallem_not_in_wPNKC} from '
                    'Hallem data to be used for tuning'
                )

            # this will be concatenated with orn_deltas below, and we don't want to add
            # back the glomeruli not in wPNKC
            hallem_orn_deltas = hallem_orn_deltas.loc[
                [c for c in hallem_orn_deltas.index if c in glomerulus_index]
            ].copy()

        orn_deltas_pre_filling = orn_deltas.copy()

        # TODO simplify this. not a pandas call for it? reindex_like seemed to not
        # behave as expected, but maybe it's for something else / i was using it
        # incorrectly
        # TODO just do w/ pd.concat? or did i want shape to match hallem exactly in that
        # case? matter?
        # TODO reindex -> fillna?
        orn_deltas = pd.DataFrame([
                orn_deltas.loc[x].values if x in orn_deltas.index
                # TODO correct? after concat across odors in tune_on_hallem=True case?
                else np.zeros(len(orn_deltas.columns))
                for x in wPNKC.columns
            ], index=glomerulus_index, columns=orn_deltas.columns
        )

        # TODO need to be int (doesn't seem so)?
        mean_sfr = sfr.mean()

        non_hallem_gloms = sorted(set(glomerulus_index) - set(sfr.index))
        if len(non_hallem_gloms) > 0:
            warn(f'imputing mean Hallem SFR ({mean_sfr:.2f}) for non-Hallem glomeruli:'
                f' {non_hallem_gloms}'
            )

        sfr = pd.Series(index=glomerulus_index,
            data=[(sfr.loc[g] if g in sfr else mean_sfr) for g in glomerulus_index]
        )
        assert sfr.index.equals(orn_deltas.index)
    #

    odor_index = orn_deltas.columns
    n_input_odors = orn_deltas.shape[1]


    extra_orn_deltas = None
    # TODO delete/comment after i'm done?
    #'''
    eb_mask = orn_deltas.columns.get_level_values('odor').str.startswith('eb @')
    # should be true for megamat and hallem
    if repro_preprint_s1d and eb_mask.sum() == 1:
        # TODO TODO finish support for "extra" odors (to be simmed, but not tuned on)
        # (expose as kwarg eventually prob)
        # (would not be conditional on eb if so... just a hack to skip validation, and
        # only want S1D if we do have eb, as thats what preprint one used)

        # TODO try to move up above any modifications to orn_deltas (mainly the
        # glomeruli filling above) after getting it to work down here? (why? just to
        # make easier to convert to kwarg?)

        fake_odors = ['fake ms @ 0']
        if 'V' in orn_deltas.index:
            fake_odors.append('fake CO2 @ 0')

        # should only be in 'hallem' cases
        else:
            # TODO TODO how did matt handle this? (not in wPNKC I'm currently using in
            # Hallem case) (pretty sure most of his modelling is done w/o 'V' (or any
            # non-Hallem glomeruli) in wPNKC. so what is he doing for wPNKC here? and
            # what data is he using for the non-Hallem glomeruli for tuning?)
            #
            # (not doing fake-CO2 in 'hallem' context for now)
            warn("glomerulus 'V' not in wPNKC, so not adding fake CO2!")

        # TODO convert to kwarg -> def in model_mb... and pass in thru there?
        extra_orn_deltas = pd.DataFrame(index=orn_deltas.index, columns=fake_odors,
            data=0
        )
        assert 'odor' in orn_deltas.columns.names
        extra_orn_deltas.columns.name = 'odor'
        # TODO handle appending ' @ 0' automatically if needed (only if i actually
        # expose extra_orn_deltas as a kwarg)?

        extra_orn_deltas.loc['DL1', 'fake ms @ 0'] = 300
        if 'V' in orn_deltas.index:
            extra_orn_deltas.loc['V', 'fake CO2 @ 0'] = 300

        eb_deltas = orn_deltas.loc[:, eb_mask]

        if 'panel' in eb_deltas.columns.names:
            eb_deltas = eb_deltas.droplevel('panel', axis='columns')

        assert eb_deltas.shape[1] == 1
        eb_deltas = eb_deltas.iloc[:, 0]
        assert len(eb_deltas) == len(extra_orn_deltas)

        # eb_deltas.name will be like 'eb @ -3' (previous odor columns level value)
        extra_orn_deltas[eb_deltas.name] = eb_deltas

        if sim_odors is not None:
            assert hallem_input
            # sim_odors contents not used for anything other than internal plots below,
            # so sufficient to only grow hallem_sim_odors here (which is used to subset
            # responses right before returning, and for nothing else [past this point at
            # least])
            #
            # not also growing by extra `eb_deltas.name`, because that gets removed
            # before hallem_sim_odors used (extra eb is not returned in hallem or any
            # other case. only checked against responses to existing eb col)
            hallem_sim_odors.extend(fake_odors)
    #'''

    n_extra_odors = 0
    if extra_orn_deltas is not None:
        n_extra_odors = extra_orn_deltas.shape[1]

        if 'panel' in orn_deltas.columns.names:
            assert 'extra' not in set(orn_deltas.columns.get_level_values('panel'))
            extra_orn_deltas = util.addlevel(extra_orn_deltas, 'panel', 'extra',
                axis='columns'
            )

        # TODO assert row index unchanged and column index up-to-old-length too?
        #
        # removed verify_integrity=True since there is currently duplicate 'eb' in
        # hallem case (w/o 'panel' level to disambiguate) (only when adding extra odors)
        orn_deltas = pd.concat([orn_deltas, extra_orn_deltas], axis='columns')

        odor_index = orn_deltas.columns

        # TODO make sure we aren't overwriting either of these below before running!
        mp.kc.tune_from = range(n_input_odors)
        mp.sim_only = range(n_input_odors)


    # TODO maybe set tune_on_hallem=False (early on) if orn_deltas is None?
    if tune_on_hallem and not hallem_input:
        # TODO maybe make a list (largely just so i can access it more than once)?
        # TODO where is default defined for this? not seeing it... behave same as if not
        # passed (e.g. if only have hallem odors)
        mp.kc.tune_from = range(n_hallem_odors)

        # Will need to change this after initial (threshold / inhibition setting) sims.
        # TODO interactions between this and tune_from? must sim_only contain tune_from?
        mp.sim_only = range(n_hallem_odors)

        # TODO worth setting a seed here (as model_mix_responses.py did, but maybe not
        # for good reason)?

        # at this point, if i pass in orn_deltas=orns.orns(add_sfr=False).T, only
        # columns differ (b/c odor renaming)

        # TODO TODO adapt to work w/ panel col level? what to use for hallem? 'hallem'?
        # None?
        # TODO assert this concat doesn't change odor (col) index? inspect what it's
        # doing to sanity check?
        #
        # TODO TODO test on my actual data (just tried hallem duped so far).
        # (inspect here to check for weirdness?)
        # TODO TODO need to align (if mismatching sets of glomeruli)?
        # TODO add metadata to more easily separate?
        # TODO TODO maybe add verify_integrity=True (or at least test that everything
        # works in case where columns are verbatim duplicated across the two, which
        # could probably happen if an odor was at minus 2, or if i add support for other
        # concentrations?)
        orn_deltas = pd.concat([hallem_orn_deltas, orn_deltas], axis='columns')

        # since other checks will compare these two indices later
        assert set(sfr.index) == set(orn_deltas.index)

        orn_deltas = orn_deltas.loc[sfr.index].copy()

        # TODO delete? not sure if it's triggered outside of case where i accidentally
        # passed input where all va/aa stuff was dropped (by calling script w/
        # 2023-04-22 as end of date range, rather than start)
        try:
            # TODO support input w/ panel level on odor index / delete
            #
            # TODO if i wanna keep this, move earlier (or at least have another version
            # of this earlier? maybe in one of first lines in fit_mb_model, or in
            # whatever is processing orn_deltas before it's passed to fit_mb_model?)
            # (the issue seems to be created before we get into fit_mb_model)
            assert sim_odors is None or sim_odors == set(
                odor_index.get_level_values('odor')
            ), 'why'
        except AssertionError:
            import ipdb; ipdb.set_trace()

    if not hallem_input:
        # TODO maybe i should still have an option here to tune on more data than what i
        # ultimately return (perhaps including diagnostic data? though they prob don't
        # have representative KC sparsities either...)

        # TODO TODO try to implement other strategies where we don't need to throw
        # away input glomeruli/receptors
        # (might need to make my own gkc_wide csv equivalent, assuming it only contains
        # the connections involving the hallem glomeruli)
        # (also, could probably not work in the tune_on_hallem case...)

        # TODO delete here (already moved into conditional below)
        #hallem_glomeruli = hallem_orn_deltas.index

        # TODO TODO raise NotImplementedError/similar if tune_on_hallem=True,
        # not hallem_input, and not drop_receptors_not_in_hallem?

        # TODO TODO maybe this needs to be True if tune_on_hallem=True? at least as
        # implemented now?
        # TODO rename to drop_glomeruli_not_in_hallem?
        if drop_receptors_not_in_hallem:
            # NOTE: this should already have had 'DM3+DM5' (Or33b) removed above
            hallem_glomeruli = hallem_orn_deltas.index

            glomerulus2receptors = orns.task_glomerulus2receptors()
            receptors = np.array(
                ['+'.join(glomerulus2receptors[g]) for g in orn_deltas.index]
            )
            # technically this would also throw away 33b, but that is currently getting
            # thrown out above w/ the drop_multiglomerular_receptors path
            receptors_not_in_hallem = ~orn_deltas.index.isin(hallem_glomeruli)
            if receptors_not_in_hallem.sum() > 0:
                # TODO warn differently (/only?) for stuff that was actually in our
                # input data, and not just zero filled above?
                msg = 'dropping glomeruli not in Hallem:'
                # TODO sort on glomeruli names (seems it already is. just from hallem
                # order? still may want to sort here to ensure)
                msg += '\n- '.join([''] + [f'{g} ({r})' for g, r in
                    zip(orn_deltas.index[receptors_not_in_hallem],
                        receptors[receptors_not_in_hallem])
                ])
                msg += '\n'
                warn(msg)

            orn_deltas = orn_deltas[~receptors_not_in_hallem].copy()
            sfr = sfr[~receptors_not_in_hallem].copy()

            # TODO refactor to not use glomerulus index, and just always use
            # wPNKC.columns, to not have to deal w/ the two separately? (here and
            # elsewhere...)
            assert glomerulus_index.equals(wPNKC.columns)
            glomerulus_index = glomerulus_index[~receptors_not_in_hallem].copy()
            wPNKC = wPNKC.loc[:, ~receptors_not_in_hallem].copy()

        # TODO TODO another option to use this input for fitting thresholds (+ APL
        # inhibition), w/o using hallem at all?

        # TODO TODO am i not seeing inhibition to the extent that i might expect by
        # comparing the deltas to hallem? is it something i can improve by changing my
        # dF/F -> spike delta estimation process, or is it a limitation of my
        # measurements / the differences between the hallem data and ours

    # TODO TODO probably still support just one .name == 'odor' tho...
    # (esp for calls w/ just hallem input, either old ones here or model_test.py?)
    # TODO move earlier?
    assert orn_deltas.columns.name == 'odor' or (
        orn_deltas.columns.names == ['panel', 'odor']
    )

    # TODO would we or would we not have removed it in that case? and what about
    # pratyush wPNKC case?
    # If using Matt's wPNKC, we may have removed this above:
    if 'DA4m' in hallem_orn_deltas.index:
        assert np.array_equal(hallem_orn_deltas, mp.orn.data.delta)

        if hallem_input:
            # TODO just do this before we would modify sfr (in that one branch above)?
            assert np.array_equal(sfr, mp.orn.data.spont[:, 0])

    # TODO TODO merge da4m/l hallem data (pretty sure they are both in my own wPNKC?)?
    # TODO TODO do same w/ 33b (adding it into 47a and 85a Hallem data, for DM3 and DM5,
    # respectively)?

    # TODO TODO add comment explaining circumstances when we wouldn't have this.  it
    # seems to be zero filled (presumably just b/c in wPNKC earlier, and i think that's
    # the case whether _use_matt_wPNKC is True or False). maybe just in non-hemibrain
    # stuff? can i assert it's always true and delete some of this code?
    # TODO TODO TODO only drop DA4m if it's not in wPNKC (which should only be if
    # _use_matt_wPNKC=False?)?
    #
    # We may have already implicitly dropped this in the zero-filling code
    # (if that code ran, and if wPNKC doesn't have DA4m in its columns)
    have_DA4m = 'DA4m' in sfr.index or 'DA4m' in orn_deltas.index

    # TODO replace by just checking one for have_DA4m def above, w/ an assertion the
    # indices are (still) equal here?
    if have_DA4m:
        assert 'DA4m' in sfr.index and 'DA4m' in orn_deltas.index

    # TODO delete
    # currently getting tripped by model_test.py case that passes in hallem orn_deltas
    #print(f'{have_DA4m=}')
    #if not have_DA4m:
    #    print()
    #    print('did not have DA4m in sfr.index. add comment explaining current input')
    #    import ipdb; ipdb.set_trace()
    #

    # TODO also only do if _use_matt_wPNKC=True (prat's seems to have DA4m...)?
    #if (hallem_input or tune_on_hallem) and have_DA4m:
    # TODO this aligned with what i want?
    # TODO revert to using wPNKC.columns instead of glomerulus_index, for clarity?
    if 'DA4m' not in glomerulus_index and have_DA4m:
        # TODO why was he dropping it tho? was it really just b/c it wasn't in (his
        # version of) hemibrain?
        # DA4m should be the glomerulus associated with receptor Or2a that Matt was
        # dropping.
        # TODO TODO TODO why was i doing this? delete? put behind descriptive flag at
        # least? if i didn't need to keep receptors in line w/ what's already in osm,
        # then why do the skipping above? if i did, then is this not gonna cause a
        # problem? is 2a (DA4m) actually something i wanted to remove? why?
        # (was it just b/c it [for some unclear reason] wasn't in matt's wPNKC?)

        # TODO maybe replace by just having wPNKC all 0 for DA4m in _use_matt_wPNKC
        # case, where i would need to fill in those zeros in wPNKC (which doesn't
        # already have DA4m (Or2a), i believe)? could be slightly less special-casey...?
        sfr = sfr[sfr.index != 'DA4m'].copy()
        orn_deltas = orn_deltas.loc[orn_deltas.index != 'DA4m'].copy()

        # TODO TODO also remove DA4m from orn_deltas_pre_filling?
        # (maybe just subset to what's in sfr/orn_deltas but not orn_deltas_pre_filling,
        # but down by usage of orn_deltas_pre_filling?)

    # TODO don't do if 'uniform' draw path (/ cxn_distrib, but check that there)?
    assert sfr.index.equals(orn_deltas.index)
    # TODO TODO warn here? this always OK?
    # TODO what is purpose here? comment explaining

    # TODO delete
    _wPNKC_shape_changed = False
    if wPNKC.shape != wPNKC[sfr.index].shape:
        print()
        print(f'wPNKC shape BEFORE subsetting to sfr.index: {wPNKC.shape}')
        _wPNKC_shape_changed = True
    #

    # TODO TODO also need to subset glomerulus_index here now? just always use
    # wPNKC.columns and remove glomerulus_index?
    wPNKC = wPNKC[sfr.index].copy()

    # TODO delete
    if _wPNKC_shape_changed:
        # TODO TODO is this only triggered IFF have_DA4m? move all this wPNKC stuff into
        # that conditional above?
        print(f'wPNKC shape AFTER subsetting to sfr.index: {wPNKC.shape}')
        print()
        print('NEED TO SUBSET GLOMERULUS_INDEX HERE (/ refactor to just use wPNKC)?')
        import ipdb; ipdb.set_trace()
        print()
    del _wPNKC_shape_changed
    #

    # TODO try removing .copy()?
    mp.orn.data.spont = sfr.copy()
    mp.orn.data.delta = orn_deltas.copy()

    # TODO need to remove DA4m (2a) from wPNKC first too (already out, it seems)?
    # don't see matt doing it in hemimat-modeling... (i don't think i need to.
    # rv.pn.pn_sims below had receptor-dim length of 22)

    # TODO also take an optional parameter to control this number?
    # (for variable_n_claws cases mainly)
    # TODO or if always gonna use wPNKC, option to use # from [one of] fafb data
    # sources (2482 in left), instead of hemibrain?
    mp.kc.N = len(wPNKC)

    if variable_n_claws:
        # TODO is seed actually only used in variable_n_claws=True cases?
        # (seems so, and doesn't seem to matter it is set right before KC sims)
        # TODO should seed be Optional?
        mp.kc.seed = seed
        mp.kc.nclaws = n_claws

    if pn2kc_connections in connectome_options:
        mp.kc.preset_wPNKC = True

    elif pn2kc_connections == 'hemidraw':
        # TODO support using wPNKC from fafb-left/fafb-right (currently just hemibrain,
        # w/ _use_matt_wPNKC=False. i.e. using the newer data from prat's query)?

        # TODO check index (glomeruli) is same as sfr/etc (all other things w/ glomeruli
        # that model uses)
        # TODO just set directly into mp.kc.cxn_distrib?
        # (and in other places that set this)
        cxn_distrib = wPNKC.sum()

        # TODO delete?
        if hallem_input:
            # TODO compute this from something?
            n_hallem_glomeruli = 23
            assert mp.kc.cxn_distrib.shape == (1, n_hallem_glomeruli)
        #

        # TODO TODO TODO what currently happens if using # glomeruli other > hallem?
        # seems like it may already be broken? (and also in uniform case. not sure if
        # this is why tho)
        #
        # TODO can we modify olfsysm to break if input shape is wrong? why does it work
        # for mp.orn.data.spont but not this? (shape of mp.orn.data.spont is (n, 1)
        # before, not (1, n) as this is)
        # (maybe it was fixed in commit that added allowdd option, and maybe that's why
        # i hadn't noticed it? or i just hadn't actually tested this path before?)
        #
        # NOTE: this reshaping (from (n_glomeruli,) to (1, n_glomeruli)) was critical
        # for correct output (at least w/ olfsysm.cpp from 0d23530f, before allowdd)
        mp.kc.cxn_distrib = cxn_distrib.to_frame().T
        assert mp.kc.cxn_distrib.shape == (1, len(cxn_distrib))

        wPNKC = None

    # NOTE: if i implement this, need to make sure cxn_distrib is getting reshaped as in
    # 'hemidraw' case above. was critical for correct behavior there.
    #elif pn2kc_connections == 'caron':
    #    # TODO could modify this (drop same index for 2a) if i wanted to use caron
    #    # distrib Of shape (1, 23), where 23 is from 24 Hallem ORs minus 33b probably?
    #    cxn_distrib = mp.kc.cxn_distrib[0, :].copy()
    #    assert len(cxn_distrib) == 23

    elif pn2kc_connections == 'uniform':
        mp.kc.uniform_pns = True

        wPNKC = None

    rv = osm.RunVars(mp)

    # TODO need delete=False?
    temp_log_file = NamedTemporaryFile(suffix='.olfsysm.log', delete=False)

    # also includes directory
    temp_log_path = temp_log_file.name

    if print_olfsysm_log is None:
        print_olfsysm_log = al_util.verbose

    if print_olfsysm_log:
        print(f'writing olfsysm log to {temp_log_path}')

    try:
        # TODO should i only be doing this right before running? it causing issues?
        #
        # it seems to just append to this file, if it already exists (should no longer
        # an issue now that I'm making temp files)
        rv.log.redirect(temp_log_path)

    # just so i can experiment w/ reverting to old olfsysm, before i added this
    except AttributeError:
        # TODO is this currently the path being taken?
        pass

    # may or may not care to relax this later
    # (so that we can let either be defined from one, or to try varying separately)
    if wAPLKC is None and wKCAPL is not None:
        raise NotImplementedError('wKCAPL can only be specified if wAPLKC is too')

    def _single_unique_val(arr: np.ndarray) -> float:
        """Returns single unique value from array.

        Raises AssertionError if array has more than one unique value (including NaNs).
        """
        unique_vals = set(np.unique(arr))
        assert len(unique_vals) == 1
        return unique_vals.pop()

    # TODO TODO double check olfsysm flag that used to be called enable_apl
    # actually only affected tuning, and that my hardcoded weights are still being used
    # TODO TODO double check olfsysm tuning is only changing the 3 params i'm
    # hardcoding (and especially in step that goes from 0.2 to 0.1 response rate)
    if wAPLKC is not None:
        assert target_sparsity is None
        assert target_sparsity_factor_pre_APL is None
        assert fixed_thr is not None, 'for now, assuming both passed if either is'

        mp.kc.tune_apl_weights = False

        # NOTE: min/max for these should all be the same. they are essentially scalars,
        # at least as tuned before
        # rv.kc.wKCAPL.shape=(1, 1630)
        # rv.kc.wKCAPL.max()=0.002386503067484662
        # rv.kc.wAPLKC.shape=(1630, 1)
        # rv.kc.wAPLKC.max()=3.8899999999999992
        #
        # TODO still need allclose/similar (at output)?
        # TODO TODO specify one by dividing other by / double(p.kc.N)
        #wAPLKC = 3.8899999999999992
        rv.kc.wAPLKC = np.ones((mp.kc.N, 1)) * wAPLKC

        # TODO TODO (delete) what is required to go from 0.2 to 0.1 response rate again
        # (and why am i having such a hard time finding other values that will do
        # that?)?

        if wKCAPL is not None:
            rv.kc.wKCAPL = np.ones((1, mp.kc.N)) * wKCAPL
        else:
            wKCAPL = wAPLKC / mp.kc.N
            # this shape should be correct (wAPLKC's shape is transposed, so defining
            # this from that matrix could have caused issues?). passing transpose of
            # this in does NOT work correctly (how to get model to recognize that?).
            rv.kc.wKCAPL = np.ones((1, mp.kc.N)) * wKCAPL

        # TODO try setting wAPLKC = 1 (or another reasonable constant), and only vary
        # wKCAPL?
        # (or probably vice versa, where wKCAPL = 1 / mp.kc.N, and wAPLKC varies)

        # TODO save/print APL activity (timecourse?) to check it's reasonable?
        # (but it's non-spiking... what is reasonable?)

    if pn2kc_connections in connectome_options:
        rv.kc.wPNKC = wPNKC

    osm.run_ORN_LN_sims(mp, rv)
    osm.run_PN_sims(mp, rv)

    before_any_tuning = time.time()

    # This is the only place where build_wPNKC and fit_sparseness are called, and they
    # are only called if the 3rd parameter (regen=) is True.
    osm.run_KC_sims(mp, rv, True)

    tuning_time_s = time.time() - before_any_tuning

    # TODO is it all zeros after the n_hallem odors?
    # TODO do responses to first n_hallem odors stay same after changing sim_only and
    # re-running below?
    # Of shape (n_kcs, n_odors). odors as columns, as elsewhere.
    responses = rv.kc.responses.copy()
    responses_after_tuning = responses.copy()

    if target_sparsity is not None:
        if len(mp.kc.tune_from) > 0:
            # mp.kc.tune_from is an empty list if not explicitly set
            sp_actual = responses[:, mp.kc.tune_from].mean()
        else:
            sp_actual = responses.mean()

        # NOTE: if this fails, may want to check if
        # (rv.kc.tuning_iters == mp.kc.max_iters)
        # (and increase if needed [/ add a check we haven't reached max_iters])
        #
        # matt's tuning loop runs while:
        # (abs(sp - p.kc.sp_target) > (p.kc.sp_acc * p.kc.sp_target)
        abs_sp_diff = abs(sp_actual - mp.kc.sp_target)
        rel_sp_diff = abs_sp_diff / mp.kc.sp_target
        # (if tune_apl_weights is hardcoded True above, this will likely fail)
        assert rel_sp_diff <= mp.kc.sp_acc

    spike_counts = rv.kc.spike_counts.copy()

    if extra_orn_deltas is not None:
        # TODO maybe just sim the last bit and concat to existing responses,
        # instead of re-running all (check equiv tho)
        #mp.sim_only = range(n_input_odors, n_input_odors + n_extra_odors)
        mp.sim_only = range(n_input_odors + n_extra_odors)

        osm.run_ORN_LN_sims(mp, rv)
        osm.run_PN_sims(mp, rv)
        # Don't want to do either build_wPNKC or fit_sparseness here (after tuning)
        osm.run_KC_sims(mp, rv, False)

        # TODO also .copy() for both here (or don't above), for consistency
        responses = rv.kc.responses
        spike_counts = rv.kc.spike_counts

        assert np.array_equal(
            responses_after_tuning[:, :n_input_odors], responses[:, :n_input_odors]
        )


    if tune_on_hallem and not hallem_input:
        # TODO TODO fix!
        # ./al_analysis.py -d pebbled -n 6f -t 2023-04-22 -e 2023-06-22 -s
        #   ijroi,intensity,corr
        # ...
        # fitting model (responses_to='pebbled', tune_on_hallem=True,
        #   drop_receptors_not_in_hallem=True, pn2kc_connections=hemibrain,
        #   target_sparsity=0.03)...
        # ...
        # AssertionError
        try:
            assert (responses[:, n_hallem_odors:] == 0).all()
        except AssertionError:
            print('TRIGGERED ABOVE ASSERTIONERROR')
            import ipdb; ipdb.set_trace()

        # TODO also assert in here that sim_odors is None or sim_odors == odor_index?
        # (or move that assertion, which should be somewhere above, outside other
        # conditionals)

        mp.sim_only = range(n_hallem_odors,
            n_hallem_odors + n_input_odors + n_extra_odors
        )

        osm.run_ORN_LN_sims(mp, rv)
        osm.run_PN_sims(mp, rv)

        # Don't want to do either build_wPNKC or fit_sparseness here (after tuning)
        osm.run_KC_sims(mp, rv, False)

        # TODO also .copy() for both here (or don't above), for consistency
        responses = rv.kc.responses
        spike_counts = rv.kc.spike_counts

        # TODO TODO in hallem/hemibrain (or hemidraw???) case, add fake MS / CO2 odor
        # responses (300Hz in each cognate glomerulus) -> check i can recreate S1D? i
        # assume it's all actually from hemibrain (despite what legend says)? ms input
        # actually is just 300Hz, or did they use hallem data there?

        assert np.array_equal(
            responses_after_tuning[:, :n_hallem_odors], responses[:, :n_hallem_odors]
        )

        # TODO also test where appended stuff has slightly diff number of odors than
        # hallem (maybe missing [one random/first/last] row?)
        responses = responses[:, n_hallem_odors:]
        spike_counts = spike_counts[:, n_hallem_odors:]


    temp_log_file.close()

    if print_olfsysm_log:
        print('olfsysm log:')
        log_txt = Path(temp_log_path).read_text()
        cprint(log_txt, 'light_yellow')

    Path(temp_log_path).unlink()

    try:
        # line that would trigger the AttributeError
        spont_in = rv.kc.spont_in

    # to allow trying older versions of olfsysm, that didn't have rv.kc.spont_in
    # (which is what this would be doing, if i WAS still `pass`-ing instead of
    # `raise`-ing below)
    except AttributeError:
        # was previously just `pass`-ing here, but don't think i need to support this
        # again moving forward.
        raise

    if fixed_thr is not None:
        # just checking what we set above hasn't changed
        assert mp.kc.fixed_thr == fixed_thr
        assert mp.kc.add_fixed_thr_to_spont == True
        # actually do need this. may or may not need thr_type='fixed' too
        assert mp.kc.use_fixed_thr == True
        assert mp.kc.thr_type == 'fixed'
        # TODO some assertion w/ spont_in here? should we be able to calculate fixed_thr
        # same way?
    else:
        thr = rv.kc.thr

        # TODO use _single_unique_val for this too?
        #
        # this should correspond to the thr_const variable inside
        # olfsysm.choose_KC_thresh_uniform (and can be set by passing as the fixed_thr
        # kwarg to this function, which will also set mp.kc.add_fixed_thr_to_spont=True)
        unique_fixed_thrs = np.unique(thr - 2*spont_in)
        # could take median instead? shouldn't really matter tho...
        fixed_thr = unique_fixed_thrs[0]
        assert np.allclose(fixed_thr, unique_fixed_thrs)

        # TODO return / put in input dict instead (/ too)?
        print(f'fixed_thr: {fixed_thr}')

    # these should either be the same as any hardcoded wAPLKC [+ wKCAPL] inputs, or the
    # values chosen by the tuning process. _single_unique_val will raise AssertionError
    # if the input arrays contain more than one unique value.
    rv_scalar_wAPLKC = _single_unique_val(rv.kc.wAPLKC)
    rv_scalar_wKCAPL = _single_unique_val(rv.kc.wKCAPL)

    if wAPLKC is not None:
        # TODO delete? just checking what we set above hasn't changed
        assert mp.kc.tune_apl_weights == False

        assert rv_scalar_wAPLKC == wAPLKC

        # this should now be defined whenever wAPLKC is, whether passed in or not...
        assert wKCAPL is not None
        assert rv_scalar_wKCAPL == wKCAPL
    else:
        # TODO delete prints?
        print(f'wAPLKC: {rv_scalar_wAPLKC}')
        print(f'wKCAPL: {rv_scalar_wKCAPL}')
        wAPLKC = rv_scalar_wAPLKC
        wKCAPL = rv_scalar_wKCAPL

    param_dict = {
        'fixed_thr': fixed_thr,
        'wAPLKC': wAPLKC,
        'wKCAPL': wKCAPL,

        'kc_spont_in': spont_in,
    }

    tuning_dict = {
        # TODO TODO expose at least these first two (sp_acc, max_iters) as kwargs.
        # prob also sp_lr_coeff.
        # TODO TODO TODO + maybe default to smaller tolerance (+ more iterations if
        # needed). what currently happens if tolerance not reached in max_iters?
        # add my own assertion (in this script) that we are w/in sp_acc?
        #
        # parameters relevant to model threshold + APL tuning process
        # default=0.1 (fraction +/- sp_target)
        'sp_acc': mp.kc.sp_acc,

        # default=10
        'max_iters': mp.kc.max_iters,

        'sp_lr_coeff': mp.kc.sp_lr_coeff,
        'apltune_subsample': mp.kc.apltune_subsample,

        # should be how many iterations it took to tune,
        'tuning_iters': rv.kc.tuning_iters,

        # removed tuning_time_s from this, because it would cause -c checks to fail
    }
    if fixed_thr is None:
        print('tuning parameters:')
        pprint(tuning_dict)
        print('tuning time: {tuning_time_s:.1f}s')
        print()

    param_dict = {**param_dict, **tuning_dict}

    assert responses.shape[1] == (n_input_odors + n_extra_odors)
    responses = pd.DataFrame(responses, columns=odor_index)
    responses.index.name = 'model_kc'

    assert spike_counts.shape[1] == (n_input_odors + n_extra_odors)
    assert len(responses) == len(spike_counts)
    spike_counts = pd.DataFrame(spike_counts, columns=odor_index)
    spike_counts.index.name = 'model_kc'

    if extra_orn_deltas is not None:
        extra_responses = responses.iloc[:, -n_extra_odors:]

        if 'panel' in extra_responses.columns.names:
            extra_responses = extra_responses.droplevel('panel', axis='columns')

        old_eb = responses.iloc[:, :-n_extra_odors].loc[:, eb_mask]
        if 'panel' in responses.columns.names:
            old_eb = old_eb.droplevel('panel', axis='columns')

        assert old_eb.shape[1] == 1
        old_eb = old_eb.iloc[:, 0]

        eb_idx = -1

        new_eb = extra_responses.iloc[:, eb_idx]
        assert new_eb.name.startswith('eb @')

        assert new_eb.equals(old_eb)

        # just removing eb, so there won't be that duplicate, which could cause some
        # problems later (did cause some of the plotting code in here to fail i think).
        # doesn't matter now that we know new and old are equal.
        responses = responses.iloc[:, :eb_idx].copy()
        spike_counts = spike_counts.iloc[:, :eb_idx].copy()

        # TODO delete? am i not removing 'eb' now anyway?
        if make_plots:
            # causes errors re: duplicate ticklabels in some of the orn_deltas plots
            # currently (would need to remove 'eb' from all of those plots, but also
            # prob want to remove all extra_orn_deltas odors for them. still need to
            # keep in returned responses tho)
            warn('fit_mb_model: setting make_plots=False since not currently supported '
                'in extra_orn_deltas case'
            )
        make_plots = False
        #

    if variable_n_claws:
        assert mp.kc.seed == seed

        wPNKC = rv.kc.wPNKC
        # TODO should these not also be the case in variable_n_claws == False case?
        # move these two assertions out?
        assert wPNKC.shape[1] == len(glomerulus_index)
        assert len(wPNKC) == len(responses)

        wPNKC = pd.DataFrame(data=wPNKC, index=responses.index,
            columns=glomerulus_index
        )

    # TODO why is this seemingly a list of arrays, while the equiv kc variable seems to
    # be an array immediately? binding code seems similar...
    orn_sims = np.array(rv.orn.sims)
    # orn_sims.shape=(110, 22, 5500)

    # orn_sims is of shape (n_odors, n_glomeruli, n_timepoints)
    n_samples = orn_sims.shape[-1]
    # from default parameters:
    # p.time.pre_start  = -2.0;
    # p.time.start      = -0.5;
    # p.time.end        = 0.75;
    # p.time.stim.start = 0.0;
    # p.time.stim.end   = 0.5;
    # p.time.dt         = 0.5e-3;
    assert (n_samples * mp.time_dt) == (mp.time_end - mp.time_pre_start)
    assert (mp.time_pre_start < mp.time_start < mp.time_stim_start < mp.time_stim_end <
        mp.time_end
    )

    # also a list out of the box
    # pn_sims.shape=(110, 22, 5500)
    pn_sims = np.array(rv.pn.pn_sims)
    assert pn_sims.shape[-1] == n_samples

    if tune_on_hallem and not hallem_input:
        orn_sims = orn_sims[n_hallem_odors:]
        pn_sims = pn_sims[n_hallem_odors:]

    ts = np.linspace(mp.time_pre_start, mp.time_end, num=n_samples)

    # TODO delete
    # units seems to be firing rates (absolute i think. actually, there are some
    # negative values, even in hallem_input=True [w/ matt config] case. that's not a
    # mistake though, is it?)
    '''
    dl5 = glomerulus_index.get_loc('DL5')
    t2h = odor_index.get_loc('t2h @ -2')

    fig, ax = plt.subplots()

    ax.plot(ts, orn_sims[t2h, dl5, :], label='ORN')
    ax.plot(ts, pn_sims[t2h, dl5, :], label='PN')

    ax.axvline(mp.time_stim_start, label='stim start')
    ax.axvline(mp.time_stim_end, label='stim end')

    ax.set_title('DL5 response to t2h')
    ax.legend()
    '''
    # TODO so should i average starting a bit after stim start? seems it still take
    # a bit to peak. what did matt do?
    # in (DL5, t2h) case, PN peak is ~0.047, and ORN plateaus after ~0.077

    stim_start_idx = np.searchsorted(ts, mp.time_stim_start)
    stim_end_idx = np.searchsorted(ts, mp.time_stim_end)

    # TODO stim_end_idx + 1?
    orn_df = pd.DataFrame(index=odor_index, columns=glomerulus_index,
        data=orn_sims[:, :, stim_start_idx:stim_end_idx].mean(axis=-1)
    )
    pn_df = pd.DataFrame(index=odor_index, columns=glomerulus_index,
        data=pn_sims[:, :, stim_start_idx:stim_end_idx].mean(axis=-1)
    )

    if sim_odors is not None:
        # TODO have parse_odor_name return input (rather than current ValueError), if
        # input doesn't have '@' in it (or add in model_test.py r1 call that currently
        # is failing b/c of this)
        input_odor_names = {olf.parse_odor_name(x) for x in sim_odors}
    else:
        input_odor_names = {
            olf.parse_odor_name(x) for x in odor_index.get_level_values('odor')
        }

    # TODO may want to discard some for some plots (e.g. in cases when input is not
    # hallem, but also has diagnostics / fake odors added in addition to megamat odors)?
    # (kinda like plots as is there actually, but may want to add lines to separate
    # megamat from rest?)
    #
    # OK if we have more odors (for Hallem input case, right?)
    megamat = len(megamat_odor_names - input_odor_names) == 0
    del input_odor_names
    if megamat:
        # TODO assert 'panel' only megamat if we do have it?
        panel = None if 'panel' in orn_deltas.columns.names else 'megamat'

        if not hallem_input:
            # at least as configured now, this isn't doing anything.
            # TODO just assert that all of orn_deltas_pre_filling.index are still in
            # orn_deltas.index for now (commentin this)?
            orn_deltas_pre_filling = orn_deltas_pre_filling.loc[
                [g for g in orn_deltas_pre_filling.index if g in orn_deltas.index]
            ].copy()

            # TODO can i rely on panel already being there now? just remove all this
            # sorting? (or sort orn_deltas before, unconditionally)
            # (i assume it's not there when input is hallem?)
            # (but still want to sort when input is hallem, adding megamat, so all
            # megamat odors are first)

            orn_deltas_pre_filling = sort_odors(orn_deltas_pre_filling, panel=panel,
                warn=False
            )

            # TODO is it a problem that i'm now also only are only doing all below if
            # `not hallem_input`? (change any outputs in my main al_analysis.py
            # remy-paper analyses?) changed to fix impact of sorting on check in
            # model_test.py, but could also add kwarg to disable this sorting...

            orn_deltas = sort_odors(orn_deltas, panel=panel, warn=False)

            # TODO maybe only do this one on a copy we don't return? probably don't
            # really care if it's already sorted tho...
            # TODO even care to do this? if just for a corr diff thing, even need?
            responses = sort_odors(responses, panel=panel, warn=False)
            spike_counts = sort_odors(spike_counts, panel=panel, warn=False)

            orn_df = sort_odors(orn_df, panel=panel, warn=False)
            pn_df = sort_odors(pn_df, panel=panel, warn=False)
    else:
        # TODO delete if i modify below to also make plots for other panels
        # (replacing w/ sorting -> dropping levels for all these, as above)
        #
        # the only place these should all be used is in the plotting code below, which
        # currently only runs in `megamat == True` case
        del orn_deltas, orn_df, pn_df

    # TODO still want?
    if 'panel' in responses.columns.names:
        assert 'panel' in spike_counts.columns.names
        responses = responses.droplevel('panel', axis='columns')
        spike_counts = spike_counts.droplevel('panel', axis='columns')

    # TODO probably also do for other panels?
    #if plot_dir is not None and make_plots:
    if (plot_dir is not None and make_plots) and megamat:
        orn_df = orn_df.T
        pn_df = pn_df.T

        # TODO probably also a seperate plot including any hallem deltas tuned on (but
        # not in responses that would be returned). not that i actualy use that path
        # now...

        plot_dir = plot_dir / 'model_internals'
        plot_dir.mkdir(exist_ok=True)

        fig, _ = viz.matshow(sfr.to_frame(), xtickrotation='horizontal',
            **diverging_cmap_kwargs
        )
        savefig(fig, plot_dir, 'sfr')

        # TODO rename to be clear it's just orn/pn stuff?
        def _plot_internal_responses_and_corrs(subset_fn=lambda x: x, suffix='',
            **plot_kws):

            # TODO TODO maybe subset these down to things that are still in
            # orn_deltas/sfr/wPNKC though?
            #
            # orn_deltas_pre_filling only defined in this case. otherwise, there
            # shouldn't really *be* any filling.
            if not hallem_input:
                orn_delta_prefill_corr = plot_responses_and_corr(
                    subset_fn(orn_deltas_pre_filling), plot_dir,
                    f'orn-deltas-prefill{suffix}', title=title, **plot_kws
                )
            else:
                orn_delta_prefill_corr = None

            # TODO also do orn_deltas + sfr?
            # (to see how that changes correlation)
            # TODO with and without filling? or just post filling?

            # TODO TODO why doesn't drop_nonhallem...=True
            # [and/or tune_on_hallem=True] not look better? try again? shouldn't input
            # not get correlated so much?
            # TODO TODO compare ORN correlations in that case (after dropping and
            # delta estimate) vs hallem correlations: to what extent are they different?
            # TODO TODO is it related to wPNKC handling? not returning to ~halfmat or
            # whatever if starting from hallem/hemibrain case?

            orn_delta_corr = plot_responses_and_corr(subset_fn(orn_deltas), plot_dir,
                f'orn-deltas{suffix}', title=title, **plot_kws
            )
            orn_corr = plot_responses_and_corr(subset_fn(orn_df), plot_dir,
                f'orns{suffix}', title=title, **plot_kws
            )
            # not going to subtract pn corrs from kc corrs, so don't need return value
            plot_responses_and_corr(subset_fn(pn_df), plot_dir, f'pns{suffix}',
                title=title, **plot_kws
            )

            # model KC corr + responses should be plotted externally. responses plotted
            # differently there too, clustering after dropping silent cells.
            return orn_delta_prefill_corr, orn_delta_corr, orn_corr

        # TODO TODO why in hallem_input cases do orn-deltas* outputs seem to be
        # pre-filling (and we have no separate orn-deltas_prefill* outputs). fix for
        # consistency?

        if hallem_input:
            # (not necessarily true in general? but probably for only case i actually
            # want to plot. might need to change some of these conditionals/assertions)
            assert orn_df.shape[1] == 110
            assert pn_df.shape[1] == 110
            # TODO delete?
            #assert sim_odors is not None

            def subset_sim_odors(df):
                # TODO delete? still necessary? seems we currently do this earlier
                if megamat:
                    df = sort_odors(df, panel='megamat', warn=False)
                #

                if sim_odors is None:
                    return df
                else:
                    return df.loc[:, [x in sim_odors for x in df.columns]]

            # assuming we won't be passing sim_odors for other cases (other than what?
            # which case is this? elaborate in comment), for now
            if sim_odors is not None:
                # TODO move this assertion into _plot_internal... ? why here?
                # (after sorting above, all these things should have matching columns)
                assert orn_deltas.columns.equals(orn_df.columns)
                _plot_internal_responses_and_corrs(subset_fn=subset_sim_odors)

            # to compare to figures in ann's paper, where 110 odors are in hallem order
            def resort_into_hallem_order(df):
                # don't need to worry about panel level being present on odor_index, as
                # it never is in hallem_input case (only place this is used)
                return df.loc[:, odor_index]

            # TODO TODO are these not being generated in latest hallem_input call
            # (restoring hemibrain path)?
            orn_delta_prefill_corr, orn_delta_corr, orn_corr = \
                _plot_internal_responses_and_corrs(suffix='_all-hallem',
                    subset_fn=resort_into_hallem_order
                )

            if megamat:
                responses = resort_into_hallem_order(responses).copy()
                spike_counts = resort_into_hallem_order(spike_counts).copy()
        else:
            orn_delta_prefill_corr, orn_delta_corr, orn_corr = \
                _plot_internal_responses_and_corrs()

        # TODO plot distribution of spike counts -> compare to w/ ann's outputs

        if hallem_input:
            suffix = '_all-hallem'
        else:
            suffix = ''

        # NOTE: this should be the only time the model KC responses are used inside the
        # plotting this fn does (and thus, the only time this flag is relevant here).
        # still returning silent cells regardless, so stuff can make this decision
        # downstream of response caching.
        if drop_silent_cells_before_analyses:
            title += _get_silent_cell_suffix(responses)
            model_kc_corr = drop_silent_model_cells(responses).corr()
            # drop_silent_model_cells should also work w/ spike count input
            spike_count_corr = drop_silent_model_cells(spike_counts).corr()
        else:
            model_kc_corr = responses.corr()
            spike_count_corr = spike_counts.corr()

        # for sanity checking some of the diffs. should also be saving this outside.
        plot_corr(model_kc_corr, plot_dir, f'kcs_corr{suffix}', title=title)
        #
        # TODO TODO also sanity check by extra plot_corr calls w/ orn_[delta_]corr
        # inputs (to check i'm using the right ones, etc)? should be exactly same as
        # orn-deltas_corr.pdf / orns_corr.pdf generated above.

        plot_corr(spike_count_corr, plot_dir, f'kcs_spike-count_corr{suffix}',
            title=title
        )

        if orn_delta_prefill_corr is not None:
            corr_diff_from_prefill_deltas = model_kc_corr - orn_delta_prefill_corr
            plot_corr(corr_diff_from_prefill_deltas, plot_dir,
                f'model_vs_orn-deltas-prefill_corr_diff{suffix}',
                title=title, xlabel=f'model KC - model ORN (deltas, pre-filling) corr'
            )

        # TODO keep the seperate versions comparing against orn-deltas vs average
        # of dynamic internal orns? actually ever diff?
        corr_diff_from_deltas = model_kc_corr - orn_delta_corr
        plot_corr(corr_diff_from_deltas, plot_dir,
            f'model_vs_orn-deltas_corr_diff{suffix}',
            title=title, xlabel=f'model KC - model ORN (deltas) corr'
        )

        corr_diff = model_kc_corr - orn_corr
        # the 'dyn' prefix is to differentiate from a plot saved in parent of plot_dir,
        # by other code.
        plot_corr(corr_diff, plot_dir, f'model_vs_dyn-orn_corr_diff{suffix}',
            title=title, xlabel=f'model KC - model ORN (avg of dynamics) corr'
        )

        if hallem_input:
            # TODO delete?
            #
            # for sanity checking some of the diffs. should also be saving this outside.
            model_kc_corr_only_megamat = subset_sim_odors(
                subset_sim_odors(model_kc_corr).T
            )
            plot_corr(model_kc_corr_only_megamat, plot_dir, 'kcs_corr', title=title)
            #

            spike_count_corr_only_megamat = subset_sim_odors(
                subset_sim_odors(spike_count_corr).T
            )
            plot_corr(spike_count_corr_only_megamat, plot_dir, 'kcs_spike-count_corr',
                title=title
            )

            # TODO also support subset_fn for plot_corr, rather than this kind of
            # subsetting?
            corr_diff_from_deltas_only_megamat = subset_sim_odors(
                subset_sim_odors(corr_diff_from_deltas).T
            )
            plot_corr(corr_diff_from_deltas_only_megamat, plot_dir,
                'model_vs_orn-deltas_corr_diff', title=title,
                xlabel=f'model KC - model ORN (deltas) corr'
            )

            corr_diff_only_megamat = subset_sim_odors(subset_sim_odors(corr_diff).T)
            plot_corr(corr_diff_only_megamat, plot_dir, 'model_vs_dyn-orn_corr_diff',
                title=title, xlabel=f'model KC - model ORN (avg of dynamics) corr'
            )

    # NOTE: currently doing after simulation, because i haven't yet implemented support
    # for tuning running on the full set of (hallem) odors, with subsequent simulation
    # running on a different set of stuff
    # TODO why checking sim_odors is not None if i'm just using hallem_sim_odors here?
    # this a mistake?
    if hallem_input and sim_odors is not None:
        assert all(x in responses.columns for x in hallem_sim_odors)
        # TODO delete (replace w/ setting up sim_only s.t. only hallem_sim_odors are
        # simulated)
        responses = responses[hallem_sim_odors].copy()
        spike_counts = spike_counts[hallem_sim_odors].copy()

        # TODO also print fraction of silent KCs here
        # (refactor that printing to an internal fn here)

        # TODO print out threshold(s) / inhibition? possible to summarize each? both
        # scalar? (may want to use these values from one run / tuning to parameterize
        # for more glomeruli / diff runs?)

    # TODO also return model if i can make it pickle-able (+ verify that. it's possible,
    # but not likely, that it can already be [de]serialized)
    #
    # TODO maybe in wPNKC index name clarify which connectome they came from (or
    # something similarly appropriate for each type of random draws)
    return responses, spike_counts, wPNKC, param_dict


n_seeds = 100

# TODO TODO and re-run whole script once implemented for those 2 (to compare
# sd / ('ci',95) / ('pi',95|50) for each)
#
# relevant for # odors vs fraction of KCs (response breadth) plot, as well
# as ORN vs KC correlation scatterplot.
#
# currently also used to show error across flies in plot_n_odors_per_cell
# (those plots will also have model seed errors shown in separate lines)
#
# +/- 1 SD (~68% of data, if normal. ('sd', 2) should be ~95% if normal).
# this should be same as ('sd', 1), if i understand docs correctly.
#seed_errorbar = 'sd'
#
# IQR (i.e. 25th - 75th percentile)
#seed_errorbar = ('pi', 50)
# TODO TODO also try iqr ('pi', 50), or other percentile based methods?
# default errorbar is ('ci', 95) (and this is what the preprint says it is
# plotting)
#
seed_errorbar = ('ci', 95)

# was at B'c request to make new 2E versions using ('ci', 95), taking the first 20
# (/100) seeds.
#
# TODO clarify. not sure yet if she wants me to handle other seed_errorbar plots
# this way too... (don't think we do)
# TODO revert to 20 (but maybe ignore for 3B scatterplots [and S1C?])
#n_first_seeds_for_errorbar = 20
n_first_seeds_for_errorbar = None

def _get_seed_err_text_and_fname_suffix(*, errorbar=seed_errorbar,
    n_first_seeds=n_first_seeds_for_errorbar):

    if errorbar is None:
        fname_suffix = ''
    elif type(errorbar) is not str:
        fname_suffix = f'_{"-".join([str(x) for x in errorbar])}'
    else:
        fname_suffix = f'_{errorbar}'

    # for use in plot titles / similar
    err_text = f'errorbar={errorbar}'

    if n_first_seeds is not None:
        fname_suffix += f'_{n_first_seeds}first-seeds-only'
        err_text += (f'\nonly analyzing first {n_first_seeds}/{n_seeds} '
            'seeds'
        )

    return err_text, fname_suffix


seed_err_text, seed_err_fname_suffix = _get_seed_err_text_and_fname_suffix()

# TODO factor to hong2p.util
# TODO use in other places that do something similar?
# TODO use monotonic / similar dynamic attributes if input is an appropriate pandas
# type (e.g. Index. is Series?)?
def is_sequential(data) -> bool:
    # works with np.ndarray input (and probably also pandas Series)
    #
    # NOTE: will not currently work w/ some other things I might want to use it on
    # (e.g. things that don't have  .min()/.max() methods)
    return set(range(data.min(), data.max() + 1)) == set(data)


def select_first_n_seeds(df: pd.DataFrame, *,
    n_first_seeds: Optional[int] = n_first_seeds_for_errorbar) -> pd.DataFrame:

    # assuming this function simply won't be called otherwise
    assert n_first_seeds is not None

    # assuming this fn only called on data w/ seed information (either as a column or
    # row index level)
    if 'seed' in df.columns:
        seed_vals = df.seed
    else:
        assert 'seed' in df.index.names
        seed_vals = df.index.get_level_values('seed')

    warn(f'subsetting model data to first {n_first_seeds} seeds!')

    first_n_seeds = seed_vals.sort_values().unique()[:n_first_seeds]
    assert seed_vals.min() == first_n_seeds.min() and is_sequential(first_n_seeds)

    # NOTE: not copy-ing. assuming caller won't try to mutate output w/o manually
    # .copy()-ing it first.
    subset = df[seed_vals.isin(first_n_seeds)]

    # wouldn't play nice if there were ever e.g. a diff number of cells per seed, but
    # that's not how it is now. this assertion isn't super important though, just a
    # sanity check.
    assert np.isclose(len(subset) / len(df), min(n_first_seeds, n_seeds) / n_seeds)

    return subset


def plot_n_odors_per_cell(responses, ax, *, ax_for_ylabel=None, title=None,
    label='# odors per cell', label_suffix='', color='blue', linestyle='-',
    log_yscale=False) -> None:

    # TODO say how many total cells (looks like 1630 in halfmat model now?)

    # 'stim' is what Remy binary responses currently has
    # 'odor' seems to be what I get from my saved responses pickles
    assert responses.columns.name in ('odor1', 'stim', 'odor')

    n_odors = responses.shape[1]

    n_odors_col = 'n_odors'
    frac_responding_col = 'frac_responding_to_n_odors'

    # need the +1 on stop to be inclusive of n_odors
    # (so we can have a bin for cells that respond to 0 odors, as well as bin for
    # cells that respond to all 110 odors)
    n_odor_index = pd.RangeIndex(0, (n_odors + 1), name=n_odors_col)

    lineplot_kws = dict(
        # TODO refactor to share (subset of) these w/ other plots using seed_errorbar?
        #
        # like 'white' more than 'None' for markerfacecolor here.
        marker='o', markerfacecolor='white', linestyle=linestyle, legend=False,
        ax=ax
    )

    label = f'{label}{label_suffix}'

    def _n_odors2frac_per_cell(n_odors_per_cell):
        # TODO delete sort_index? prob redundant since i'm reindexing below...
        #
        # this will be ordered w/ silent cells first,
        # cells responding to 1 odor 2nd, ...
        n_odors_per_cell_counts = n_odors_per_cell.value_counts().sort_index()
        # TODO delete? made irrelevant by reindex below (prob)?
        n_odors_per_cell_counts.name = n_odors_col

        # .at[0] raising a KeyError should have the same interpretation
        assert n_odors_per_cell_counts.at[0] > 0, ('plot would be wrong if input '
            'already had silent cells dropped'
        )

        assert n_odors_per_cell.sum() == (
            (n_odors_per_cell_counts.index * n_odors_per_cell_counts).sum()
        )

        # shouldn't really need .fillna(0), b/c either 0/NaN shouldn't show up in
        # (currently log scaled) plots.
        # TODO may want to keep anyway, in case i want to try switching off log scale?
        n_odors_per_cell_counts = n_odors_per_cell_counts.reindex(n_odor_index
            ).fillna(0)

        assert n_odors_per_cell_counts.sum() == len(n_odors_per_cell)

        frac_responding_to_n_odors = n_odors_per_cell_counts / len(n_odors_per_cell)
        frac_responding_to_n_odors.name = frac_responding_col

        # (was 0.9999999999999999 in some cases)
        assert np.isclose(frac_responding_to_n_odors.sum(), 1)

        return frac_responding_to_n_odors


    # NOTE: works whether responses contains {0.0, 1.0} or {False, True}
    assert set(np.unique(responses.values)) == {0, 1}
    # how many odors each cell responds to
    n_odors_per_cell = responses.sum(axis='columns')

    experimental_unit_opts = {remy_fly_id, 'seed'}

    experimental_unit_levels = set(responses.index.names) & experimental_unit_opts
    assert len(experimental_unit_levels) <= 1

    if len(experimental_unit_levels) > 0:
        experimental_unit = experimental_unit_levels.pop()

        if n_first_seeds_for_errorbar is not None and experimental_unit == 'seed':
            responses = select_first_n_seeds(responses)

        errorbar = seed_errorbar
        lineplot_kws['errorbar'] = errorbar
        lineplot_kws['seed'] = bootstrap_seed
        lineplot_kws['err_style'] = 'bars'

        # assuming each use of this fn will have at least SOME model data (true as of
        # 2024-08-06), otherwise may not want to always use `seed_err_text` which
        # sometimes has an extra line about only using first N seeds (when relevant
        # variable is set)
        if title is None:
            title = seed_err_text
        else:
            title += f'\n{seed_err_text}'

        lineplot_kws['x'] = n_odors_col
        lineplot_kws['y'] = frac_responding_col

        frac_responding_to_n_odors = n_odors_per_cell.groupby(level=experimental_unit
            ).apply(_n_odors2frac_per_cell)

        # TODO reset_index necessary?
        frac_responding_to_n_odors = frac_responding_to_n_odors.reset_index()

    # should only happen for modelling inputs w/ hemibrain wPNKC (as the other wPNKC
    # options should have a 'seed' level)
    else:
        frac_responding_to_n_odors = _n_odors2frac_per_cell(n_odors_per_cell)

    sns.lineplot(frac_responding_to_n_odors, label=label, color=color,
        markeredgecolor=color, **lineplot_kws
    )

    if log_yscale:
        # TODO increase if needed (i.e. if we ever use fafb wPNKC / cell #s, which have
        # 2482 in fafb-left, and probably similar # in right)
        n_cells_for_ylim = 2000

        # TODO this working w/ twinx() (seems to be, but why? why only need to use
        # ax_for_ylabel for ylabel, and not yscale / etc)?
        # TODO add comment explaining what nonpositive='mask' does (and are there any
        # alternatives? what, and why did i pick this?)
        # (i think nonpositive='clip' is the default, and the only alternative)
        ax.set_yscale('log', nonpositive='mask')

        # TODO try just using len(responses)? (would cause problems if that ever
        # differed across calls made on same Axes...)
        ax.set_ylim([1 / n_cells_for_ylim, 1])

    ylabel = 'cell fraction responding to N odors'
    if ax_for_ylabel is None:
        ax.set_ylabel(ylabel)
    else:
        ax_for_ylabel.set_ylabel(ylabel)

    # https://stackoverflow.com/questions/30914462
    ax.xaxis.set_major_locator(MaxNLocator(integer=True))

    xlabel = '# odors'
    ax.set_xlabel(xlabel)

    if title is not None:
        ax.set_title(title)


# TODO TODO still needed? don't i have some corr calc that resorts to input order?
# plot_corr?
# TODO try to fix corr calc to not re-order stuff (was that the issue?) -> delete
# this?
# NOTE: need to re-sort since corr_triangular necessarily (why? can't i have it sort
# to original order or something?) sorts internally
def _resort_corr(corr, add_panel, **kwargs):
    return sort_odors(corr, panel=add_panel, **kwargs)


# TODO factor to hong2p.viz
def add_unity_line(ax: Axes, *, linestyle='--', color='r', **kwargs) -> None:
    ax.axline((0, 0), slope=1, linestyle=linestyle, color=color, **kwargs)


# TODO delete? (for debugging)
_spear_inputs2dfs = dict()
#
def bootstrapped_corr(df: pd.DataFrame, x: str, y: str, *, n_resamples=1000,
    # TODO default to 95% ci?
    # TODO delete debug _plot_dir kwarg?
    ci=90, method='spearman', _plot_dir=None) -> str:
    # TODO update doc to include new values also returned:
    # corr_text, corr, ci_lower, ci_upper, pval
    """Returns str summary of Spearman's R between columns x and y.

    Summary contains Spearman's R, the associated p-value, and a bootstrapped 95% CI.
    """
    assert 0 < ci < 100, 'ci must be between 0 and 100'

    # TODO delete
    # (after replacing model_mb...  _spear_inputs2dfs usage w/ equiv corrs from loaded
    # responses)
    #
    # rhs check just to exclude hallem stuff i don't care about that is causing resort
    # to fail
    if (_plot_dir is not None and not _plot_dir.name.startswith('data_hallem__') and
        # should already have an equivalent 'orn_corr' version here (w/ corresponding
        # non-dist y too)
        y != 'orn_corr_dist'):

        assert method == 'spearman'

        key = (_plot_dir, x, y)
        assert key not in _spear_inputs2dfs, f'{key=} already seen!'
        _spear_inputs2dfs[key] = df.copy()

        pdf = df.copy()
        if pdf.index.names != ['odor1','odor2']:
            assert all(x in pdf.columns for x in ['odor1','odor2'])
            pdf = df.set_index(['odor1','odor2'])

        if x.endswith('_dist'):
            assert y.endswith('_dist')
            # converting back from correlation distance to correlation
            pdf[x] = 1 - pdf[x]
            pdf[y] = 1 - pdf[y]
        else:
            assert not y.endswith('_dist')

    to_check = df.copy()
    if x.endswith('_dist'):
        assert y.endswith('_dist')
        # converting back from correlation distance to correlation
        to_check[x] = 1 - to_check[x]
        to_check[y] = 1 - to_check[y]
    else:
        assert not y.endswith('_dist')

    assert to_check[x].max() <= 1, f'{x=} probably mislabelled correlation DISTANCE'
    assert to_check[y].max() <= 1, f'{y=} probably mislabelled correlation DISTANCE'
    del to_check

    if df[[x,y]].isna().any().any():
        # TODO fix NaN handling in method='pearson' case
        # (just dropna in all cases, and remove nan_policy arg to spearmanr)
        assert method == 'spearman', ('would need to restore NaN dropping. pearsonr '
            'does not have the same nan_policy arg spearmanr does.'
        )

        # TODO delete
        # only the Hallem cases (which dont' pass _plot_dir) should have any null model
        # corrs
        assert _plot_dir is None
        #
        assert x == 'model_corr' or x == 'model_corr_dist'
        assert not df[y].isna().any()
        # so that spearmanr doesn't return NaN here (dropping seems consistent w/ what
        # pandas calc does by default)
        #df = df.dropna(subset=[x])

    if method == 'spearman':
        # nan_policy='omit' consistent w/ pandas behavior. should only be relevant for a
        # small subset of the Hallem model outputs (default spearmanr behavior would be
        # to return NaN here)
        results = spearmanr(df[x], df[y], nan_policy='omit')

    elif method == 'pearson':
        # NOTE: no nan_policy arg here. would need to manually drop, as I had before.
        results = pearsonr(df[x], df[y])

    else:
        raise ValueError(f"{method=} unrecognized. should be either "
            "'spearman'/'pearson'"
        )

    corr = results.correlation
    pval = results.pvalue

    # the .at[x,y] is to get a scalar from matrix like
    #                mean_kc_corr  mean_orn_corr
    # mean_kc_corr       1.000000       0.657822
    # mean_orn_corr      0.657822       1.000000
    assert np.isclose(df[[x,y]].corr(method=method).at[x,y], corr)

    # TODO try this kind of CI as well?
    # https://stats.stackexchange.com/questions/18887
    # TODO try "jacknife" version mentioned in wikipedia? is my basic bootstrapping
    # approach even reasonable?

    # TODO tqdm? slow (when doing 1000) (yes! but tolerable)?
    result_list = []
    for i in range(n_resamples):
        resampled_df = df[[x, y]].sample(
                n=len(df), replace=True, random_state=(bootstrap_seed + i)
            ).reset_index(drop=True)

        if method == 'spearman':
            # TODO TODO also need nan_policy='omit' here? (or just drop in advance, to
            # also work in pearson case...)
            curr_results = spearmanr(resampled_df[x], resampled_df[y])
        elif method == 'pearson':
            curr_results = pearsonr(resampled_df[x], resampled_df[y])

        # (we would have raised an error already if method wasn't in one of two options
        # above)
        # pylint: disable=possibly-used-before-assignment
        result_list.append({
            'sample': i,
            method: curr_results.correlation,
            'pval': curr_results.pvalue,
        })

    bootstrap_corrs = pd.DataFrame(result_list)

    alpha = (1 - ci / 100) / 2

    corr_ci = bootstrap_corrs[method].quantile(q=[alpha, 1 - alpha])
    corr_ci_lower = corr_ci.iloc[0]
    corr_ci_upper = corr_ci.iloc[1]
    corr_ci_text = f'{ci:.0f}% CI = [{corr_ci_lower:.2f}, {corr_ci_upper:.2f}]'

    # TODO put n_resamples in text too?

    # .2E will show 2 places after decimal then exponent (scientific notation),
    # e.g. 1.89E-180
    corr_text = f'{method}={corr:.2f}, p={pval:.2E}, {corr_ci_text}'
    return corr_text, corr, corr_ci_lower, corr_ci_upper, pval


model_responses_cache_name = 'responses.p'
model_spikecounts_cache_name = 'spike_counts.p'

_fit_and_plot_seen_param_dirs = set()
# TODO why is sim_odors an explicit kwarg? just to not have included in strs describing
# model params? i already special case orn_deltas to exclude it. why not do something
# like that (if i keep the param at all)?
# TODO try to get [h|v]lines between components, 2-component mixes, and 5-component
# mix for new kiwi/control data (at least for responses and correlation matrices)
# (could just check for '+' character, to handle all cases)
def fit_and_plot_mb_model(plot_dir: Path, sensitivity_analysis: bool = False,
    sens_analysis_kws: Optional[Dict[str, Any]] = None, try_cache: bool = True,
    # TODO rename comparison_responses to indicate it's only used for sensitivity
    # analysis stuff? (and to be more clear how it differs from comparison_[kcs|orns])
    comparison_responses: Optional[pd.DataFrame] = None,
    n_seeds: int = 1, restrict_sparsity: bool = False,
    min_sparsity: float = 0.03, max_sparsity: float = 0.25,
    _in_sens_analysis: bool = False,
    # TODO just use model_kws for fixed_thr/wAPLKC?
    # (may now make sense, if i'm gonna add a flag to indicate whether we are in a
    # sensitivity analysis subcall)
    fixed_thr: Optional[float] = None, wAPLKC: Optional[float] = None,
    drop_silent_cells_before_analyses: bool = drop_silent_model_kcs,
    _add_combined_plot_legend=False, sim_odors=None, comparison_orns=None,
    comparison_kc_corrs=None, responses_to_suffix='',
    _strip_concs_comparison_kc_corrs=False, param_dir_prefix: str = '',
    title_prefix: str= '', extra_params: Optional[dict] = None,
    _only_return_params: bool = False, **model_kws) -> Optional[Dict[str, Any]]:
    # TODO doc which extra plots made by each of comparison* inputs (or which plots are
    # changed, if no new ones)
    """
    Args:
        plot_dir: parent to directory that will be created to contain model outputs and
            plots. created model output directories will have names incuding key
            parameters.

        sensitivity_analysis: if True, will run multiple versions of the model
            (saving output for each to a separte subdirectory), with `fixed_thr` and
            `wAPLKC` stepped around tuned values from primary model outputs.
            use `sens_analysis_kws` to control steps.

        try_cache: set False to force* any model cache to be ignored. Calls via
            `al_analysis.py` CLI with `-i model` will have the same effect.

        min_sparsity: (internal use only) only used for models parameterized with fixed
            `fixed_thr` and `wAPLKC` (typically in context of sensitivity analysis).
            return before generating plots if output sparsity is outside these bounds.

        max_sparsity: (internal use only) see min_sparsity.

        extra_params: saved alongside internal params in cache pickle/CSV
            (for keeping tracking of important external parameters, for reproducibility)

        **model_kws: passed to `fit_mb_model` (see its docstring, particularly for key
            inputs, such as `orn_deltas`)

    Returns dict with key model parameters (some tuned), any input `extra_params`, as
    well as 'output_dir' with name of created directory (which contains model outputs
    and plots).

    Notes:
    The contents of the `orn_deltas.csv` files copied to each model output directory
    should reflect the `model_kws['orn_deltas']` input to this function.
    """
    assert n_seeds >= 1

    # TODO delete restrict_sparsity? currently used?
    # TODO delete [min|max]_sparsity too?
    if not restrict_sparsity:
        min_sparsity = 0
        max_sparsity = 1

    # TODO delete. isn't responses_to just overwritten w/ 'pebbled' below
    # (before being used, right?)
    my_data = f'pebbled {dff_latex}'

    # TODO fix how this might misrepresent stuff if i pass hallem data in manually?
    # currently using responses_to_suffix to try to clarify it is in fact (modified)
    # hallem input in those cases

    if 'orn_deltas' in model_kws:
        responses_to = my_data
    else:
        responses_to = 'hallem'
    #

    # TODO also try tuning on remy's subset of hallem odors?
    # (did i already? is it clear that what's in preprint was not done this way?)

    # TODO share default w/ fit_mb_model somehow?
    tune_on_hallem = model_kws.get('tune_on_hallem', False)
    if tune_on_hallem:
        tune_from = 'hallem'
    else:
        tune_from = my_data
    del my_data

    pn2kc_connections = model_kws.get('pn2kc_connections', 'hemibrain')

    # TODO also use param_str for title? maybe just replace in the other direction, to
    # add dff_latex in pebbled case as necessary?). or just use filename only for many
    # parameters?

    # responses_to handled below, circa def of param_dir
    param_abbrevs = {
        'tune_on_hallem': 'hallem-tune',
        'pn2kc_connections': 'pn2kc',
        'target_sparsity': 'target_sp',
        'target_sparsity_factor_pre_APL': 'sp_factor_pre_APL',
        '_drop_glom_with_plus': 'drop-plusgloms',
    }
    exclude_params = ('orn_deltas', 'title', 'repro_preprint_s1d')
    # TODO sort params first? (so changing order in code doesn't cause
    # cache miss...)
    # TODO why adding [''] again? why not just prepend ', ' to output if i want?
    param_str = ', '.join([''] + [
        f'{param_abbrevs[k] if k in param_abbrevs else k}={v}'
        for k, v in model_kws.items() if k not in exclude_params
    ])
    if fixed_thr is not None or wAPLKC is not None:
        assert fixed_thr is not None and wAPLKC is not None

        # TODO maybe only do if _in_sens_analysis. don't think i actually want in
        # hemibrain case (other than within sens analysis subcalls)
        #
        # in n_seeds > 1 case, fixed_thr/wAPLKC will be lists of floats, and will be too
        # cumbersome to format into this
        if n_seeds == 1:
            param_str += f', fixed_thr={fixed_thr:.0f}, wAPLKC={wAPLKC:.2f}'

    if n_seeds > 1:
        param_str += f', n_seeds={n_seeds}'

    # TODO clean up / refactor. hack to make filename not atrocious when these are
    # 'pebbled_\$\\Delta_F_F\$'
    if responses_to.startswith('pebbled'):
        responses_to = 'pebbled'

    if tune_from.startswith('pebbled'):
        tune_from = 'pebbled'

    responses_to = f'{responses_to}{responses_to_suffix}'
    # TODO rename this kwarg (responses_to_suffix) to indicate it applies to tune_from
    # as well?
    if not tune_on_hallem:
        tune_from = f'{tune_from}{responses_to_suffix}'

    # this way it will also be included in params_for_csv, and we won't need to manually
    # pass to all fit_mb_model calls
    model_kws['drop_silent_cells_before_analyses'] = drop_silent_cells_before_analyses

    # TODO refactor so param_str defined from this, and then f-str below (+ for_dirname
    # def) doesn't separately specify {responses_to=}?
    params_for_csv = {
        'responses_to': responses_to,
        'tune_from': tune_from,
    }
    params_for_csv.update(
        {k: v for k, v in model_kws.items() if k not in exclude_params}
    )
    # prefix defaults to empty str
    title = title_prefix
    if _in_sens_analysis:
        assert fixed_thr is not None and wAPLKC is not None

        # assumed to be passed in (but not created by) sensitivity analysis calls
        # (recursive calls below)
        #
        # the parent directory of this should have plot_dir_prefix in it, and don't feel
        # the need to also include here.
        param_dir = plot_dir

        # TODO if i allow vector fixed_thr/wAPLKC, will need to special case here
        # TODO delete? should always be redefed below...
        # (if so, then why is this code even here?)
        #title += f'thr={fixed_thr:.2f}, wAPLKC={wAPLKC:.2f} (sparsity={sparsity:.2f})'
        title += f'thr={fixed_thr:.2f}, wAPLKC={wAPLKC:.2f}'
        title_including_silent_cells = title
    else:
        # TODO one hardcode flag to control whether these are dropped or not?
        # TODO TODO start excluding parts from dirname unless they are different from
        # some default? parts to consider excluding for default values (hardly change):
        # - dff_scale-to-avg-max
        # - data_pebbled
        # - hallem-tune_False
        # - target-sp_0.0915

        # TODO refactor for_dirname handling to not specialcase responses_to/others?
        # possible to have simple code not split by fixed_thr/wAPLKC None or not?
        # TODO need to pass thru util.to_filename / simliar normalization myself now
        # (since i'm putting this in a dirname now, not the final filename of the plot)
        for_dirname = f'data_{responses_to}'
        if len(param_str) > 0:
            for_dirname += '__'
            # TODO factor this into some fn? single flag (for_fname?) to
            # util.format_params to enable this kind of behavior?
            for_dirname += param_str.strip(', ').replace('_','-').replace(', ','__'
                ).replace('=','_')

        # TODO rename plot_dir + this to be more clear?
        # plot_dir contains all modelling (mb_modeling)
        # param_dir contains outputs from model run w/ specific choice of params
        # (and only contains stuff downstream of dF/F -> spiking model
        # creation/application)
        param_dir = plot_dir / f'{param_dir_prefix}{for_dirname}'
        del for_dirname

        # TODO will this title get cut off (about a 1/3rd of last (of 3) lines, yes)?
        # fix! (still?)
        title += (
            # TODO TODO be clear in the drop_nonhallem=True
            # (drop_receptors_not_in_hallem=True) case about the fact that each of these
            # is a subset?
            #
            # TODO TODO remove / condense these first two parts of title?
            # pretty much always the same these days... (may also want tune_from to be
            # able to indicate we tuned on kiwi+control data (can it currently?)
            # TODO TODO and do the same w/ param_str def, so dirnames aren't as
            # cluttered
            f'KC thresh [/APL inh] from: {tune_from}\n'
            # TODO even need this one? were hallem / pebbled (i.e. megamat) plots ever
            # possible to confuse?
            f'responses to: {responses_to}\n'

            f'wPNKC: {pn2kc_connections}\n'
        )

        weight_divisor = model_kws.get('weight_divisor')
        if weight_divisor is not None:
            title += f'weight_divisor: {weight_divisor:.1f}\n'

        if fixed_thr is not None or wAPLKC is not None:
            assert fixed_thr is not None and wAPLKC is not None
            # TODO assert target_sparsity_factor_pre_APL is None?

            # in n_seeds > 1 case, fixed_thr/wAPLKC will be lists of floats, and will be
            # too cumbersome to format into this
            if n_seeds == 1:
                title += f'fixed_thr={fixed_thr:.0f}, wAPLKC={wAPLKC:.2f}\n'
        else:
            if 'target_sparsity' in model_kws:
                assert model_kws['target_sparsity'] is not None
                target_sparsity = model_kws['target_sparsity']
            else:
                target_sparsity = 0.1
                warn(f'using default target_sparsity of {target_sparsity:.3g}')

            assert fixed_thr is None and wAPLKC is None
            # .3g will show up to 3 sig figs (regardless of their position wrt decimal
            # point), but also strip any trailing 0s (0.0915 -> '0.0915', 0.1 -> '0.1')
            title += f'target_sparsity: {target_sparsity:.3g}\n'

        # NOTE: this is for analyses that either always include or always drop silent
        # cells, regardless of value of `drop_silent_cells_before_analyses`
        # (e.g. should be used for analyses using `responses_including_silent_cells`)
        # TODO move this below too? (to where title is updated w/ silent cells suffix)
        title_including_silent_cells = title

        # to save plots of internal ORN / PN matrices (and their correlations, etc),
        # exactly as used to run model
        model_kws['plot_dir'] = param_dir
        model_kws['title'] = title
    #

    params_for_csv['output_dir'] = param_dir.name

    model_responses_cache = param_dir / model_responses_cache_name
    model_spikecounts_cache = param_dir / model_spikecounts_cache_name
    # TODO rename this to have "fit"/"tuned" in name or something (and change
    # model_mb_responses `tuned` var/outputs to not), since this is all tuned, and
    # latter is a mix (stuff from this, but also stuff hardcoded from above / output
    # statistics)
    param_cache_name = 'params_for_csv.p'
    param_dict_cache = param_dir / param_cache_name
    wPNKC_cache_name = 'wPNKC.p'
    wPNKC_cache = param_dir / wPNKC_cache_name
    kc_spont_in_cache = param_dir / 'kc_spont_in.p'
    made_param_dir = False

    extra_responses_cache_name = 'extra_responses.p'
    extra_responses_cache = param_dir / extra_responses_cache_name
    extra_responses = None

    extra_spikecounts_cache_name = 'extra_spikecounts.p'
    extra_spikecounts_cache = param_dir / extra_spikecounts_cache_name
    extra_spikecounts = None

    use_cache = try_cache and (not should_ignore_existing('model')) and (
        # checking both since i had previously only been returning+saving the 1st
        model_responses_cache.exists() and model_spikecounts_cache.exists()
    )

    tuning_output_dir = None
    # TODO delete? or implement somewhere else? (maybe just add flag to force ignore on
    # certain calls, and handle in model_mb...?)
    # TODO refactor def of 'tuning_output_dir' str
    if (extra_params is not None and 'tuning_output_dir' in extra_params and
        # NOTE: currently code in this conditional not working on _in_sens_analysis=True
        # subcalls, and we don't need anything defined in here in any of those cases
        # anyway
        not _in_sens_analysis):

        assert 'tuning_panels' in extra_params
        tuning_panels_str = extra_params['tuning_panels']

        # e.g. plot_dir=PosixPath('pebbled_6f/pdf/ijroi/mb_modeling/kiwi') ->
        # tuning_panel_dir=PosixPath('pebbled_6f/pdf/ijroi/mb_modeling/control-kiwi')
        tuning_panel_dir = plot_dir.parent / tuning_panels_str
        # NOTE: before i added `not _in_sens_analysis` condition, this was tripped in
        # those subcalls
        assert tuning_panel_dir.is_dir()

        tuning_output_dir = tuning_panel_dir / extra_params['tuning_output_dir']
        assert tuning_output_dir.is_dir()

        tuning_responses_cache = tuning_output_dir / model_responses_cache_name
        assert tuning_responses_cache.exists()

        # TODO delete? doesn't really matter unless fixed_thr/wAPLKC actually changed,
        # right? isn't that what i should be testing?
        if model_responses_cache.exists():
            curr_cache_mtime = getmtime(model_responses_cache)
            tuning_cache_mtime = getmtime(tuning_responses_cache)

            if tuning_cache_mtime >= curr_cache_mtime:
                warn(f'{tuning_responses_cache} was newer than {model_responses_cache}'
                    '! setting use_cache=False!'
                )
                use_cache = False

        if param_dict_cache.exists():
            param_dict = read_pickle(param_dict_cache)

            # np.array_equal works with both float and list-of-float inputs
            if (not np.array_equal(fixed_thr, param_dict['fixed_thr']) or
                not np.array_equal(wAPLKC, param_dict['wAPLKC'])
                ):

                warn(f'{param_dict_cache} fixed_thr/wAPLKC did not match current '
                    'inputs! setting use_cache=False!'
                )
                use_cache = False
        else:
            assert not use_cache

        # TODO also check that cached params references same tuning_output_dir (and set
        # use_cache = False if not)? or just assert it's same if already in cache?
        # NOTE: would have to load the param CSV instead of the pickle. the pickle
        # doesn't have those extra params
    #

    # TODO give better explanation as to why this is here.
    # to make sure we are accounting for all parameters we might vary in filename
    if param_dir in _fit_and_plot_seen_param_dirs:
        # otherwise, param_dir being in seen set would indicate an error
        assert _only_return_params, f'{param_dir=} already seen!'
        use_cache = True

    _fit_and_plot_seen_param_dirs.add(param_dir)

    # NOTE: this currently will cause -c/-C checks to fail
    # TODO TODO want to fix that (i.e. remove this from that output, but still keep long
    # enough to use for what i wanted? possible?)?
    params_for_csv['used_model_cache'] = use_cache

    print()
    # TODO TODO default to also skipping any plots made before returning? maybe add
    # another ignore-existing option ('model-plots'?) if i really want to be able to
    # remake plots w/o changing model outputs? takes a lot of time to make plots on all
    # the model outputs...
    if use_cache:
        print(f'loading model responses (+params) from cache {model_responses_cache}')
        # TODO why using my read_pickle wrapper for only some of these?
        responses = pd.read_pickle(model_responses_cache)
        spike_counts = pd.read_pickle(model_spikecounts_cache)
        param_dict = read_pickle(param_dict_cache)

        if extra_responses_cache.exists():
            extra_responses = pd.read_pickle(extra_responses_cache)

        if extra_spikecounts_cache.exists():
            extra_spikecounts = pd.read_pickle(extra_spikecounts_cache)
    else:
        # doesn't necessarily matter if it already existed. will be deleted if sparsity
        # outside bounds (and inside a sensitivity analysis call)
        made_param_dir = True

        # TODO use makedirs instead? (so if empty at end, will be deleted?)
        param_dir.mkdir(exist_ok=True, parents=True)

        print(f'fitting model ({responses_to=}{param_str})...', flush=True)

        # TODO check i can replace model_test.py portion like this w/ this
        # implementation?
        if n_seeds > 1:
            assert fixed_thr is None or type(fixed_thr) is list
            assert wAPLKC is None or type(wAPLKC) is list

            # only to regenerate model internal plots (which only ever are saved on the
            # first seed, in cases where there would be multiple runs w/ diff seeds)
            # without waiting for rest of seed runs to finish. will NOT write to
            # responses cache or make any plots based on output responses in this case!
            #first_seed_only = True
            first_seed_only = False
            if first_seed_only:
                # first_seed_only=True only intended for regenerating these internal
                # plots. probably a mistake if it's True any other time.
                assert 'plot_dir' in model_kws

            # TODO make kwarg
            # same seed Matt starts at in
            # matt-modeling/docs/independent-draw-reference.html
            initial_seed = 94894 + 1

            # TODO get good desc for tqdm
            #desc=f'{draw_type} ({n_claws=})'
            seeds = []
            responses_list = []
            spikecounts_list = []
            param_dict_list = []
            first_param_dict = None
            wPNKC_list = []

            _fixed_thr = None
            _wAPLKC = None

            if fixed_thr is not None:
                # this branch should not run in any sensitivity analyis subcalls, as
                # currently only doing that for n_seeds=1 (i.e. hemibrain) case
                # (otherwise, we would need to test if tuning_output_dir is None / etc)
                assert not _in_sens_analysis

                assert len(fixed_thr) == len(wAPLKC) == n_seeds

                assert tuning_output_dir is not None

                tuning_wPNKC_cache = tuning_output_dir / wPNKC_cache_name
                assert tuning_wPNKC_cache.exists()

                tuning_wPNKC = read_pickle(tuning_wPNKC_cache)
                # assuming all entries of a given seed are at adjacent indices in the
                # seed level values (should never be False given how i'm implementing
                # things)
                tuning_seeds = tuning_wPNKC.index.get_level_values('seed').unique()

            # TODO TODO include at least pn2kc_... in progress bar
            # TODO some way to have a nested progress bar, so that outer on (in
            # model_mb_... i'm imagining) increments for each model type, and this inner
            # one increments for each seed? or do something else to indicate outer
            # progress?
            for i in tqdm(range(n_seeds), unit='seed'):
                seed = initial_seed + i
                seeds.append(seed)
                assert 'seed' not in model_kws

                if fixed_thr is not None:
                    _fixed_thr = fixed_thr[i]
                    _wAPLKC = wAPLKC[i]
                    # would need to use same seed sequence if this ever failed
                    assert seed == tuning_seeds[i]

                responses, spike_counts, wPNKC, param_dict = fit_mb_model(
                    # TODO or can i handle fixed_thr/wAPLKC thru model_kws (prob not)?
                    # (maybe i will soon be able to, if i'm gonna replace some of their
                    # usage with a flag to indicate whether we are in a sensitivity
                    # analysis subcall...)
                    sim_odors=sim_odors, fixed_thr=_fixed_thr, wAPLKC=_wAPLKC,
                    seed=seed,
                    # ORN/PN plots would be redundant, and overwrite each other.
                    # currently those are the only plots I'm making in here.
                    make_plots=(i == 0), **model_kws
                )

                if fixed_thr is not None:
                    # could prob delete. should be sufficienet to check the seeds equal,
                    # as we are doing above
                    assert tuning_wPNKC.loc[seed].equals(wPNKC)

                if first_seed_only:
                    warn('stopping after model run with first seed '
                        '(first_seed_only=True)! model response caches / downstream '
                        'plots not updated!'
                    )
                    return None

                responses = util.addlevel(responses, 'seed', seed)
                spike_counts = util.addlevel(spike_counts, 'seed', seed)

                # TODO assert order of wPNKC columns same in each?
                wPNKC = util.addlevel(wPNKC, 'seed', seed)

                if first_param_dict is None:
                    first_param_dict = param_dict
                else:
                    assert param_dict.keys() == first_param_dict.keys()

                responses_list.append(responses)
                spikecounts_list.append(spike_counts)
                param_dict_list.append(param_dict)

                wPNKC_list.append(wPNKC)

            responses = pd.concat(responses_list, verify_integrity=True)
            spike_counts = pd.concat(spikecounts_list, verify_integrity=True)
            wPNKC = pd.concat(wPNKC_list, verify_integrity=True)

            param_dict = {
                k: [x[k] for x in param_dict_list] for k in first_param_dict.keys()
            }
        else:
            # isinstance works w/ both float and np.float64 (but not int)
            assert fixed_thr is None or isinstance(fixed_thr, float)
            assert wAPLKC is None or isinstance(wAPLKC, float)

            # TODO rename param_dict everywhere -> tuned_params?
            responses, spike_counts, wPNKC, param_dict = fit_mb_model(
                # TODO or can i handle fixed_thr/wAPLKC thru model_kws (prob not)?
                # (maybe i will soon be able to, if i'm gonna replace some of their
                # usage with a flag to indicate whether we are in a sensitivity analysis
                # subcall...)
                sim_odors=sim_odors, fixed_thr=fixed_thr, wAPLKC=wAPLKC, **model_kws
            )

        print('done', flush=True)

        orn_deltas = None
        if responses_to != 'hallem':
            orn_deltas = model_kws['orn_deltas']
            input_odors = orn_deltas.columns
        else:
            # NOTE: not saving model input (the Hallem ORN deltas) here, b/c it's added
            # by fit_mb_model internally, and it should be safe to assume this will not
            # change across runs. If it does change, hopefully the history of that is
            # accurately reflected in commit history of my drosolf repo.
            # TODO maybe refactor so i can define orn_deltas for this case too (and thus
            # so it's also saved below in that case)?
            n_hallem_odors = 110
            assert responses.shape[1] >= n_hallem_odors
            input_odors = responses.columns[:n_hallem_odors]

        # remove any odors added by `extra_orn_deltas` code (internal to fit_mb_model)
        if len(input_odors) < responses.shape[1]:
            if 'panel' in input_odors.names:
                input_odors = input_odors.droplevel('panel')

            assert responses.columns[:len(input_odors)].equals(input_odors)
            # (defined as None above)
            extra_responses = responses.iloc[:, len(input_odors):].copy()
            extra_spikecounts = spike_counts.iloc[:, len(input_odors):].copy()

            responses = responses.iloc[:, :len(input_odors)].copy()
            spike_counts = spike_counts.iloc[:, :len(input_odors)].copy()

        del input_odors

        # TODO TODO also pass in + save a copy of full input ORN data (same as in
        # ij_certain-roi_stats.[csv|p], maybe just load that in here, to not need to
        # pass? assuming mtime is since the start of run?). or just shutil copy
        # ij_certain-roi_stats.[csv+p]?
        if orn_deltas is not None:
            # just saving these for manual reference, or for use in -c check.
            # not loaded elsewhere in the code.
            to_pickle(orn_deltas, param_dir / 'orn_deltas.p')

            # TODO also save a hemibrain-filled version of this?
            #
            # current format like:
            # panel	megamat	         megamat          ...
            # odor	2h @ -3	         IaA @ -3         ...
            # glomerulus
            # D	40.845711426286  37.2453183810278 ...
            # DA2	15.325702916103	 11.4666387062239 ...
            # ...
            to_csv(orn_deltas, param_dir / 'orn_deltas.csv')

        # NOTE: saving raw (unsorted, etc) responses to cache for now, so i can modify
        # that bit. CSV saving is currently after all sorting / post-processing.
        to_pickle(responses, model_responses_cache)
        to_pickle(spike_counts, model_spikecounts_cache)
        to_pickle(wPNKC, wPNKC_cache)

        # TODO just save w/ param_dict? poping to have this make old outputs trip -c/-C
        # checks
        # TODO edit earlier (in fit_mb_model?) to be a pandas object w/ same cell
        # labels, rather than just numpy array?
        kc_spont_in = param_dict.pop('kc_spont_in')
        # TODO convert to pandas series before saving? did seem to get loading of this
        # working in natmix_data/analysis.py (which is only place that uses this now)
        # tho...
        to_pickle(kc_spont_in, kc_spont_in_cache)

        # TODO update this comment? i think param_dict might have a lot more stuff
        # now...
        #
        # in n_seeds=1 case, param_dict keys are:
        # 'fixed_thr', 'wAPLKC', and 'wKCAPL' (all w/ scalar values)
        #
        # in n_seeds > 1 case, should be same keys, but list values (of length equal to
        # n_seeds)
        to_pickle(param_dict, param_dict_cache)

        # TODO don't save in sensitivity analysis subcalls? as this should not change
        # across those
        to_csv(wPNKC, param_dir / 'wPNKC.csv', verbose=(not _in_sens_analysis))

        # saving after all the other things, so that (if script run w/ -c) checks
        # against old/new outputs have an opportunity to trip and fail before this is
        # written
        if extra_responses is not None:
            assert extra_spikecounts is not None
            to_pickle(extra_responses, extra_responses_cache)
            to_pickle(extra_spikecounts, extra_spikecounts_cache)
        else:
            assert extra_spikecounts is None

            # delete any existing extra_responses pickles
            # (don't want stale versions of these being loaded alongside newer
            # responses.p data)
            if extra_responses_cache.exists():
                extra_responses_cache.unlink()

            if extra_spikecounts_cache.exists():
                extra_spikecounts_cache.unlink()

    # param_dict should include 'fixed_thr', 'wAPLKC' and 'wKCAPL' parameters, as
    # they are at the end of the model run (either tuned or
    # hardcoded-from-the-beginning)
    # TODO just assert the things mentioned in comment above are there in
    # param_dict?
    assert not any(k in params_for_csv for k in param_dict.keys())
    params_for_csv.update(param_dict)

    # NOTE: if there were ever different number of cells for the different seeds (in the
    # cases where the row index has a 'seed' level, in addition to the 'cell' level,
    # e.g. the pn2kc_connections='uniform' case), then we'd want to compute sparsities
    # within seeds and then average those (to not weight different MB instantiations
    # differently, which is consistent w/ how Remy mean sparsity computed on real fly
    # data).
    sparsity = (responses > 0).mean().mean()
    params_for_csv['sparsity'] = sparsity

    # TODO factor out this subsetting to (internal?) fn? or just use megamat_responses
    # directly below?
    # TODO .get_level_values if i restore panel level preservation thru fit_mb_model
    megamat_mask = responses.columns.map(odor_is_megamat)

    # should be true in both hallem (which has ~110 odors, including all the 17
    # megamat) and pebbled-megamat input cases
    have_megamat = megamat_mask.values.sum() >= 17

    if have_megamat:
        megamat_responses = responses.loc[:, megamat_mask]
        megamat_sparsity = (megamat_responses > 0).mean().mean()
        del megamat_responses
        params_for_csv['megamat_sparsity'] = megamat_sparsity

    if extra_params is not None:
        assert not any(k in params_for_csv for k in extra_params.keys())
        params_for_csv.update(extra_params)

    # TODO does param_series have anything useful for repro that we dont have in
    # params_for_csv.p (param_dict_cache, saved earlier)?
    param_series = pd.Series(params_for_csv)
    try:
        # just to manually inspect all relevant parameters for outputs in a given
        # param_dir
        to_csv(param_series, param_dir / 'params.csv', header=False,
            verbose=(not _in_sens_analysis)
        )

    # TODO change code to avoid this happening in the first place?
    # (should only happen on second call used for getting inh params on a panel set, to
    # then run a model with a single panel and those inh params later)
    except MultipleSavesPerRunException:
        if _only_return_params:
            return params_for_csv
        else:
            raise

    del param_series

    # TODO even need to rename at this point? anything downstream actually not work with
    # 'odor' instead of 'odor1'?
    # TODO just fix natmix.plot_corr to also work w/ level named 'odor'?
    # (or maybe odor_corr_frame_to_dataarray?)
    #
    # even if input to fit_mb_model has a 'panel' level on odor index, the output will
    # not
    assert len(responses.columns.shape) == 1 and responses.columns.name == 'odor'
    responses.columns.name = 'odor1'

    assert len(spike_counts.columns.shape) == 1 and spike_counts.columns.name == 'odor'
    spike_counts.columns.name = 'odor1'

    panel = None
    if responses_to == 'hallem':
        assert have_megamat
        # the non-megamat odors will just be sorted to end
        panel = 'megamat'
    else:
        orn_deltas = model_kws['orn_deltas']
        assert 'panel' in orn_deltas.columns.names

        panels = set(orn_deltas.columns.get_level_values('panel'))
        del orn_deltas

        if len(panels) == 1:
            panel = panels.pop()
            assert type(panel) is str
        else:
            # should currently only be true in the calls w/ multiple panel inputs (e.g.
            # for pre-tuning on kiwi+control, to then run this fn w/ just kiwi input).
            # just gonna return early w/ params, skipping this stuff, fow now.
            panel = None

    if panel is not None:
        responses = sort_odors(responses, panel=panel, warn=False)
        spike_counts = sort_odors(spike_counts, panel=panel, warn=False)

    # TODO update these wrappers to also make dir if not exist (if they don't already)
    to_csv(responses, param_dir / 'responses.csv', verbose=(not _in_sens_analysis))
    to_csv(spike_counts, param_dir / 'spike_counts.csv',
        verbose=(not _in_sens_analysis)
    )

    if _only_return_params:
        return params_for_csv

    # TODO delete? should be handled by _only_return_params (cases they are triggered
    # should be the same)
    if panel is None:
        # TODO is there any code below that actually doesn't work w/ multiple panels?
        # care to get plots (prob not)?
        warn('returning from fit_and_plot_model before making plots, because input had'
            ' multiple panels (currently unsupported)'
        )
        return params_for_csv
    #

    # TODO use one/both of these col defs outside of just for s1d?
    odor_col = 'odor1'
    sparsity_col = 'response rate'

    def _per_odor_tidy_model_response_rates(responses: pd.DataFrame) -> pd.DataFrame:
        """Returns dataframe with [odor_col, sparsity_col [, 'seed']] columns.

        Returned dataframe also has a 'seed' column, if input index has a 'seed' level,
        with response rates computed within each 'seed' value in input.
        """
        # TODO warn / err if there are no silent cells in responses (would almost
        # certainly indicate mistake in calling code)?

        if 'seed' in responses.index.names:
            response_rates = responses.groupby('seed', sort=False).mean()
            assert response_rates.columns.name == odor_col
            assert response_rates.index.name == 'seed'

            response_rates = response_rates.melt(value_name=sparsity_col,
                # ignore_index=False to keep seed
                ignore_index=False
            ).reset_index()

            assert 'seed' in response_rates.columns
            assert odor_col in response_rates.columns
        else:
            response_rates = responses.mean()
            assert response_rates.index.name == odor_col
            response_rates = response_rates.reset_index(name=sparsity_col)

        return response_rates


    # TODO rename to plot_and_save... / something? consistent way to indicate which of
    # my plotting fns (also) save, and which do not?
    # TODO refactor to use this for s1d (maybe w/ boxplot=True option or something?
    # requiring box plot if there are multiple seeds on input?)?
    # TODO move def outside of fit_and_plot... (near plot_n_odors_per_cell def?)?
    def plot_sparsity_per_odor(sparsity_per_odor, comparison_sparsity_per_odor, suffix,
        *, ylim=None) -> Tuple[Figure, Axes]:

        fig, ax = plt.subplots()
        # TODO rename sparsity -> response_fraction in all variables / col names too
        # (or 'response rate'/response_rate, now in col def for s1d?)
        ylabel = 'response fraction'

        title = title_including_silent_cells

        err_kws = dict()
        if 'seed' in sparsity_per_odor.columns:
            assert n_first_seeds_for_errorbar is None, 'implement here if using'

            # TODO factor (subset of?) these kws into a seed_errorbar_style_kws or
            # something? to share these w/ plot_n_odors_per_cell (+ other places that
            # should use same errorbar style)
            #
            # TODO have markerfacecolor='None', whether or not we want to show
            # errorbars (maybe after a -c check that hemibrain stuff unchanged w/o)?
            err_kws = dict(markerfacecolor='white', errorbar=seed_errorbar,
                seed=bootstrap_seed, err_style='bars'
            )
            # TODO refactor to share w/ place copied from (plot_n_odors_per_cell)?
            if title is None:
                title = seed_err_text
            else:
                title += f'\n{seed_err_text}'

        color = 'blue'
        sns.lineplot(sparsity_per_odor, x=odor_col, y=sparsity_col, color=color,
            marker='o', markeredgecolor=color, legend=False, label=ylabel, ax=ax,
            **err_kws
        )
        if comparison_sparsity_per_odor is not None:
            # TODO how to label this? label='tuned'?
            color = 'gray'
            sns.lineplot(comparison_sparsity_per_odor, x=odor_col, y=sparsity_col,
                color=color, marker='o', markeredgecolor=color, legend=False,
                label=f'{ylabel} (tuned)', ax=ax, **err_kws
            )

        # renaming from column name odor_col
        ax.set_xlabel('odor'
            f'\nmean response rate: {sparsity_per_odor[sparsity_col].mean():.3g}'
        )

        # TODO add dotted line for target sparsity, when applicable?

        rotate_xticklabels(ax, 90)

        ax.set_title(title)
        ax.set_ylabel(ylabel)

        if ylim is not None:
            ymin, ymax = ylim

            response_rates = sparsity_per_odor[sparsity_col]
            assert (response_rates >= ymin).all(), f'{response_rates.min()=}'
            assert (response_rates <= ymax).all(), f'{response_rates.max()=}'

            if comparison_sparsity_per_odor is not None:
                comparison_response_rates = comparison_sparsity_per_odor[sparsity_col]

                assert (comparison_response_rates >= ymin).all(), \
                    f'{comparison_response_rates.min()=}'

                assert (comparison_response_rates <= ymax).all(), \
                    f'{comparison_response_rates.max()=}'

            ax.set_ylim([ymin, ymax])

        savefig(fig, param_dir, f'sparsity_per_odor{suffix}')
        return fig, ax


    repro_preprint_s1d = model_kws.get('repro_preprint_s1d', False)

    eb_mask = responses.columns.get_level_values(odor_col).str.startswith('eb @')
    assert eb_mask.sum() <= 1
    # should only be true if panel is validation2 (pebbled/megamat or hallem should
    # both have it)
    if repro_preprint_s1d and eb_mask.sum() == 0:
        repro_preprint_s1d = False

    if repro_preprint_s1d:
        assert extra_responses is not None

        s1d_responses = pd.concat([extra_responses, responses.loc[:, eb_mask]],
            axis='columns', verify_integrity=True
        )
        # (extra_responses still had 'odor' and responses had 'odor1' at time of concat)
        s1d_responses.columns.name = odor_col

        s1d_sparsities = _per_odor_tidy_model_response_rates(s1d_responses)

        # TODO delete 1 of these 2 plots below?

        fig, ax = plt.subplots()
        # TODO TODO adjust formatting so outlier points don't overlap (reduce alpha /
        # jitter?) (see pebbled/hemidraw one, which may be the one we want to use)
        sns.boxplot(data=s1d_sparsities, x=odor_col, y=sparsity_col, ax=ax, color='k',
            fill=False, flierprops=dict(alpha=0.175)
        )
        ax.set_title(title_including_silent_cells)
        savefig(fig, param_dir, 's1d_private_odor_sparsity')

        # TODO rewrite plot_sparsity... to make these 2nd arg optional?
        plot_sparsity_per_odor(s1d_sparsities, None, '_s1d')


    responses_including_silent = responses.copy()
    # TODO TODO what did Ann do for this?
    # (Matt did not drop silent cells. not sure about what Ann did.)
    if drop_silent_cells_before_analyses:
        # NOTE: important this happens after def of sparsity above
        responses = drop_silent_model_cells(responses)
        # TODO in n_seeds > 1 case, use old suffix w/o saying total numbers of cells?
        # (or some other format?)
        title += _get_silent_cell_suffix(responses_including_silent, responses)

    # TODO delete (/ move up before early return, right after sparsity calc)
    # TODO is there a big mismatch betweeen target_sparsity and sparsity (yes, see
    # below)?
    # TODO err / warn if differs much at all
    # TODO inspect cases where it differs (including olfsysm log)
    # (seems like nearly every case differs somewhat seriously)
    # ...
    # fitting model (responses_to='pebbled', tune_on_hallem=True,
    #   drop_receptors_not_in_hallem=True, pn2kc_connections=hemibrain,
    #   target_sparsity=0.1)...
    # ...
    # model_kws.get("target_sparsity")=0.1
    # sparsity=0.1804732780428448
    # ...
    # fitting model (responses_to='pebbled', tune_on_hallem=True,
    #   drop_receptors_not_in_hallem=True, pn2kc_connections=hemibrain,
    #   target_sparsity=0.05)...
    # ...
    # fixed_thr: 221.8443262928323
    # wAPLKC: 5.523460576698389
    # wKCAPL: 0.0030067831119751703
    # done
    # responses.shape=(1837, 17)
    # model_kws.get("target_sparsity")=0.05
    # sparsity=0.08511319606775754
    #
    # TODO TODO anything i can change to make tuning converge better?
    # relevant parameters:
    # rv.kc.tuning_iters (not a cap tho, just to keep track of it?)
    # mp.kc.max_iters (a cap for above) (default=10)
    # mp.kc.apltune_subsample (default=1)
    # mp.kc.sp_lr_coeff (initial learning rate, from which subsequent iteration learning
    #     rates decrease w/ sqrt num iters i think) (default=10.0)
    #
    #     ...
    #     double lr = p.kc.sp_lr_coeff / sqrt(double(rv.kc.tuning_iters));
    #     double delta = (sp - p.kc.sp_target) * lr/p.kc.sp_target;
    #     rv.kc.wAPLKC.array() += delta;
    #     ...
    #
    # mp.kc.sp_acc (the tolerance acceptable) (default=0.1)
    #     "the fraction +/- of the given target that is considered an acceptable
    #     sparsity"
    #
    # tuning proceeds while: ( (abs(sp-p.kc.sp_target)>(p.kc.sp_acc*p.kc.sp_target)) &&
    # (rv.kc.tuning_iters <= p.kc.max_iters) )
    if 'target_sparsity' in model_kws:
        target_sparsity = model_kws['target_sparsity']
        print()
        print(f'target_sparsity={target_sparsity:.3g}')
        print(f'sparsity={sparsity:.3g}')

        adiff = sparsity - target_sparsity
        rdiff = adiff / target_sparsity
        # TODO TODO use to inspect improvement when increasing tuning time (+ also
        # time model running, to see how much extra time extra tuning adds)
        print(f'{(rdiff * 100):.1f}% rel sparsity diff')
        print(f'{adiff:.2g} abs sparsity diff')

        # TODO TODO assert one/both below some threshold? warn above some thresh?

        # TODO delete?
        # TODO what fraction is passing atol=0.005? should i make it higher? .01?
        # (at least for target_sparsity=.1, w/ other params in the single choice i
        # actually do sens analysis on, it's only off by ~.009...)
        #if not np.isclose(sparsity, model_kws['target_sparsity'], atol=0.005):
        #    import ipdb; ipdb.set_trace()

        if have_megamat:
            if not np.isclose(megamat_sparsity, sparsity):
                print(f'megamat_sparsity={megamat_sparsity:.3g}')

        print()
    #

    # TODO drop panel here (before computing) if need be (after switching to pass input
    # that has that as a level on odor axis)
    if responses.index.name is None and 'seed' in responses.index.names:
        # TODO TODO refactor to use new mean_of_fly_corrs (passing in 'seed' for id
        # level)?
        corr_list = []
        seeds = []
        # level=<x> vs <x> didn't seem to matter here (at least, checking seed_corrs
        # after concat)
        for seed, seed_df in responses.groupby(level='seed', sort=False):
            seeds.append(seed)

            # each element of list is a Series now, w/ a 2-level multiindex for odor
            # combinations
            # NOTE: odor levels currently ('odor1', 'odor1') (SAME NAME, which might
            # cause problems...)
            corr_list.append(corr_triangular(seed_df.corr()))

        seed_corrs = pd.concat(corr_list, axis=1, keys=seeds, names='seed',
            verify_integrity=True
        )
        assert list(seed_corrs.columns) == seeds

        # converts from (row=['odor1','odor2'] X col='seed') to
        # row=['odor1','odor2','seed'] series
        # TODO TODO TODO has adding dropna=False broken anything? it was to fix odor
        # pairs not matching up in merge in comparison_orns code below
        # (only was triggered in hallem/uniform case)
        seed_corr_ser = seed_corrs.stack(dropna=False)

        # TODO can i convert below comment to an assertion / delete then (seems from
        # parenthetical below i felt i had figured it out)
        # TODO why is len(seed_corr_ser) (or len(model_corr_df)) == 56290, while
        # seed_corrs.size == 59950 (= n_seeds (10) * 5995 (= [110**2 - 110]/2) )
        # ipdb> seed_corrs.size - seed_corrs.isna().sum().sum()
        # 56290
        # (so it's just NaN elements that are the issue)
        seed_corr_ser.name = 'model_corr'
        model_corr_df = seed_corr_ser.reset_index()

        # TODO rename to 'odor_a', 'odor_b'? (here and in *corr_triangular?)? assuming
        # 'odor2' here isn't the for-mixtures 'odor2' i often have in odor
        # multiindices...
        odor_levels = ['odor1', 'odor2']
        mean_pearson_ser = seed_corr_ser.groupby(level=odor_levels, sort=False).mean()

        # TODO below 2 comments still an issue?
        # TODO TODO fix! (only in hallem/uniform, after no longer only passing
        # megamat odors as sim_odors)
        # TODO TODO check at input of this what is reducing length of
        # triangular series below expected shape. missing at least (in either order):
        # a='g-decalactone @ -2'
        # b='glycerol @ -2'
        try:
            # TODO rename _index kwarg?
            # TODO TODO +fix so i don't need to pass it (what was the purpose of
            # passing it again? doc in comment) (doesn't seem to still be triggering?)
            pearson = invert_corr_triangular(mean_pearson_ser, _index=seed_corrs.index)
        except AssertionError:
            print()
            traceback.print_exc()
            print()
            import ipdb; ipdb.set_trace()
    else:
        pearson = responses.corr()

        corr_ser = corr_triangular(pearson)
        corr_ser.name = 'model_corr'
        model_corr_df = corr_ser.reset_index()

        # TODO just start w/ 'odor_a', 'odor_b' here, to avoid issues later?
        # or even 'odor_row', 'odor_col'?
        #
        # just to match invert_corr_triangular output above (from 'odor1' for both here)
        pearson.index.name = 'odor'
        pearson.columns.name = 'odor'

    pearson = _resort_corr(pearson, panel,
        warn=False if responses_to == 'hallem' else True
    )

    # TODO TODO try deleting this and checking i can remake all the same
    # megamat/validation plots? feel like i might not need this anymore (or maybe i want
    # to stop needing it anyway... could then support mix dilutions for kiwi/control)
    # TODO refactor to share w/ other places?
    def _strip_index_and_col_concs(df):
        assert df.index.name.startswith('odor')
        assert df.columns.name.startswith('odor')

        # assuming no duplicate odors in input
        assert len(set(df.index)) == len(df.index)
        assert len(set(df.columns)) == len(df.columns)

        # TODO just use hong2p.olf.parse_odor_name instead of all this?
        delim = ' @ '
        assert df.index.str.contains(delim).all()
        assert df.columns.str.contains(delim).all()
        df = df.copy()
        df.index = df.index.map(lambda x: x.split(delim)[0])
        df.columns = df.columns.map(lambda x: x.split(delim)[0])
        #

        # TODO delete try/except (did i not rename diag 'ms @ -3' appropriately?)
        try:
            # assuming dropping concentration info hasn't created duplicates
            # (which would happen if input has any 1 odor presented at >1 conc...)
            assert len(set(df.index)) == len(df.index)
        except AssertionError:
            # TODO deal with other odors duplicated
            # (either by also mangling before, like 'ms @ -3' -> 'diag ms @ -3', or by
            # subsetting after?)
            # ipdb> df.index.value_counts()
            # 2h         2
            # t2h        2
            # aphe       2
            # 2-but      2
            # va         2
            # 1-6ol      2
            print(f'{len(set(df.index))=}')
            print(f'{len(df.index)=}')

            # TODO also, why are 'pfo' row / col NaN here (including identity...)? data?
            # mishandling? want to drop pfo anyway?

            # TODO TODO has air mix been handled appropriately up until here?
            # seeems like we may have just dropped odor2 and lumped them in w/ ea/oct,
            # which would be bad. prob want to keep air mix? could also drop and just
            # use in-vial 2-component mix

            # (not currently an issue since i added the hack to move conc info to name
            # part for those odors)
            # TODO fix for new kiwi vs control data
            # (seems to be caused by dilutions of mixture. just drop those first? could
            # call the natmix fn for that)
            import ipdb; ipdb.set_trace()

        assert len(set(df.columns)) == len(df.columns)
        return df

    pearson = _strip_index_and_col_concs(pearson)

    if _in_sens_analysis:
        assert fixed_thr is not None and wAPLKC is not None

        if ((min_sparsity is not None and sparsity < min_sparsity) or
            (max_sparsity is not None and sparsity > max_sparsity)):

            warn(f'sparsity out of [{min_sparsity}, {max_sparsity}] bounds! returning '
                'without making plots!'
            )

            # TODO register atexit instead (use some kind of wrapped dir creation fn
            # that handles that for me automatically? factor out of savefig/whatever i
            # have that currently does something like that?)?
            if made_param_dir:
                # TODO err here if -c CLI arg passed?
                print('deleting {param_dir}!')
                shutil.rmtree(param_dir)

            return params_for_csv

        # don't think i wanted to return this for stuff outside sparsity bounds
        # (return above)
        params_for_csv['pearson'] = pearson

        if n_seeds == 1:
            title = (
                f'thr={fixed_thr:.2f}, wAPLKC={wAPLKC:.2f} (sparsity={sparsity:.3g})'
            )
        else:
            title = f'sparsity={sparsity:.3g}'

    plot_corr(pearson, param_dir, 'corr', xlabel=title)

    if responses_to == 'hallem':
        # TODO factor to use len(megamat_odor_names) / something instead of 17...
        #
        # all megamat odors should have been sorted before other hallem odors, so we
        # should be able to get the megamat17 subset by indexing this way
        plot_corr(pearson.iloc[:17, :17], param_dir, 'corr_megamat', xlabel=title)
    #

    def _compare_model_kc_to_orn_data(comparison_orns, desc=None):
        # TODO assert input odors match comparison_orns odors exactly?
        # (currently stripping conc in at least corr diff case?)
        # (or assert around merge below, that we have all same odor pairs in both
        # dataframes being merged)

        if desc is None:
            orn_fname_part = 'orn'
            # might cause some confusion if comparison_orns are hallem data...
            orn_label_part = 'ORN'
        else:
            # assuming we don't need to normalize desc for filename
            orn_fname_part = f'orn-{desc}'
            orn_label_part = f'ORN ({desc})'

        # TODO switch to checking if ['date', 'fly_num'] (or 'fly_id') in column levels,
        # maybe adding an assertion columns.name == 'glomerulus' if not? might make it
        # nicer to refactor into plot_corr (for deciding whether to call
        # mean_of_fly_corrs)
        if comparison_orns.columns.name == 'glomerulus':
            mean_orn_corrs = corr_triangular(comparison_orns.T.corr())
        else:
            assert comparison_orns.columns.names == ['date', 'fly_num', 'roi']
            # will exclude NaN (e.g. va/aa in first 2 megamat flies)
            mean_orn_corrs = mean_of_fly_corrs(comparison_orns, square=False)

        mean_orn_corrs.name = 'orn_corr'

        model_corr_df_odor_pairs = set(
            model_corr_df.set_index(['odor1', 'odor2']).index
        )
        orn_odor_pairs = set(mean_orn_corrs.index)

        # NOTE: changing abbrev_hallem_odor_index will likely cause this to fail if
        # model outputs are not also regenerated (via CLI arg `-i model`)
        assert model_corr_df_odor_pairs == orn_odor_pairs

        # TODO delete? seems like it would fail if any NaN...
        assert len(mean_orn_corrs.values) == len(np.unique(mean_orn_corrs.values))

        df = model_corr_df.merge(mean_orn_corrs, on=['odor1', 'odor2'])

        # TODO any reason to think this is actually an issue? couldn't we just
        # have bona fide duplicate corrs (yea, we prob do)?
        #
        # 2024-05-17: still an issue (seemingly only in hallem/uniform case, not
        # pebbled/uniform or hallem/hemibrain)
        #
        # TODO fix to work w/ some NaN corr values?
        # TODO was this actually caused by duplicate correlat
        # TODO why just failing in hallem/uniform, and not hallem/hemibrain,
        # case? both have some NaN kc corrs... (i think it was probably more a matter of
        # one having duplicate corrs...)
        # TODO TODO was this actually duplicate corrs though? that would make more sense
        # for KC outputs w/ small number of inputs (maybe?), but in ORN inputs?
        try:
            assert len(mean_orn_corrs.values) == len(np.unique(df['orn_corr']))
        except AssertionError:
            # TODO actually summarize these if i want to keep warn here at all?
            # or just delete?
            warn('some duplicate corrs! (may not actually be an issue...)')
            # ipdb> len(mean_orn_corrs.values)
            # 5995
            # 2024-05-20: this is now 5886 (one NaN? no)
            # ipdb> len(np.unique(df['orn_corr']))
            # 5885
            #import ipdb; ipdb.set_trace()

        # converting to correlation distance, like in matt's
        df['model_corr_dist'] = 1 - df['model_corr']
        df['orn_corr_dist'] = 1 - df['orn_corr']

        # TODO only do in megamat case
        df['odor1_is_megamat'] = df.odor1.map(odor_is_megamat)
        df['odor2_is_megamat'] = df.odor2.map(odor_is_megamat)
        df['pair_is_megamat'] = df[['odor1_is_megamat','odor2_is_megamat']
            ].all(axis='columns')

        if n_first_seeds_for_errorbar is not None and 'seed' in df.columns:
            df = select_first_n_seeds(df)

        def _save_kc_vs_orn_corr_scatterplot(metric_name):
            # to recreate preprint fig 3B

            if metric_name == 'correlation distance':
                col_suffix = '_corr_dist'

                # TODO rename dists -> dist (to share w/ col_suffix -> deleting this
                # after)?
                fname_suffix = '_corr_dists'

                # TODO double check language
                help_str = 'top-left: decorrelated, bottom-right: correlated'

                # TODO just derive bounds for either corr or corr-dist version from the
                # other -> consolidate to share assertion?
                # (/ refactor some other way...)
                plot_max = 1.5
                plot_min = 0.0

            elif metric_name == 'correlation':
                col_suffix = '_corr'
                fname_suffix = '_corr'

                # confident in language on this one
                help_str = 'top-left: correlated, bottom-right: decorrelated'

                plot_max = 1
                plot_min = -.5
            else:
                assert False, 'only above 2 metric_name values supported'

            if 'seed' in df.columns:
                errorbar = seed_errorbar
            else:
                # no seeds to compute CI over here. sns.lineplot would generate a
                # RuntimeWarning (about an all-NaN axis), if I tried to generate error
                # bars same way.
                errorbar = None

            if not df.pair_is_megamat.all():
                # just removing errorbar b/c was taking a long time and don't really
                # care about this plot anymore... (shouldn't take long if not using
                # bootstrapped CI, if i change errorbar)
                errorbar = None

                color_kws = dict(
                    hue='pair_is_megamat', hue_order=[False, True],
                    # TODO also try to have diff err/marker alphas here? prob not worth
                    # it, considering i don't really use this version of the plots...
                    palette={True: to_rgba('red', 0.7), False: to_rgba('black', 0.1)}
                )
            else:
                color_kws = dict(color='black')

            fig, ax = plt.subplots()
            add_unity_line(ax)

            orn_col = f'orn{col_suffix}'
            model_col = f'model{col_suffix}'

            lineplot_kws = dict(
                ax=ax, data=df, x=orn_col, y=model_col, linestyle='',
            )
            lineplot_kws = {**lineplot_kws, **color_kws}

            marker_only_kws = dict(
                markers=True, marker='o', errorbar=None,

                # to remove white edge of markers (were not respecting alpha)
                # (seem to work file w/ alpha, at least when set in non-'palette' case
                # below... was probably an issue when using hue/palette?)
                markeredgecolor='none',
            )
            # TODO should point / error display not be consistent between this and S1C /
            # 2E?
            err_only_kws = dict(
                markers=False, errorbar=errorbar, err_style='bars', seed=bootstrap_seed,
                # TODO make these thinner (to not need such fine tuning on alpha?)?
            )
            # more trouble than worth w/ palette (where values are 4 tuples w/ alpha)
            if 'palette' not in color_kws:
                # seems to default to white otherwise
                marker_only_kws['markeredgecolor'] = color_kws['color']
                # TODO if i like, refactor to share w/ other seed_errorbar plots?
                # TODO TODO like 'None' more than 'white' here? for some other pltos
                # (mainly those w/ lines thru them too), i liked 'white' more.
                marker_only_kws['markerfacecolor'] = 'None'

                # TODO still want some alpha < 1, when just showing edge (not face) of
                # markers?
                #
                # .3 too high, .2 pretty good, .15 maybe too low
                marker_only_kws['alpha'] = 0.175

                # 0.5 maybe verging on too low by itself, but still bit too crowded when
                # overlapping. .4 pretty good
                err_only_kws['alpha'] = 0.35

            # no other way I could find to get separate alpha for markers and errorbars,
            # other than to make 2 calls. setting alpha in kws led to a duplicate kwarg
            # error (rather than overwriting one from general kwargs).

            # plot points
            sns.lineplot(**lineplot_kws, **marker_only_kws)

            if errorbar is not None:
                # plot errorbars
                sns.lineplot(**lineplot_kws, **err_only_kws)

            ax.set_xlabel(f'{metric_name} of {orn_label_part} tuning (observed)'
                f'\n{help_str}'
            )
            if 'pn2kc_connections' in model_kws:
                ax.set_ylabel(
                    f'{metric_name} of {model_kws["pn2kc_connections"]} model KCs'
                )
            else:
                ax.set_ylabel(f'{metric_name} of model KCs')

            metric_max = max(df[model_col].max(), df[orn_col].max())
            metric_min = min(df[model_col].min(), df[orn_col].min())

            assert metric_max <= plot_max, \
                f'{param_dir}\n{desc=}: {metric_max=} > {plot_max=}'
            assert metric_min >= plot_min, \
                f'{param_dir}\n{desc=}: {metric_min=} < {plot_min=}'

            ax.set_xlim([plot_min, plot_max])
            ax.set_ylim([plot_min, plot_max])

            # should give us an Axes that is of square size in figure coordinates
            ax.set_box_aspect(1)

            if 'seed' in df.columns:
                # averaging correlations over seed, before calculating bootstrapped
                # spearman (so that CI is correct. otherwise showed no error)
                for_spearman = df.groupby(['odor1','odor2'])[[model_col,orn_col]].mean()
            else:
                for_spearman = df.copy()

            spear_text, _, _, _, _ = bootstrapped_corr(for_spearman, model_col, orn_col,
                # TODO delete (for debugging)
                # don't want to do for 'orn-est-spike-delta' case, as would need code
                # changes and don't care about that
                _plot_dir=param_dir if orn_fname_part == 'orn-raw-dff' else None,
                #
            )

            if errorbar is None:
                ax.set_title(f'{title}\n\n{spear_text}')
            else:
                ax.set_title(f'{title}\n\n{seed_err_text}\n{spear_text}')

            savefig(fig, param_dir, f'model_vs_{orn_fname_part}{fname_suffix}')


        _save_kc_vs_orn_corr_scatterplot('correlation distance')
        _save_kc_vs_orn_corr_scatterplot('correlation')


        # TODO will probably need to pass _index here to have invert_corr_triangular
        # work.... (doesn't seem like we've been failing w/ same AssertionError i had
        # needed to catch in other place...)
        square_mean_orn_corrs = _resort_corr(invert_corr_triangular(mean_orn_corrs),
            panel, warn=False if responses_to == 'hallem' else True
        )

        # stripping conc to match processing of `pearson` above
        square_mean_orn_corrs = _strip_index_and_col_concs(square_mean_orn_corrs)
        try:
            assert pearson.index.equals(square_mean_orn_corrs.index)
            assert pearson.columns.equals(square_mean_orn_corrs.columns)
        # TODO still reachable? delete?
        except AssertionError:
            # TODO care enough to find intersection and just take diff there?
            # (or dropna and resort?)
            print(f'not plotting corr diff wrt {orn_label_part} (index mismatch)')
            return

        corr_diff = pearson - square_mean_orn_corrs
        plot_corr(corr_diff, param_dir, f'model_vs_{orn_fname_part}_corr_diff',
            title=title, xlabel=f'model KC corr - {orn_label_part} corr'
        )
        # square_mean_corrs should be plotted elsewhere (potentially in diff places
        # depending on whether input is Hallem vs pebbled data?)
        # TODO check + comment where each should be saved?


    if comparison_orns is not None:
        if type(comparison_orns) is dict:
            for desc, comparison_data in comparison_orns.items():
                _compare_model_kc_to_orn_data(comparison_data, desc)
        else:
            _compare_model_kc_to_orn_data(comparison_orns)


    if comparison_kc_corrs is not None:
        # TODO assert input odors match comparison_kc_corrs odors exactly?

        # TODO just do this unconditionally like in comparison_orns code? or make that
        # part explicit too?
        if _strip_concs_comparison_kc_corrs:
            comparison_kc_corrs = _strip_index_and_col_concs(comparison_kc_corrs)

            n_combos_before = len(model_corr_df[['odor1', 'odor2']].drop_duplicates())
            # TODO use parse_odor_name in other stripping fn here too?
            model_corr_df['odor1'] = model_corr_df.odor1.apply(olf.parse_odor_name)
            model_corr_df['odor2'] = model_corr_df.odor2.apply(olf.parse_odor_name)

            n_combos_after = len(model_corr_df[['odor1', 'odor2']].drop_duplicates())
            assert n_combos_before == n_combos_after

        kc_corrs = corr_triangular(comparison_kc_corrs)
        kc_corrs.name = 'observed_kc_corr'

        df = model_corr_df.merge(kc_corrs, on=['odor1', 'odor2'])

        # converting to correlation distance, like in matt's
        df['model_corr_dist'] = 1 - df['model_corr']
        df['observed_kc_corr_dist'] = 1 - df['observed_kc_corr']

        fig, ax = plt.subplots()

        # doing this first so everything else gets plotted over it
        # TODO why do points from call below seem to be plotted under this? way to force
        # a certain Z order?
        add_unity_line(ax)

        sns.regplot(data=df, x='observed_kc_corr_dist', y='model_corr_dist',
            x_estimator=np.mean, x_ci=None, color='black', scatter_kws=dict(alpha=0.3),
            fit_reg=False
        )

        # averaging over 'seed' level to get mean correlation for each pair, because we
        # don't show error for each point in this plot (i.e. error across seeds). we
        # only show a CI for the regression line shown (handled in regplot call below)
        if 'seed' in df.columns:
            df = df.groupby(['odor1','odor2']).mean().reset_index()

        # TODO assert len(df) always n_choose_2(n_odors) at this point?
        # (seems true in pebbled/hemibrain at least. check uniform)

        corr_dist_max = max(df.model_corr_dist.max(), df.observed_kc_corr_dist.max())
        corr_dist_min = min(df.model_corr_dist.min(), df.observed_kc_corr_dist.min())
        plot_max = 1.3
        plot_min = 0.0
        assert corr_dist_max <= plot_max, f'{param_dir}\n{corr_dist_max=} > {plot_max=}'
        assert corr_dist_min >= plot_min, f'{param_dir}\n{corr_dist_min=} < {plot_min=}'

        # need to set these before regplot call below (which makes regression line +
        # CI), so that the line actually goes to these limits.
        ax.set_xlim([plot_min, plot_max])
        ax.set_ylim([plot_min, plot_max])

        # NOTE: none of the KC vs model KC scatter plots in preprint have seed-error
        # shown, so not including errorbar=seed_errorbar here. just want error on
        # regression line in this plot, which we have (and we are happy w/ default 95%
        # CI on mean for that)
        sns.regplot(data=df, x='observed_kc_corr_dist', y='model_corr_dist',
            color='black', scatter=False, truncate=False, seed=bootstrap_seed
        )

        spear_text, _, _, _, _ = bootstrapped_corr(df, 'model_corr_dist',
            'observed_kc_corr_dist', method='spearman',
            # TODO delete (for debugging)
            _plot_dir=param_dir,
            #
        )
        ax.set_title(f'{title}\n\n{spear_text}')

        ax.set_xlabel('KC correlation distance (observed)')
        ax.set_ylabel('model KC correlation distance')

        # should give us an Axes that is of square size in figure coordionates
        ax.set_box_aspect(1)

        # TODO rename to indicate they are corr-dists, not just corrs (no other version
        # of the plot tho...)?
        #
        # to reproduce preprint figures 3 Di/Dii
        savefig(fig, param_dir, 'model_vs_kc_corrs')


    # TODO why am i getting the following error w/ my current viz.clustermap usage?
    # 3625, in _dendrogram_calculate_info
    #     _dendrogram_calculate_info(
    #   File "/home/tom/src/al_analysis/venv/lib/python3.8/site-packages/scipy/cluster/hierarchy.py", line 3658, in _dendrogram_calculate_info
    #     _dendrogram_calculate_info(
    #   File "/home/tom/src/al_analysis/venv/lib/python3.8/site-packages/scipy/cluster/hierarchy.py", line 3625, in _dendrogram_calculate_info
    #     _dendrogram_calculate_info(
    #   File "/home/tom/src/al_analysis/venv/lib/python3.8/site-packages/scipy/cluster/hierarchy.py", line 3625, in _dendrogram_calculate_info
    #     _dendrogram_calculate_info(
    #   File "/home/tom/src/al_analysis/venv/lib/python3.8/site-packages/scipy/cluster/hierarchy.py", line 3555, in _dendrogram_calculate_info
    #     _append_singleton_leaf_node(Z, p, n, level, lvs, ivl,
    #   File "/home/tom/src/al_analysis/venv/lib/python3.8/site-packages/scipy/cluster/hierarchy.py", line 3433, in _append_singleton_leaf_node
    #     ivl.append(str(int(i)))
    # RecursionError: maximum recursion depth exceeded while getting the str of an object

    # https://github.com/scipy/scipy/issues/7271
    # https://github.com/MaayanLab/clustergrammer/issues/34
    sys.setrecursionlimit(100000)
    # TODO maybe don't need to change sys setrecursionlimit now that i'm dropping silent
    # cells?

    # TODO try a version of this using first N (< n_seeds) seeds? try all (i assume it'd
    # be much too slow, and also unreadable [assuming we try to show all cells, and not
    # cluster means / similar reduction])?
    #
    # only including silent here so we can count them in line below
    to_cluster = responses_including_silent
    clust_suffix = ''

    if n_seeds > 1:
        first_seed = to_cluster.index.get_level_values('seed')[0]
        to_cluster = to_cluster.loc[first_seed]
        clust_suffix = '_first-seed-only'

    silent_cells = (to_cluster == 0).all(axis='columns')

    if silent_cells.all():
        # TODO also return before generating the corr plots (+ any others) above?
        # TODO err instead?
        #
        # TODO why was* (can not currently repro) this the case for ALL attempts in
        # 'kiwi' case now?  really need steps so different from 'megamat' case? if so,
        # why? or am i just not calling it right at all for some reason (related to
        # that failing check to repro output w/ fixed wAPLKC/fixed_thr?)
        warn('all model cells were silent! returning before generating further plots!')
        return params_for_csv

    # ~30" height worked for ~1837 cells, but don't need all that when not plotting
    # silent cells
    cg = cluster_rois(to_cluster[~ silent_cells].T, odor_sort=False, figsize=(7, 12),
        # seeing if this resolves recursion error...
        # does not fix it.
        #optimal_ordering=False
    )

    cg.fig.suptitle(f'{title_including_silent_cells}\n\n'
        # TODO just define n_silent_cells alongside responses_including_silent/responses
        # def, then remove separate use of `silent_cells` here (+ use responses instead
        # of responses_including_silent)?
        # (doing it earlier would be complicated/impossible in n_seeds > 1 case
        # though...)
        f'{silent_cells.sum()} silent cells / {len(to_cluster)} total'
    )
    savefig(cg, param_dir, f'responses_nosilent{clust_suffix}')

    # TODO TODO also plot wPNKC (clustered?) for matts + my stuff?
    # TODO same for other model vars? thresholds?

    # TODO corr diff plot too (even if B doesn't want to plot it for now)?

    # TODO and (maybe later) correlation diffs wrt model w/ tuned params

    # TODO assert no sparsity (/ value) goes outside cbar/scale limits
    # (do in sparsity plotting fn?)

    sparsity_per_odor = _per_odor_tidy_model_response_rates(responses_including_silent
        ).set_index(odor_col)

    if comparison_responses is not None:
        comparison_sparsity_per_odor = _per_odor_tidy_model_response_rates(
            comparison_responses
        )
        comparison_sparsity_per_odor = comparison_sparsity_per_odor.sort_values(
            sparsity_col
        )
        # TODO also sort correlation odors by same order?
        # TODO assert set of odors are the same first
        sparsity_per_odor = sparsity_per_odor.loc[comparison_sparsity_per_odor.odor1]
    else:
        comparison_sparsity_per_odor = None

    sparsity_per_odor = sparsity_per_odor.reset_index()

    sparsity_per_odor.odor1 = sparsity_per_odor.odor1.map(lambda x: x.split(' @ ')[0])

    if comparison_responses is not None:
        # TODO need (just to remove diff numbering in index, wrt sparsity_per_odor, in
        # case that changes behavior of some plotting...)?
        # TODO why drop=True here, but not in sparsity_per_odor.reset_index() above?
        comparison_sparsity_per_odor = comparison_sparsity_per_odor.reset_index(
            drop=True
        )
        # TODO refactor (duped above)?
        comparison_sparsity_per_odor.odor1 = comparison_sparsity_per_odor.odor1.map(
            lambda x: x.split(' @ ')[0]
        )
        assert comparison_sparsity_per_odor.odor1.equals(sparsity_per_odor.odor1)

    if responses_to == 'hallem':
        # assuming megamat for now (otherwise this would be empty)
        megamat_sparsity_per_odor = sparsity_per_odor.loc[
            sparsity_per_odor.odor1.isin(panel2name_order['megamat'])
        ]
        # this is only used in sensitivity analysis now anyway. would need to also
        # subset this if not.
        assert comparison_sparsity_per_odor is None
        plot_sparsity_per_odor(megamat_sparsity_per_odor, comparison_sparsity_per_odor,
            '_megamat'
        )

    panel2sparsity_ylims = {
        # TODO add one for megamat? (only sensitivity analysis currently has these figs
        # in paper for megamat, but maybe betty will end up wanting these plots alone
        # anyway? in both cases, just need to find a range that works).
        #
        # 0.21 not enough for some.
        'validation2': [0, 0.22],
        # could prob do [0, 2], but might as well keep same as validation2. could just
        # hardcode this in general (or at least as long as the data is within limit?)?
        # TODO TODO fix (+ update validation?) actually some stuff is past this
        # apparently... (oh, it was actually on hallem data that it was failing, in the
        # uniform model)
        # TODO TODO fix so we fall back to no scale set (w/ warning), or so hallem isn't
        # considered megamat here (almost certainly former)?
        #'megamat': [0, 0.22],
    }
    # ylim=None will let plot_sparsity_per_odor set it
    ylim = panel2sparsity_ylims.get(panel)
    combined_fig, sparsity_ax = plot_sparsity_per_odor(sparsity_per_odor,
        comparison_sparsity_per_odor, '', ylim=ylim
    )

    #sparsity_ylim_max = 0.5
    # to exceed .706 in (fixed_thr=120.85, wAPLKC=0.0) param case
    sparsity_ylim_max = 0.71
    # TODO TODO are these the ylims used for validation2 modelling plots in current
    # (2025-02-18) modeling.svg for paper? are any other plots in paper using this?
    # maybe for sensitivity analysis?
    sparsity_ax.set_ylim([0, sparsity_ylim_max])
    # TODO TODO fix sensitivity analysis to not give us stuff outside this
    # comparison_sparsity_per_odor isn't being pushed to the same extreme, and should be
    # well within this limit.
    #assert not (sparsity_per_odor[sparsity_col] > sparsity_ylim_max).any()

    # https://stackoverflow.com/questions/33264624
    # NOTE: without other fiddling, need to keep references to both of these axes, as
    # the Axes created by `ax.twinx()` is what we need to control the ylabel
    # https://stackoverflow.com/questions/54718818
    n_odor_ax_for_ylabel = sparsity_ax.twinx()
    n_odor_ax = n_odor_ax_for_ylabel.twiny()

    fig, ax = plt.subplots()
    plot_n_odors_per_cell(responses_including_silent, ax,
        title=title_including_silent_cells
    )
    if comparison_responses is not None:
        plot_n_odors_per_cell(comparison_responses, ax, label_suffix=' (tuned)',
            color='gray', title=title_including_silent_cells
        )

    savefig(fig, param_dir, 'n_odors_per_cell')

    if responses_to == 'hallem':
        fig, ax = plt.subplots()
        # TODO assert this is getting just megamat odors (/reimplement so it only could)
        # (b/c prior sorting, that should have put all them before rest of hallem odors,
        # they should be)
        # TODO factor out + use megamat subsetting fn here
        plot_n_odors_per_cell(responses_including_silent.iloc[:, :17], ax,
            title=title_including_silent_cells
        )

        assert comparison_responses is None
        # TODO delete?
        # if assertion fails, will also need to subset comparison_responses to megamat
        # odors (in commented code below)
        #if comparison_responses is not None:
        #    plot_n_odors_per_cell(comparison_responses, ax, label_suffix=' (tuned)',
        #        color='gray', title=title_including_silent_cells
        #    )

        savefig(fig, param_dir, 'n_odors_per_cell_megamat')

    # only currently running sensitivity analysis in pebbled/hemibrain case.
    # some of code below (all of which should deal with sensitivity analysis in some
    # way, from here on) may not work w/ multiple seeds. could delete this early return
    # and try though.
    if n_seeds > 1:
        assert not sensitivity_analysis
        return params_for_csv

    # only want to save this combined plot in case of senstivity analysis
    # (where we need as much space as we can save)
    if _in_sens_analysis:
        assert fixed_thr is not None and wAPLKC is not None

        plot_n_odors_per_cell(responses_including_silent, n_odor_ax,
            ax_for_ylabel=n_odor_ax_for_ylabel, linestyle='dashed', log_yscale=True
        )

        if comparison_responses is not None:
            plot_n_odors_per_cell(comparison_responses, n_odor_ax,
                ax_for_ylabel=n_odor_ax_for_ylabel, label_suffix=' (tuned)',
                color='gray', linestyle='dashed', log_yscale=True
            )

        # TODO figure out how to build one legend from the two axes?
        if _add_combined_plot_legend:
            # planning to manually adjust when assembling figure
            sparsity_ax.legend(loc='upper right')
            n_odor_ax.legend(loc='center right')

        savefig(combined_fig, param_dir, 'combined_odors-per-cell_and_sparsity')

    if sensitivity_analysis:
        # TODO TODO where are dirs like
        # <panel>/tuned-on_control-kiwi__dff_scale-to-avg-max__data_pebbled__hallem-tune_False__pn2kc_hemibrain__fixed-thr_0__wAPLKC_0.00
        # coming from??? something not working right?

        # TODO probably remove these assertions (to do sensitivity analysis around each
        # of kiwi/control runs, where they had inh params set from one run w/
        # kiwi+control data)
        #
        # TODO delete
        print('want to restore some of these assertions?')
        #assert fixed_thr is None and wAPLKC is None
        #assert ('target_sparsity' in model_kws and
        #    model_kws['target_sparsity'] is not None
        #)

        shared_model_kws = {k: v for k, v in model_kws.items() if k not in (
            # plot_dir would conflict with first positional arg of
            # fit_and_plot_mb_model, and we don't want plots for sensitivity analysis
            # subcalls anyway (could fix if we did want those plots).
            'plot_dir',
            # excluding target_sparsity b/c that is mutually exclusive w/ fixing
            # threshold and KC<->APL inhibition, as all these calls will.
            'target_sparsity',
            # TODO also add target_sparsity_factor_pre_APL here? matter?
            #'target_sparsity_factor_pre_APL',

            # will default to False (via fit_mb_model default) once I remove this, which
            # is what I want
            'repro_preprint_s1d',
        )}
        # TODO try to not need to specially treat wAPLKC / fixed_thr (instead trying to
        # continue handling just via model_kws) tho? make things too difficult?

        tuned_fixed_thr = param_dict['fixed_thr']
        tuned_wAPLKC = param_dict['wAPLKC']
        assert tuned_fixed_thr is not None and tuned_wAPLKC is not None

        checks = True
        # TODO move to some kind of unit test. in olfsysm, maybe?
        if checks:
            print('checking we can recreate responses by hardcoding tuned '
                'fixed_thr/wAPLKC...', end=''
            )
            # TODO TODO figure out why this wasn't working on some runs of
            # kiwi/control data (re-running w/ `-i model` seemed to fix it, but unclear
            # on why that would be. using cache after break it again?) (doesn't seem
            # like it...) (encountered again 2025-02-20. going to re-run w/ `-i model`,
            # but do have a recent backup of whole mb_modeling dir, if i want to compare
            # outputs)
            # TODO why *was* responses2.sum().sum() == 0 in kiwi/control case???
            # (prob same reason sens analysis failing there)
            # (not sure i can repro, same as w/ failing assertion below)
            #
            # TODO silence output here? makes surrounding prints hard to follow
            # TODO also check spike_counts (2nd / 4 returned values)?
            responses2, _, _, _ = fit_mb_model(sim_odors=sim_odors,
                fixed_thr=tuned_fixed_thr, wAPLKC=tuned_wAPLKC, **shared_model_kws
            )
            try:
                assert responses_including_silent.equals(responses2)
            except AssertionError:
                # TODO delete
                r1 = responses_including_silent
                r2 = responses2
                import ipdb; ipdb.set_trace()
                #
            print(' we can!\n')

        parent_output_dir = param_dir / 'sensitivity_analysis'

        # deleting this directory (and all contents) before run, to clear plot dirs from
        # param choices only in previous sweeps (and tried.csv with same)
        if parent_output_dir.exists():
            # TODO TODO warn / err if -c (can't check new plots against existing if we
            # are deleting whole dir...)?
            # TODO make a rmtree wrapper (->use here and elsewhere) and always warn/err
            # if -c?
            warn(f'deleting {parent_output_dir} and all contents!')
            shutil.rmtree(parent_output_dir)

        # savefig will generally do this for us below
        parent_output_dir.mkdir(exist_ok=True)

        tried_param_cache = parent_output_dir / 'tried.csv'
        # should never exist as we're currently deleting parent dir above
        # (may change to not delete above though, so keeping this)
        if tried_param_cache.exists():
            tried = pd.read_csv(tried_param_cache)
            # TODO assert columns are same as below (refactor col def above conditional)
        else:
            tried = pd.DataFrame(columns=['fixed_thr', 'wAPLKC', 'sparsity'])

        sens_analysis_kw_defaults = dict(
            n_steps=3,
            fixed_thr_param_lim_factor=0.5,
            wAPLKC_param_lim_factor=5.0,
            drop_nonpositive_fixed_thr=True,
            drop_negative_wAPLKC=True,
        )
        if sens_analysis_kws is None:
            sens_analysis_kws = dict()
        else:
            assert all(k in sens_analysis_kw_defaults for k in sens_analysis_kws.keys())

        sens_analysis_kws = {**sens_analysis_kw_defaults, **sens_analysis_kws}

        # TODO TODO try implementing alternative means of specifying bounds of sens
        # analysis: try sweeping each parameter until we reach some 2nd target response
        # rate (e.g. 2-5%, << typical target response rate when tuning both params of
        # ~10%) (then go the same distance [relative?] the other way? or what? not sure
        # this works...)
        #
        # want to avoid trial-and-error setting of below parameters such that corners of
        # grid each see similarly low/extreme response rates
        # TODO TODO TODO easier to implement such a thing by just slightly modifying
        # olfsysm C++ code?

        # needs to be odd so grid has tuned values (that produce outputs used elsewhere
        # in paper) as center.
        n_steps = sens_analysis_kws['n_steps']
        assert n_steps % 2 == 1, f'n_steps must be odd (got {n_steps=})'

        # TODO might need to be <1 to have lower end be reasonable?  but that prob won't
        # be enough (meaning? delete comment or elaborate) for upper ends...
        #
        # had previously tried up to at least 3, as well (not sure how it compared now
        # though...)
        #
        # for getting upper left 3x3 from original 4x4 (which was generated w/ this
        # param doubled, and n_steps=5 rather than 3)
        fixed_thr_param_lim_factor = sens_analysis_kws['fixed_thr_param_lim_factor']

        # TODO try seeing if we can push this high enough to start getting missing
        # correlations. 1000? why only getting those for high fixed_thr? and where
        # exactly do they come from?
        #
        # NOTE: was not seemingly able to get odors w/ no cells responding to them by
        # increasing this param (up to 100.0)...
        # 10.0 was used for most of early versions of this
        wAPLKC_param_lim_factor = sens_analysis_kws['wAPLKC_param_lim_factor']

        drop_nonpositive_fixed_thr = sens_analysis_kws['drop_nonpositive_fixed_thr']
        drop_negative_wAPLKC = sens_analysis_kws['drop_negative_wAPLKC']

        # TODO factor out?
        def steps_around_tuned(tuned_param: float, param_lim_factor: float,
            param_name: Optional[str] = None, *, n_steps: int = n_steps,
            drop_negative: bool = True, drop_zero: bool = False):

            # TODO rename this? it's not the size of the single steps, now is it?
            step_size = tuned_param * param_lim_factor
            param_steps = np.linspace(tuned_param - step_size, tuned_param + step_size,
                num=n_steps
            )

            if drop_zero:
                assert drop_negative

            prefix = '' if param_name is None else f'{param_name=}: '
            if drop_negative:
                param_steps[param_steps < 0] = 0

                if (param_steps == 0).any():
                    last_zero_idx = np.argwhere(param_steps == 0)[-1, 0]

                    first_idx_to_use = last_zero_idx
                    if drop_zero:
                        first_idx_to_use += 1

                    if first_idx_to_use > 0:
                        warn(f'{prefix}setting lowest {first_idx_to_use} steps '
                            '(negative) to 0'
                        )

                    param_steps = param_steps[first_idx_to_use:]

                    if not drop_zero:
                        bottom_step_size = np.diff(param_steps[:2])[0]
                        # TODO update to include actual step size ref (current
                        # `step_size` var is half range, not related to number of steps)
                        warn(f'{prefix}step sizes uneven after setting negative values '
                            f'to 0. step size previously all {step_size:.4f} '
                            f'(from {param_lim_factor=}). bottom step now '
                            f'{bottom_step_size:.4f}'
                        )

            assert len(param_steps) >= 3, (f'{prefix}must have at least 1 step on '
                'either side of tuned param'
            )
            assert np.isclose(tuned_param, param_steps).sum() == 1, \
                f'{prefix}tuned param not in steps'

            # TODO uh, why do i have this if drop_negative=False is even an option?
            # (delete option / this?)
            # TODO or (given how it's actually implemented) are negative values
            # meaningful for threshold? (surely not for wAPLKC)
            assert (param_steps >= 0).all()

            return param_steps

        # TODO try 0 for min of fixed_thr_steps (remove drop_zero=True, and tweak step
        # size to get at least 1 <=0) (current steps not clipped by it)?
        # is 0 the lowest value that maximizes response rate? or are negative vals
        # meaningful given how this is implemented?
        # (would allow me to simplify code slightly if this could be handled as wAPLKC)
        fixed_thr_steps = steps_around_tuned(tuned_fixed_thr,
            fixed_thr_param_lim_factor, 'fixed_thr', drop_negative=True,
            drop_zero=drop_nonpositive_fixed_thr
        )
        wAPLKC_steps = steps_around_tuned(tuned_wAPLKC, wAPLKC_param_lim_factor,
            'wAPLKC', drop_negative=drop_negative_wAPLKC
        )

        print(f'{tuned_fixed_thr=}')
        print(f'{tuned_wAPLKC=}')
        print()
        print('parameter steps around sparsity-tuned values (above):')
        print(f'{fixed_thr_steps=}')
        print(f'{wAPLKC_steps=}')
        print()

        step_choice_params = pd.Series({
            'tuned_fixed_thr': tuned_fixed_thr,
            'tuned_wAPLKC': tuned_wAPLKC,

            'n_steps': n_steps,
            'fixed_thr_param_lim_factor': fixed_thr_param_lim_factor,
            'wAPLKC_param_lim_factor': wAPLKC_param_lim_factor,
            'drop_negative_wAPLKC': drop_negative_wAPLKC,
            'drop_nonpositive_fixed_thr': drop_nonpositive_fixed_thr,

            # should be fully determined by above, but just including for easier
            # inspection
            'fixed_thr_steps': fixed_thr_steps,
            'wAPLKC_steps': wAPLKC_steps,
        })
        to_csv(step_choice_params, parent_output_dir / 'step_choices.csv',
            # so column name '0' doesn't get added (also added if doing ser.to_frame())
            header=False
        )

        # TODO try to just derive from tried, but this seemed easier for now.
        tried_wide = pd.DataFrame(data=float('nan'), columns=fixed_thr_steps,
            index=wAPLKC_steps
        )
        tried_wide.columns.name = 'fixed_thr'
        tried_wide.index.name = 'wAPLKC'

        # should be something that won't appear in actual computed values. NaN may
        # appear in computed values. after loop, we check that we no longer have any of
        # these.
        corr_placeholder = 10

        row_index = pd.MultiIndex.from_product([fixed_thr_steps, wAPLKC_steps],
            names=['fixed_thr', 'wAPLKC']
        )
        col_index = corr_triangular(pearson).index
        pearson_at_each_param_combo = pd.DataFrame(index=row_index, columns=col_index,
            data=corr_placeholder
        )

        # TODO any point in having this if we are deleting root of all these above?
        # delete?
        ignore_existing = True

        _add_combined_plot_legend = True

        for fixed_thr, wAPLKC in tqdm(itertools.product(fixed_thr_steps, wAPLKC_steps),
            total=len(fixed_thr_steps) * len(wAPLKC_steps),
            unit='fixed_thr+wAPLKC combos'):

            print(f'{fixed_thr=}')
            print(f'{wAPLKC=}')

            # NOTE: created by inner fit_and_plot... call below
            output_dir = parent_output_dir / f'thr{fixed_thr:.2f}_wAPLKC{wAPLKC:.2f}'

            if output_dir.exists() and not ignore_existing:
                print(f'{output_dir} already existed. skipping!')
                continue

            if ((tried.fixed_thr <= fixed_thr) & (tried.wAPLKC <= wAPLKC) &
                (tried.sparsity < min_sparsity)).any():

                print(f'sparsity would be < {min_sparsity=}')
                continue

            elif ((tried.fixed_thr >= fixed_thr) & (tried.wAPLKC >= wAPLKC) &
                    (tried.sparsity > max_sparsity)).any():

                print(f'sparsity would be > {max_sparsity=}')
                continue

            curr_params = fit_and_plot_mb_model(output_dir,
                comparison_responses=responses_including_silent,
                sim_odors=sim_odors, sensitivity_analysis=False, _in_sens_analysis=True,
                fixed_thr=fixed_thr, wAPLKC=wAPLKC,
                _add_combined_plot_legend=_add_combined_plot_legend,
                # not passing param_dir_prefix here, b/c that should be in parent
                # directory, and should be easy enough to keep track of from that
                extra_params=extra_params,
                **shared_model_kws
            )
            # shouldn't need to call _write_inputs_for_reproducibility here, as should
            # also be in parent dir

            # should only be None in first_seed_only=True case, but not doing any
            # multi-seed runs w/ sensitivity analysis. only doing sensitivity analysis
            # for hemibrain + pebbled case currently.
            assert curr_params is not None

            # (added only if wAPLKC/fixed_thr passed)
            pearson = curr_params['pearson']

            pearson_ser = corr_triangular(pearson)
            assert pearson_ser.index.equals(pearson_at_each_param_combo.columns)
            pearson_at_each_param_combo.loc[fixed_thr, wAPLKC] = pearson_ser

            sparsity = curr_params['sparsity']
            print(f'sparsity={sparsity:.3g}')

            if output_dir.exists():
                tried_wide.loc[wAPLKC, fixed_thr] = sparsity

                # only want for first plot
                if _add_combined_plot_legend:
                    _add_combined_plot_legend = False

            tried = tried.append(
                {
                    'fixed_thr': fixed_thr, 'wAPLKC': wAPLKC, 'sparsity':  sparsity,
                },
                ignore_index=True
            )
            tried = tried.sort_values(['fixed_thr','wAPLKC'])

            # can't use my to_csv as it currently errs if same CSV would get written >1
            # time in a given run
            tried.to_csv(tried_param_cache, index=False)

            print()

        # to make rows=fixed_thr, cols=wAPLKC (consistent w/ how i had been laying out
        # the figure grids)
        tried_wide = tried_wide.T

        # (this, but not columns.name, makes it into CSV. it will be top left element.)
        tried_wide.index.name = 'rows=fixed_thr, cols=wAPLKC'
        # TODO rename var to match csv name (~similar)
        # TODO also add row / col index levels for what param_lim_factor we'd need to
        # get each of those steps?
        to_csv(tried_wide, parent_output_dir / 'sparsities_by_params_wide.csv')

        # NOTE: `not in set(...)` check probably doesn't work as intended w/ NaN,
        # but assuming corr_placeholder is not NaN, should be fine
        assert corr_placeholder not in set(
            np.unique(pearson_at_each_param_combo.values)
        )

        # TODO TODO how to deal w/ NaNs prior to spearman calc? do spearman calc in
        # a way that ignores NaN (pretty sure that's default behavior)?
        # TODO save one version w/ dropna first (to keep # of non-NaN input correlations
        # same across corrs that might or might not have NaN)?

        # after transposing, output corr will be of shape:
        # (# param combos, # param combos)
        spearman_of_pearsons = pearson_at_each_param_combo.T.corr(method='spearman')

        # TODO how to get text over (/to side of) ticklabels (to label full name of 2
        # params)? add support to viz.matshow for that?

        level_fn = lambda d: d['fixed_thr']
        group_text = True
        format_fixed_thr = lambda x: f'{x:.0f}'
        # trying to just use this to format last row/col index level (wAPLKC).
        # fixed_thr should be handled by group_text stuff, which i might want to change
        # handling of inside hong2p
        format_wAPLKC = lambda x: f'{x[1]:.1f}'
        xticklabels = format_wAPLKC
        yticklabels = format_wAPLKC

        fig, _ = viz.matshow(spearman_of_pearsons,
            cmap=diverging_cmap,
            vmin=-1.0, vmax=1.0, levels_from_labels=False,
            hline_level_fn=level_fn, vline_level_fn=level_fn,
            hline_group_text=group_text, vline_group_text=group_text,
            group_fontsize=10, xtickrotation='horizontal',
            # TODO change hong2p.viz to have any levels not used to group formatted into
            # label?
            xticklabels=xticklabels, yticklabels=yticklabels,
            vgroup_formatter=format_fixed_thr, hgroup_formatter=format_fixed_thr
        )
        fig.suptitle("Spearman of odor X odor Pearson correlations")
        savefig(fig, parent_output_dir, 'spearman_of_pearsons')

    return params_for_csv


# TODO factor out? replace w/ [light wrapper around] sklearn's minmax_scale fn?
def minmax_scale(data: pd.Series) -> pd.Series:
    scaled = data.copy()
    scaled -= scaled.min()
    scaled /= scaled.max()

    assert np.isclose(scaled.min(), 0)
    assert np.isclose(scaled.max(), 1)

    # TODO delete
    s2 = pd.Series(index=data.index, name=data.name, data=sk_minmax_scale(data))
    # not .equals, but this assertion is true
    assert np.allclose(s2, scaled)
    #
    return scaled


def maxabs_scale(data: pd.Series) -> pd.Series:
    # sklearn.preprocessing.maxabs_scale does not preserve Series input
    # (returns a numpy array)
    return pd.Series(index=data.index, name=data.name, data=sk_maxabs_scale(data))


@produces_output(verbose=False)
# TODO correct type hint for model?
# (statsmodels.regression.linear_model.RegressionResultsWrapper, but maybe something
# more general / not the wrapper?)
def save_model(model, path: Path) -> None:
    model.save(path)


# TODO modify to accept csv path for certain_df too (if that makes it easier to get some
# basic versions of the model runnable for other people)? either way, want to commit
# some example AL data to use.
# TODO rename certain_df to just df (or something less loaded)
# TODO TODO add kwarg to hardcode a dF/F -> spike delta scaling factor (or a
# general fn), and have it bypass the computation of this fn / related plotting/cache.
# (-> use for simple example uses of this fn, hardcoding scale factor = 127 from
# Remy-paper)
def model_mb_responses(certain_df: pd.DataFrame, parent_plot_dir: Path, *,
    roi_depths=None, skip_sensitivity_analysis: bool = False,
    skip_models_with_seeds: bool = False, skip_hallem_models: bool = False) -> None:
    # TODO TODO when is it ok for certain_df to have NaNs? does seem current input has
    # some NaNs, which are only for some odors [for which no odors is NaN for all
    # fly-glomeruli]. any restrictions (if none, why was sam having issues?)
    """
    Args:
        certain_df: dataframe of shape (# odors [including repeats], # fly-glomeruli),
            with dF/F values from [potentially multiple] flies.

            Column index names should be ['date', 'fly_num', 'roi' (i.e. glomerulus
            name)].

            Row index names should be ['panel', 'is_pair', 'odor1', 'odor2', 'repeat']
            (possible that not all are required, but 'panel' and 'odor1' should be)

        parent_plot_dir: where a 'mb_modeling' subdirectory will be created to contain
            any model output directories (and other across-model plots).

            In typical use from `al_analysis.py`, this might be 'pebbled_6f/pdf/ijroi'.
    """
    # TODO (not using roi_depths really, so not a big deal) why do roi_depths seem to
    # have one row per panel, rather than per recording?  current input has column
    # levels ['date', 'fly_num', 'roi'] and row levels ['panel', 'is_pair'])

    # TODO delete. for debugging.
    global _spear_inputs2dfs
    #

    # TODO make and use a subdir in plot_dir (for everything in here, including
    # fit_and_plot... calls)

    plot_dir = parent_plot_dir / 'mb_modeling'

    # TODO w/ a verbose flag to say which odors / glomeruli overlapped

    # I think deltas make more sense to fit than absolute rates, as both can go negative
    # and then we could better filter out points from non-responsive (odor, glomerulus)
    # combinations, if we wanted to.
    hallem_delta = orns.orns(columns='glomerulus', add_sfr=False)

    hallem_delta = abbrev_hallem_odor_index(hallem_delta)

    #our_odors = {olf.parse_odor_name(x) for x in certain_df.index.unique('odor1')}

    # TODO delete?
    # TODO TODO TODO or print intersection of these w/ stuff here (or at least stuff
    # that also matches some nearness criteria on the concentration? where is that
    # currently handled, if it is at all?)
    #hallem_odors = set(hallem_delta.index)

    # as of odors in experiments in the months before 2023-06-30, checked all these
    # are actually not in hallem.
    #
    # this could be a mix of stuff actually not in Hallem and stuff we dont have an
    # abbreviation mapping from full Hallem name. want to rule out the latter.
    # TODO still always print these?
    # TODO delete?
    #unmatched_odors = our_odors - hallem_odors

    # TODO delete
    #validation_odors = {olf.parse_odor_name(x) for x in
    #    certain_df.loc['validation2'].index.unique('odor1')
    #}
    #print(f'{len(validation_odors)=}')
    #print(f'{validation_odors & hallem_odors=}')
    ## (len 7 as expected)
    #print(f'{len(validation_odors & hallem_odors)=}')
    #

    # TODO TODO TODO match glomeruli up to hallem names
    # (may need to make some small decisions)
    # (basically, are there any that are currently unmatched that can be salveaged?)

    # TODO TODO TODO also check which of our_odors are in hallem lower conc data
    # TODO TODO may want to first fix drosolf so it gives us that too
    # (or just read a csv here myself?)
    #
    # odors w/ conc series in Hallem '06 (whether or not we have data):
    # - ea
    # - pa
    # - eb
    # - ms
    # - 1-6ol
    # - 1o3ol
    # - E2-hexenal (t2h)
    # - 2,3-b (i use -5 for this? same question as w/ ga below)
    # - 2h
    # - ga (treat -5 as -6? -4? interpolate?)
    #
    # (each has -2, -4, -6, -8)

    our_glomeruli = set(certain_df.columns.unique('roi'))

    assert hallem_delta.columns.name == 'glomerulus'
    hallem_glomeruli = set(hallem_delta.columns)

    # TODO delete. (after actually checking...)
    # TODO check no naming issues
    # {'DM3+DM5', 'DA4m' (2a), 'VA1d' (88a), 'DA4l' (43a), 'DA3' (23a), 'VA1v' (47b),
    # 'DL3' (65a, 65b, 65c), 'DL4' (49a, 85f)}
    print(f'{(hallem_glomeruli - our_glomeruli)=}')
    #

    # TODO delete. (after actually checking...)
    # TODO TODO check pdf receptor names matches what i get from drosolf w/o passing
    # columns='glomerulus', then use drosolf receptors to check these
    # TODO TODO check receptors of all these are not in hallem
    # TODO TODO TODO print this out and check again. not clear on why DM3 was ever
    # here...
    # - VA2 (92a)
    # - DP1m (Ir64a)
    # - VA4 (85d)
    # - DC2 (13a)
    # - VA7m (UNK)
    # - DA2 (56a, 33a)
    # - VL2a (Ir84a)
    # - DM1 (42b)
    # - VC1 (33c, 85e)
    # - VC2 (71a)
    # - VA7l (46a)
    # - VL1 (Ir75d)
    # - VM7d (42a)
    # - DC4 (Ir64a)
    # - DL2v (Ir75c)
    # - VL2p (Ir31a)
    # - V (Gr21a, Gr63a)
    # - D (69aA, 69aB)
    # - VM7v ("1") (59c)
    # - VC5 (Ir41a)
    # - DL2d (Ir75b)
    # - DP1l (Ir75a)
    # - VA3 (67b)
    # - DC3 (83c)
    print(f'{(our_glomeruli - hallem_glomeruli)=}')
    #

    glomerulus2receptors = orns.task_glomerulus2receptors()

    hallem_glomeruli = np.array(sorted(hallem_glomeruli))
    hallem_glomeruli_in_task = np.array([
        x in glomerulus2receptors.keys() for x in hallem_glomeruli
    ])
    assert set(hallem_glomeruli[~ hallem_glomeruli_in_task]) == {'DM3+DM5'}

    our_glomeruli = np.array(sorted(our_glomeruli))
    our_glomeruli_in_task = np.array([
        x in glomerulus2receptors.keys() for x in our_glomeruli
    ])
    # True for now, but may not always be?
    assert our_glomeruli_in_task.all()

    unmodified_orn_dff_input_df = certain_df.copy()

    # TODO TODO assert that 'is_pair' is all False? maybe drop it in a separate step
    # before this? (want to also drop is_pair for roi_depths in a consistent way.
    assert (certain_df.index.get_level_values('is_pair') == False).all()
    certain_df = certain_df.droplevel('is_pair', axis='index')
    if roi_depths is not None:
        assert (roi_depths.index.get_level_values('is_pair') == False).all()
        roi_depths = roi_depths.droplevel('is_pair', axis='index')

    drop_pfo = True
    # shouldn't be in megamat/validation/diagnostic data. added for new kiwi/control
    # data.
    if drop_pfo:
        odor_names = certain_df.index.get_level_values('odor1').map(
            olf.parse_odor_name
        )
        odor_concs = certain_df.index.get_level_values('odor1').map(
            olf.parse_log10_conc
        )

        pfo_mask = odor_names == 'pfo'
        if pfo_mask.any():
            pfo_conc_set = set(odor_concs[pfo_mask])

            # TODO if this gets triggered by None/NaN, adapt to also include None/NaN
            # (if there are any negative float concs, that would indicate a bug)
            #
            # NOTE: {0.0} == {0} is True
            assert pfo_conc_set == {0}

            # TODO warn that we are dropping (if any actually dropped)
            certain_df = certain_df.loc[~pfo_mask].copy()


    index_df = certain_df.index.to_frame(index=False)

    odor1_names = index_df.odor1.apply(olf.parse_odor_name)
    mix_strs = (
        odor1_names + '+' +
        index_df.odor2.apply(olf.parse_odor_name) + ' (air mix) @ 0'
    )
    # TODO work as-is (seems to...)? need to subset RHS to be same shape?
    # (could add assertions that other part, i.e. `index_df.odor2 == solvent_str`
    # doesn't change)
    index_df.loc[index_df.odor2 != solvent_str, 'odor1'] = mix_strs

    # NOTE: see panel2name_order modifications in natmix.olf.panel2name_order, which
    # define order of these hacky new "odors" ('kmix0','kmix-1',...'cmix0','cmix-1',...)
    #
    # e.g. 'kmix @ 0' -> 'kmix0' (so concentration not recognized as such, and thus it
    # should work in modelling code that currently strips that)
    hack_strs_to_fix_mix_dilutions = (
        odor1_names + index_df.odor1.apply(olf.parse_log10_conc).map(
            lambda x: f'{x:.0f}'
        )
    )
    # expecting all these to get stripped off in modelling code
    hack_strs_to_fix_mix_dilutions = hack_strs_to_fix_mix_dilutions + ' @ 0'

    index_df.loc[odor1_names.str.endswith('mix'), 'odor1'] = \
        hack_strs_to_fix_mix_dilutions

    certain_df.index = pd.MultiIndex.from_frame(index_df)
    del index_df

    # after above two hacks, for kiwi/control data, sort_odors should order odors as:
    # pfo, components, 2-component mix, 2-component air mix, 5-component mix, dilutions
    # of 5-component mix (with more dilute mixtures further towards the end)
    certain_df = sort_odors(certain_df)

    # TODO TODO may want to preserve panel just so i can fit dF/F -> spike delta fn
    # on all, then subset to specific panels for certain plots
    #
    # TODO maybe ['panel', 'odor1']? or just drop diagnostic panel 'ms @ -3'?
    # TODO sort=False? (since i didn't have that pre-panel support, may need to sort to
    # compare to that output, regardless...)
    fly_mean_df = certain_df.groupby(['panel', 'odor1'], sort=False).mean()
    # TODO delete? restore and change code to expect 'odor' instead of 'odor1'?
    # this is just to rename 'odor1' -> 'odor'
    fly_mean_df.index.names = ['panel', 'odor']

    n_before = num_notnull(fly_mean_df)
    shape_before = fly_mean_df.shape

    # TODO actually helpful to drop ['date', 'fly_num'] cols? keeping could make
    # summarizing model input easier later... (storing alongside in fly_ids for now)
    fly_mean_df = util.add_group_id(fly_mean_df.T.reset_index(), ['date', 'fly_num'],
        name='fly_id'
    )

    fly_ids = fly_mean_df[['fly_id','date','fly_num']].drop_duplicates()
    # column level names kinda non-sensical at this intermediate ['panel', 'odor'], but
    # I think it's all fine again by end of reshaping (shape, #-not-null, and set of
    # dtypes don't change)
    fly_ids = fly_ids.droplevel('odor', axis='columns')
    # nulling out nonsensical 'panel' name
    fly_ids.columns.name = None

    fly_ids = fly_ids.set_index('fly_id')

    fly_mean_df = fly_mean_df.set_index(['fly_id', 'roi']).drop(
        columns=['date', 'fly_num'], level=0).T

    # TODO replace w/ call just renaming 'roi'->'glomerulus'
    assert 'fly_id' == fly_mean_df.columns.names[0]
    # TODO also assert len of names and/or names[1] is 'roi'?
    fly_mean_df.columns.names = ['fly_id', 'glomerulus']

    assert num_notnull(fly_mean_df) == n_before
    assert fly_mean_df.shape == shape_before
    assert set(fly_mean_df.dtypes) == {np.dtype('float64')}

    # TODO delete? here and elsewhere? (was before fly_mean_df code)
    mean_df = fly_mean_df.groupby('glomerulus', axis='columns').mean()

    # TODO factor out?
    def melt_odor_by_glom_responses(df, value_name):
        n_before = num_notnull(df)
        df = df.melt(value_name=value_name, ignore_index=False)
        assert num_notnull(df[value_name]) == n_before
        return df

    # TODO factor into fn alongside current abbrev handling
    #
    # TODO actually check this? reason to think this? why did remy originally choose to
    # do -3 for everything? PID?
    # (don't think it was b/c they had reason to think that was the best intensity-match
    # of the Hallem olfactometer... think it might have just been fear of
    # contamination...)?
    #
    # TODO make adjustments for everything else then?
    # TODO TODO guess-and-check scalar adjustment factor to decrease all hallem spike
    # deltas to make more like our -3? or not matter / scalar not helpful?
    hope_hallem_minus2_is_our_minus3 = True
    if hope_hallem_minus2_is_our_minus3:
        warn('treating all Hallem data as if -2 on their olfactometer is comparable to'
            ' -3 on ours (for estimating dF/F -> spike rate fn)'
        )
        # TODO TODO pass abbreved + conc added hallem to model_mb... fn? (to not
        # recompute there...)
        # TODO maybe it'd be more natural to pass in our data, and round all concs to:
        # -2,-4,-6,-8? might simplify consideration across this case + hallem conc
        # series case?
        hallem_delta.index += ' @ -3'
    else:
        raise NotImplementedError('no alternative at the moment...')

    # TODO TODO allow slop of +/- 1 order of magnitude in general for merging w/
    # hallem (for validation stuff in particular)?

    dff_col = 'delta_f_over_f'
    spike_delta_col = 'delta_spike_rate'
    est_spike_delta_col = f'est_{spike_delta_col}'

    # TODO TODO delete all mean_df stuff if i get fly_mean_df version working
    # (which i think i have?)?
    # (or just scale w/in each fly before reducing fly_mean_df -> mean_df)
    # (or make choice to take mean right before plotting (to switch easier?)?
    # plus then it would work post-scaling, which is what i would want)
    mean_df = melt_odor_by_glom_responses(mean_df, dff_col)
    #

    n_notnull_before = num_notnull(fly_mean_df)
    n_null_before = num_null(fly_mean_df)

    if roi_depths is not None:
        assert fly_mean_df.columns.get_level_values('glomerulus').equals(
            roi_depths.columns.get_level_values('roi')
        )
        # to also replace the ['date','fly_num'] levels w/ 'fly_id, as was done to
        # fly_mean_df above
        roi_depths.columns = fly_mean_df.columns.copy()

    fly_mean_df = melt_odor_by_glom_responses(fly_mean_df, dff_col)

    roi_depth_col = 'roi_depth_um'

    if roi_depths is not None:
        # should be ['panel', 'odor']
        index_levels_before = fly_mean_df.index.names
        shape_before = fly_mean_df.shape

        roi_depths = melt_odor_by_glom_responses(roi_depths, roi_depth_col
            ).reset_index()

        fly_mean_df = fly_mean_df.reset_index().merge(roi_depths,
            on=['panel', 'fly_id', 'glomerulus']
        )

        fly_mean_df = fly_mean_df.set_index(index_levels_before)

        assert fly_mean_df.shape[0] == shape_before[0]
        assert fly_mean_df.shape[-1] == (shape_before[-1] + 1)

    assert num_notnull(fly_mean_df[dff_col]) == n_notnull_before
    assert num_null(fly_mean_df[dff_col]) == n_null_before

    fly_mean_df = fly_mean_df.dropna(subset=[dff_col])
    if roi_depths is not None:
        # TODO and should i also check we aren't dropping stuff that's non-NaN in depth
        # col (by dropna on dff_col) (no, some odors are nulled before)?
        #
        # this should be defined whenever dff_col is
        assert not fly_mean_df[roi_depth_col].isna().any()

    assert num_notnull(fly_mean_df[dff_col]) == n_notnull_before
    assert num_null(fly_mean_df) == 0

    # TODO delete if ends up being easier (in terms of less postprocessing) to subset
    # out + reshape stuff from merged tidy df
    hallem_delta_wide = hallem_delta.copy()
    #
    hallem_delta = melt_odor_by_glom_responses(hallem_delta, spike_delta_col)


    def scaling_method_to_col(method: Optional[str]) -> str:
        if method is None:
            return dff_col
        else:
            return f'{method}_scaled_{dff_col}'


    # quantile 0 = min, 1 = max. after unstacking, columns will be quantiles.
    fly_quantiles = fly_mean_df.groupby('fly_id')[dff_col].quantile(
        [0, 0.01, 0.05, 0.5, 0.95, 0.99, 1]).unstack()

    avg_flymin = fly_quantiles[0].mean()
    avg_flymax = fly_quantiles[1].mean()

    # TODO TODO compare to quantiles after applying new transform i come up with
    #
    # NOTE: seems to be more variation in upper end than in inhibitory values
    # TODO maybe i should be scaling the two sides diff then?
    #
    #             0.00      0.01      0.05      0.50      0.95      0.99      1.00
    # fly_id
    # 1      -0.393723 -0.185341 -0.066075  0.088996  0.771770  1.408845  2.061222
    # 2      -0.265394 -0.113677 -0.023615  0.075681  0.546017  0.915291  1.209619
    # 3      -0.290391 -0.151183 -0.032448  0.082085  0.541723  0.831624  1.747735
    # 4      -0.284210 -0.142273 -0.026323  0.103612  0.690146  1.038761  2.208872
    # 5      -0.450574 -0.142619 -0.033605  0.135551  1.006864  1.508309  2.735914
    # 6      -0.469455 -0.135344 -0.033547  0.091594  0.722606  1.246020  1.969335
    # 7      -0.301425 -0.172252 -0.039677  0.121347  0.739866  1.235905  3.101158
    # 8      -0.377237 -0.180162 -0.048570  0.114664  0.958002  1.807228  2.692789
    # 9      -0.316237 -0.066018 -0.021773  0.058443  0.525360  0.952334  1.579931
    # 10     -0.449154 -0.238127 -0.077457  0.091946  0.841725  1.547193  2.411627
    # 11     -0.444483 -0.228012 -0.073913  0.121594  0.808650  1.315351  2.476759
    # 12     -0.524139 -0.212304 -0.065836  0.079312  0.683135  1.259982  2.623360
    # 13     -0.373534 -0.193339 -0.057145  0.141604  1.019016  1.895920  3.960306
    # 14     -0.351293 -0.218182 -0.056444  0.093895  0.878827  1.745915  2.475087

    # TODO share w/ plots from model fitting below?
    dff_desc = f'mean glomerulus {dff_latex}'
    # TODO refactor to preprend dff_desc inside loop (rather than manually for each
    # of these)
    scaling_method2desc = {
        None: dff_desc,

        #'minmax': f'{dff_desc}\n[0,1] scaled within fly',

        'zscore': f'{dff_desc}\nZ-scored within fly',

        'maxabs': dff_desc + '\n$fly_{max} \\rightarrow 1$, 0-preserved',

        'to-avg-max':
            dff_desc + '\n$fly_{max} \\rightarrow \overline{fly_{max}}$, 0-preserved',

        'split-minmax':
            # TODO latex working yet?
            f'{dff_desc}\n$+ \\rightarrow [0, 1]$\n$- \\rightarrow [-1, 0]$',

        'split-minmax-to-avg': (dff_desc +
            # TODO latex working yet?
            '\n$+ \\rightarrow [0, \overline{fly_{max}}]$\n'
            '$- \\rightarrow [\overline{fly_{min}}, 0]$'
        ),
    }

    # TODO factor out?
    # TODO rename to "add_scaled_dff_col" or something?
    def scale_one_fly(gdf, method='zscore'):
        """Adds <method>_scaled_<dff_col> column with scaled <dff_col> values.

        Does not change any existing columns of input.
        """
        assert not gdf.fly_id.isna().any() and gdf.fly_id.nunique() == 1
        col_to_scale = dff_col
        to_scale = gdf[col_to_scale]
        n_nan_before = to_scale.isna().sum()

        new_dff_col = scaling_method_to_col(method)
        assert new_dff_col not in gdf.columns

        if method == 'minmax':
            scaled = minmax_scale(to_scale)

        elif method == 'zscore':
            scaled = (to_scale - to_scale.mean()) / to_scale.std()

        # TODO maybe try a variant of 'zscore' where we dont subtract mean first? (b/c
        # want to preserve 0) (std() doesn't seem that related to fly maxes... not
        # encouraging for this strategy)

        # also preserves 0, like split-minmax* methods below, but just one scalar
        # applied to all data
        # (new min will be > -1 (and < 0), assuming abs(min) < abs(max) (and neg min))
        elif method in ('maxabs', 'to-avg-max'):
            scaled = maxabs_scale(to_scale)

            if method == 'to-avg-max':
                # in theory, max(abs) could come from negative values, but the data
                # should have larger positive dF/F, so that shouldn't happen
                assert np.isclose(scaled.max(), 1)
                scaled *= avg_flymax
                assert np.isclose(scaled.max(), avg_flymax)

        elif method in ('split-minmax', 'split-minmax-to-avg'):
            # TODO warn if no negative values in input (tho there should always be as
            # i'm currently using it) (prob fine to keep as assertion for now)
            assert (to_scale < 0).any()

            # NOTE: to_scale.index.duplicated().any() == True, so probably can't use
            # index as-is to split/re-combine data
            # TODO delete if not needed
            index = to_scale.index
            to_scale = to_scale.reset_index(drop=True)
            #

            neg = to_scale < 0
            nonneg = to_scale >= 0
            n_neg = neg.sum()
            assert len(to_scale) == (n_neg + nonneg.sum() + to_scale.isna().sum())

            scaled = to_scale.copy()
            scaled[nonneg] = minmax_scale(scaled[nonneg])

            # after minmax_scale, just scaled * (max - min) + min, to go to new range
            scaled[neg] = minmax_scale(scaled[neg]) - 1
            assert np.isclose(scaled.min(), -1)
            assert np.isclose(scaled[neg].max(), 0)

            if method == 'split-minmax-to-avg':
                scaled[nonneg] *= avg_flymax
                scaled[neg] *= abs(avg_flymin)

            # not true b/c some max of to_scale[neg] gets mapped to 0, presumably
            #assert n_neg == (scaled < 0).sum()
            # rhs here can also include 0 from min of to_scale[nonneg]
            assert n_neg <= (scaled <= 0).sum()
            assert (scaled < 0).any()

            # so it's not really re-ordering anything. that's good.
            assert np.array_equal(
                np.argsort(scaled[scaled != 0]),
                np.argsort(gdf.reset_index()[scaled != 0][col_to_scale])
            )

            # TODO delete if i remove related code changing index above
            scaled.index = index

        # TODO try pinning particular odor(s)? how?
        # TODO maybe use diags for the pinning, to share w/ validation panel flies more
        # easily?

        else:
            raise NotImplementedError(f'scaling {method=} not supported')

        assert scaled.isna().sum() == n_nan_before, 'scaling changed number of NaN'
        gdf[new_dff_col] = scaled
        return gdf

    columns_before = fly_mean_df.columns

    methods = [
        # TODO delete
        #'minmax',

        'zscore',
        'maxabs',
        'to-avg-max',
        'split-minmax',
        'split-minmax-to-avg',
    ]
    for method in methods:
        # each of these calls adds a new column, with a scaled version of dff_col.
        fly_mean_df = fly_mean_df.groupby('fly_id', sort=False).apply(
            lambda x: scale_one_fly(x, method=method)
        )

    # TODO recompute and compare quartiles?
    # TODO replace w/ refactoring loop over scaling_method2desc.items() to loop over
    # scaling methods from added columns? (would need to track scaling methods for the
    # added cols, probably in scale_one_fly?)
    # (or just `continue` if column not in df...)
    # (OR now could prob use `methods` list above)
    scaled_cols = [c for c in fly_mean_df.columns if c not in columns_before]
    # to ensure we are making plots for each scaled column added
    assert (
        {scaling_method_to_col(x) for x in scaling_method2desc.keys()} ==
        {dff_col} | set(scaled_cols)
    )
    #

    # TODO delete
    # TODO better name for df... (or factor to fn so it doesn't matter)
    # (renamed fdf->merged_dff_and_hallem. need to also rename this, or probably just
    # delete it)
    #
    # doesn't seem to matter that odor is index and glomerulus is column. equiv to:
    # pd.merge(mean_df.reset_index(), hallem_delta.reset_index(),
    #     on=['odor', 'glomerulus']
    # )
    #df = mean_df.merge(hallem_delta, on=['odor', 'glomerulus']).reset_index()

    # TODO TODO decide how to handle panel when merging w/ hallem
    # (mean first for fitting dF/F -> spike delta fn, but then separately merge w/in
    # each panel for running model?)
    # (what is currently happening?)A

    # TODO TODO delete? or move to before odor2 level effectively dropped?
    # TODO gate behind there being and odor2 level?
    #
    # TODO to make this merging easier, might actually want to format mixtures down to
    # one str column, so that if [hypothetically, not in current data] odor1=solvent and
    # odor2 is in hallem, we can still match it up
    # TODO just rename hallem 'odor' -> 'odor1', and reset_index() on both
    # TODO add solvent odor2 to hallem and merge on=(odor_cols + ['glomerulus'])?
    #
    # if we only have odor2 odors for mixtures where odor1 is also an odor, we would
    # never want to merge those with hallem anyway, so we can just drop those rows
    # before merging
    '''
    odor1 = fly_mean_df.index.get_level_values('odor1')
    odor2 = fly_mean_df.index.get_level_values('odor2')
    # TODO TODO warn / err if any of this odor2 stuff is actually != solvent_str?
    # (since not currently supporting that, nor thinking that's the way i'll try to do
    # it...)
    assert not (odor1[odor2 != solvent_str] == solvent_str).any()
    # .reset_index() b/c left_on didn't seem to work w/ a mix of cols and index levels
    for_merging = fly_mean_df[odor2 == solvent_str].reset_index()

    merged_dff_and_hallem = for_merging.merge(hallem_delta,
    '''
    merged_dff_and_hallem = fly_mean_df.reset_index().merge(hallem_delta,
        left_on=['odor', 'glomerulus'], right_on=['odor', 'glomerulus']
    ).reset_index()

    assert not merged_dff_and_hallem[spike_delta_col].isna().any()

    # TODO delete
    # (note this was before 'odor'->odor_cols change)
    # TODO what's going on here? don't i have geraniol dF/F data?
    # (2024-05-09: can't repro, at least not passing all data as input. maybe passing
    # just validation2? not seeing geraniol at all now tho... that an issue?
    # can't repro w/ that input either. maybe if i don't use consensus df for input?
    # that'd probably still be dropped above tho...)
    #
    # must have been one that changed conc? after also excluding flies 2023-10-15/1,2
    # and 2023-10-19/2 (and 2024-01-05/4, not that I think this one mattered here), now
    # I'm getting the empty set for this (and geraniol not here anymore, as it's been -2
    # since 2023-11-19)
    #ipdb> set(merged_dff_and_hallem.odor.unique()) -
    # set(merged_dff_and_hallem.dropna().odor.unique())
    #{'ger @ -3'}

    # TODO print odors left after merging. something like
    # sorted(merged_dff_and_hallem.odor.unique())
    # TODO print # of (fly X glomeruli) combos (at least those that overlap w/
    # hallem) too, for each odor

    # TODO filter out low intensity stuff? (more points there + maybe more noise in
    # dF/F)
    # TODO fit from full matrix input rather than just each glomerulus as attempt at
    # ephaptic stuff?

    # TODO also print / save fly_id -> (date, fly_num) legend
    assert not merged_dff_and_hallem.fly_id.isna().any(), 'nunique does not count NaN'
    fly_palette = dict(zip(
        sorted(merged_dff_and_hallem.fly_id.unique()),
        sns.color_palette(cc.glasbey, merged_dff_and_hallem.fly_id.nunique())
    ))

    # still too hard to see density when many overlap, but 0.2 also has that issue, and
    # too hard to make out single fly colors at that point (when points arent
    # overlapping)
    scatterplot_alpha = 0.3
    # existing values in fly_palette are 3-tuples (color w/o alpha)
    fly_palette = {f: c + (scatterplot_alpha,) for f, c in fly_palette.items()}

    _cprint_color = 'blue'
    # hack to tell whether we should fit model (if input is megamat panel [which
    # overlaps well enough w/ hallem], and has at least 7 flies there, we should).
    # otherwise, we should try to load a saved model, and use that.
    try:
        # TODO delete
        '''
        if len(certain_df.loc['megamat'].dropna(axis='columns', how='all'
            ).columns.to_frame(index=False)[['date','fly_num']].drop_duplicates()
            ) >= 7:
        '''

        # TODO TODO TODO should i be recomputing now that i collected the new
        # kiwi/control data after the imaging system had a lot of time to drift?
        # check hists of dF/F / something (to compare old megamat/etc to new
        # kiwi/control data)?
        # TODO TODO maybe now i should switch to always z-scoring/similar the dF/F input
        # (before passing thru a fn to get spike deltas out), so that i can tune on one
        # panel and run on another more easily? (current attempt to tune on megamat and
        # run on control/kiwi had all silent cells in first call)

        # TODO TODO cleaner solution for this hack (probably involving preserving
        # panel throughout, and splitting each panel out before passing thru model, then
        # just always recompute model and do all in one run? now doing a prior run just
        # to save model, then later runs to pass each particular panel thru model)
        #
        # hack to only fit model if we are passing all panel (including validation)
        # data, on all flies (shape is 198x517 there)
        if (certain_df.shape[1] > 500 and len(certain_df) > 150 and

                set(certain_df.index.get_level_values('panel')) == {
                    'megamat', 'validation2', 'glomeruli_diagnostics'
                } and len(
                    certain_df.columns.to_frame(index=False)[['date','fly_num']
                        ].drop_duplicates()
                # NOTE: 9 final megamat flies and 5 final validation2 flies (after
                # dropping the 1 Betty wanted). see reproducing.md or
                # CSVs under data/sent_to_anoop/v1 for the specific flies.
                ) == (9 + 5)
            ):

            use_saved_dff_to_spiking_model = False
        else:
            use_saved_dff_to_spiking_model = True

    # TODO what exacty triggers this? doc in comment
    except KeyError:
        use_saved_dff_to_spiking_model = True

    # this option currently can't actually trigger recomputation that wouldn't happen
    # anyway... (always recomputed if input data is large enough, never otherwise)
    # TODO delete this option then?
    if use_saved_dff_to_spiking_model and should_ignore_existing('dff2spiking'):
        warn('would NOT have saved dff->spiking model, but requested regeneration of '
            'it!\n\nchange args to run on all data (so that model would get saved. see '
            'reproducing.md), OR remove `-i dff2spiking` option.'
            '\n\nexiting!'
        )
        sys.exit()

    # for histograms of dF/F, transformed versions, or estimated spike deltas derived
    # from one of the former
    n_bins = 50

    if not use_saved_dff_to_spiking_model:
        # just for histogram in loop below
        tidy_merged = merged_dff_and_hallem.reset_index()
        tidy_pebbled = fly_mean_df.reset_index()

        # TODO TODO may want to also plot on just panel subsets (here? or below, but
        # easier to do on diff scaling choices here, if i want that)
        # (only panel subsets for hist plots, not linear fit plots?)

        for scaling_method, col_desc in scaling_method2desc.items():
            curr_dff_col = scaling_method_to_col(scaling_method)
            assert curr_dff_col in merged_dff_and_hallem, f'missing {curr_dff_col}'

            # TODO put all these hists into a subdir? cluttering folder...

            fig, ax = plt.subplots()
            sns.histplot(data=tidy_merged, x=curr_dff_col, bins=n_bins, ax=ax)
            ax.set_title('pebbled (only odors & receptors also in Hallem)')
            # should be same subset of data used to fit dF/F->spiking model
            # (and same values, when scaling method matches scaling_method_to_use)
            savefig(fig, plot_dir, f'hist_pebbled_hallem-overlap_{curr_dff_col}')

            fig, ax = plt.subplots()
            sns.histplot(data=tidy_pebbled, x=curr_dff_col, bins=n_bins, ax=ax)
            ax.set_title('all pebbled')
            # TODO why this (and others) getting overwritten when being run w/ -C?
            # same w/ -c now (yes)? -P matter (don't think so)? seems to be a font
            # spacing issue for the most part? not sure why i'm just seeing it now
            # (2025)
            savefig(fig, plot_dir, f'hist_pebbled_{curr_dff_col}')

            # TODO also hist megamat subset of each of these? or at least of the pebbled
            # itself?
            # TODO or just loop over panels? easier below?


    # TODO iterate over options (just None and 'to-avg-max'? others worse) and verify
    # that what i'm using is actually the best (or not far off)?
    #scaling_method_to_use = None
    # 'to-avg-max'/'split-minmax-to-avg'/None all produce extremely visually similar
    # megamat/est_orn_spike_deltas*.pdf plots (including the correlation plots)
    # (as expected, since they keep 0)
    scaling_method_to_use = 'to-avg-max'

    add_constant = False

    # tested w/ None, 'split-minmax', and 'split-minmax-to-avg'. in all cases, the fit
    # on the negative dF/F data (and aligned subset of Hallem data) looked very bad (fit
    # was equivalent across the 2 cases, as expected). slope was negative, so more
    # negative dF/F meant less inhibition, which is nonsense.
    #
    # scaling methods were verified to not be re-ordering negative component of data.
    #
    # TODO maybe also plot just negative dF/F data (w/ aligned hallem data), to sanity
    # check the fact i was getting negative slopes?
    separate_inh_model = False

    col_to_fit = scaling_method_to_col(scaling_method_to_use)

    # TODO factor all model fitting + plotting (w/ CIs) into some hong2p fns?
    # TODO factor statsmodels fitting (+ plotting as matches seaborn)
    # into hong2p.viz (-> share w/ use in
    # natural_odors/scripts/kristina/lit_total_conc_est.py)

    # TODO refactor to move type of model to one place above?
    # NOTE: RegressionResults does not seem to be a subclass of
    # RegressionResultsWrapper. sad.
    # TODO move outside this fn
    def fit_dff2spiking_model(to_fit: pd.DataFrame) -> Tuple[RegressionResultsWrapper,
        Optional[RegressionResultsWrapper]]:

        # would need to dropna otherwise
        assert not to_fit.isna().any().any()
        to_fit = to_fit.copy()
        y_train = to_fit[spike_delta_col]

        # TODO try adding (0, 0) as as point, even if still using Ax+b as a model? is
        # that actually a valid practice? probably not, right?

        if add_constant:
            X_train = sm.add_constant(to_fit[col_to_fit])
        else:
            X_train = to_fit[col_to_fit].to_frame()

        if not separate_inh_model:
            # TODO why does this model produce a different result from the seaborn call
            # above (can tell by zooming in on upper right region of plot)??
            # TODO rename to "results"? technically the .fit() returns a results wrapper
            # or something (and do i only want to serialize the model part? can that
            # even store the parameters separately) (online info seems to say it should
            # return RegressionResults, so not sure why i'm getting
            # RegressionResultsWrapper...)
            model = sm.OLS(y_train, X_train).fit()
            inh_model = None
        else:
            nonneg = X_train[col_to_fit] >= 0
            neg = X_train[col_to_fit] < 0

            model = sm.OLS(y_train[nonneg], X_train[nonneg]).fit()
            inh_model = sm.OLS(y_train[neg], X_train[neg]).fit()

        return model, inh_model


    # TODO move outside this fn
    def predict_spiking_from_dff(df: pd.DataFrame, model: RegressionResultsWrapper,
        inh_model: Optional[RegressionResultsWrapper] = None, *, alpha=0.05,
        ) -> pd.DataFrame:
        """
        Returns dataframe with 3 additional columns: [est_spike_delta_col,
        <est_spike_delta_col>_ci_[lower|upper] ]
        """
        # TODO doc input requirements

        # TODO delete unless add_constant line below w/ series input might mutate df
        # (unlikely)
        df = df.copy()

        # would otherwise need to dropna
        assert not df.isna().any().any()

        # TODO assert saved model only has const term if add_constant?
        # do above where we load model + choices?

        # TODO see if this can be replaced w/ below (and do in other place if so)
        if add_constant:
            # returns a DataFrame w/ an extra 'const' col (=1.0 everywhere)
            X = sm.add_constant(df[col_to_fit])
        else:
            X = df[col_to_fit].to_frame()

        if not separate_inh_model:
            y_pred = model.get_prediction(X)

            # TODO what are obs_ci_[lower|upper] cols? i assume i'm right to use
            # mean_ci_[upper|lower] instead (seems so)?
            # https://stackoverflow.com/questions/60963178
            #
            # alpha=0.05 by default (in statsmodels, if not passed)
            pred_df = y_pred.summary_frame(alpha=alpha)

            predicted = y_pred.predicted
        else:
            # fly_mean_df input (call where important estimates get added) currently has
            # an index that would fail the verify_integrity=True checks below, so saving
            # this index to restore later.
            # TODO do i actually need to restore index tho?
            # TODO delete?
            #index = X.index
            X = X.reset_index(drop=True)

            nonneg = X[col_to_fit] >= 0
            neg = X[col_to_fit] < 0

            y_pred_nonneg = model.get_prediction(X[nonneg])
            y_pred_neg = inh_model.get_prediction(X[neg])

            pred_df_nonneg = y_pred_nonneg.summary_frame(alpha=alpha)
            pred_df_neg = y_pred_neg.summary_frame(alpha=alpha)

            predicted_nonneg = pd.Series(
                data=y_pred_nonneg.predicted, index=X[nonneg].index
            )
            predicted_neg = pd.Series(data=y_pred_neg.predicted, index=X[neg].index)

            pred_df = pd.concat([pred_df_nonneg, pred_df_neg], verify_integrity=True)

            # just on the RangeIndex of input (should have all consecutive indices from
            # start to end after concatenating)
            pred_df = pred_df.sort_index()
            assert pred_df.index.equals(X.index)

            predicted = pd.concat([predicted_nonneg, predicted_neg],
                verify_integrity=True
            )
            predicted = predicted.sort_index()
            assert predicted.index.equals(X.index)

            # TODO TODO restore (w/ above)? will this make predict_spiking_from_dff fn
            # return val make more sense? what was issue again?
            # TODO delete?
            #X.index = index

        # NOTE: .get_prediction(...) seems to return an object where more information is
        # available about the fit (e.g. confidence intervals, etc). .predict(...) will
        # just return simple output of model (same as <PredictionResult>.predicted).
        # (also seems to be same as pred_df['mean'])
        assert np.array_equal(predicted, pred_df['mean'])
        if not separate_inh_model:
            assert np.array_equal(predicted, model.predict(X))

        # TODO was this broken in separate inh case? (still think that case was
        # probably a dead end, so not necessarily worth fixing...)
        # (but megamat/est_orn_spike_deltas[_corr].pdf plots were all NaN it seems?)
        # (not sure i can repro)
        df[est_spike_delta_col] = predicted

        # TODO how are these CI's actually computed? how does that differ from how
        # seaborn computes them? why are they different?
        # TODO what are obs_ci_[lower|upper]? seems newer versions of statsmodels might
        # not have them anyway? or at least they aren't documented...
        for c in ('mean_ci_lower', 'mean_ci_upper'):
            df[f'{est_spike_delta_col}{c.replace("mean", "")}'] = pred_df[c]

        # TODO sort df by est_spike_delta_col before returning? would that make
        # plotting fn avoid need to do that? or prob just do in plotting fn...
        return df


    # TODO (reword to make accurate again) delete _model kwarg.
    # just using to test serialization of OLS model, since i can't figure out why this
    # equality check fails (no .equals avail):
    # > model.save('test_model.p')
    # > deserialized_model = sm.load('test_model.p')
    # > deserialized_model == model
    # False
    # > deserialized_model.remove_data()
    # > model.remove_data()
    # > deserialized_model == model
    # False
    def plot_dff2spiking_fit(df: pd.DataFrame, model: RegressionResultsWrapper,
        inh_model: Optional[RegressionResultsWrapper] = None, *, scatter=True,
        title_prefix=''):
        """
        Args:
            scatter: if True, scatterplot merged data w/ a hue for each fly. otherwise,
                plot a 2d histogram of data.
        """
        assert col_to_fit in df.columns
        assert spike_delta_col in df.columns

        ci_lower_col = f'{est_spike_delta_col}_ci_lower'
        ci_upper_col = f'{est_spike_delta_col}_ci_upper'
        est_cols = (est_spike_delta_col, ci_lower_col, ci_upper_col)
        assert all(x not in df.columns for x in est_cols)

        if separate_inh_model:
            assert inh_model is not None
        else:
            assert inh_model is None

        # functions passed to FacetGrid.map[_dataframe] must plot to current Axes
        ax = plt.gca()

        # TODO was seaborn results suggesting i wanted alpha 0.025 for 95% CI here?
        # (honestly, seaborn CI [which is supposedly 95%, tho bootstrapped] looks wider
        # in all cases)
        #
        # from looking at statsmodels code, I'm pretty sure their "95% CI" is centered
        # on estimate, w/ alpha/2 on either side (so 0.05 correct for 95%, not 0.025)
        alpha_for_ci = 0.05

        # TODO include what alpha is in name of cols returned from predict (-> delete
        # explicit pass-in here)?
        df = predict_spiking_from_dff(df, model, inh_model, alpha=alpha_for_ci)

        assert all(x in df.columns for x in est_cols)

        plot_kws = dict(ax=ax, data=df, y=spike_delta_col, x=col_to_fit)
        if scatter:
            sns.scatterplot(hue='fly_id', palette=fly_palette, legend='full',
                edgecolors='none', **plot_kws
            )
        else:
            # TODO set bins= (seems OK w/o)?
            #
            # default blue 2d hist color would probably not work well w/ current blue
            # fit line
            sns.histplot(color='red', cbar=True, **plot_kws)

        # TODO can i replace all est_df below w/ just df?

        xs = df[col_to_fit]
        if not separate_inh_model:
            est_df = df
        else:
            neg = df[col_to_fit] < 0
            nonneg = df[col_to_fit] >= 0

            est_df = df[nonneg]
            xs = xs[nonneg]

        # sorting was necessary for fill_between below to work correctly
        sorted_indices = np.argsort(xs).values
        xs = xs.iloc[sorted_indices]
        est_df = est_df.iloc[sorted_indices]

        color = 'blue'
        fill_between_kws = dict(alpha=0.2,
            # TODO each of these needed? try to recreate seaborn (set color_palette the
            # same / use that seaborn blue?)
            linestyle='', linewidth=0, edgecolor='white'
        )
        ax.plot(xs, est_df[est_spike_delta_col], color=color)
        ax.fill_between(xs, est_df[ci_lower_col], est_df[ci_upper_col], color=color,
            **fill_between_kws
        )
        if separate_inh_model:
            inh_color = 'red'
            # pylint: disable=possibly-used-before-assignment
            xs = df[col_to_fit][neg]
            est_df = df[neg]

            # TODO refactor to share w/ above?
            sorted_indices = np.argsort(xs).values
            xs = xs.iloc[sorted_indices]
            est_df = est_df.iloc[sorted_indices]

            ax.plot(xs, est_df[est_spike_delta_col], color=inh_color)
            ax.fill_between(xs, est_df[ci_lower_col], est_df[ci_upper_col],
                color=inh_color, **fill_between_kws
            )

        if add_constant:
            # TODO refactor
            model_eq = (f'$\\Delta$ $spike$ $rate = {model.params[col_to_fit]:.1f} x + '
                f'{model.params["const"]:.1f}$'
            )
            # TODO assert no other parameters besides col_to_fit and const?
        else:
            model_eq = f'$\\Delta$ $spike$ $rate = {model.params[col_to_fit]:.1f} x$'
            # TODO assert no other parameters besides col_to_fit?

        if separate_inh_model:
            assert not add_constant, 'not yet implemented'
            model_eq = (f'{model_eq}\n$\\Delta$ '
                f'$spike$ $rate_{{inh}} = {inh_model.params[col_to_fit]:.1f} x$'
            )

        y_train = df[spike_delta_col]

        # https://en.wikipedia.org/wiki/Coefficient_of_determination
        ss_res = ((y_train - df[est_spike_delta_col])**2).sum()

        ss_tot = ((y_train - y_train.mean())**2).sum()

        # TODO make sense that this can be negative??? (for some of the glomerulus
        # specific fits. see by-glom_dff_vs_hallem__dff_scale-to-avg-max.pdf)
        # think so: https://stats.stackexchange.com/questions/12900
        # just means fit is worse than a horizontal line?
        #
        # using this R**2 just temporarily to be more comparable to values
        # reported for add_constant=True cases
        r_squared = 1 - ss_res / ss_tot

        if add_constant:
            assert np.isclose(r_squared, model.rsquared)

        # for why R**2 (as reported by model.rsquared) higher w/o intercept:
        # https://stats.stackexchange.com/questions/267325
        # https://stats.stackexchange.com/questions/26176

        # ...and some discussion about whether it makes sense to fit w/o intercept:
        # https://stats.stackexchange.com/questions/7948
        # https://stats.stackexchange.com/questions/102709

        # now only including this in title if we are able to recalculate R**2
        # (may be possible from model alone, but not sure how to access y_train from
        # model, if possible)
        #
        # TODO rsquared_adj useful in comparing these two models w/ diff # of
        # parameters?
        # TODO want anything else in here? don't really think p-val would be useful

        goodness_of_fit_str = (f'$R^2 = {r_squared:.4f}$'
            # TODO delete (or recalc for add_constant=False case, as w/ R**2
            # above)
            # TODO only include this if we have more than 1 param (i.e. if
            # add_constant=True). otherwise, R**2 should be equal to R**2_adj
            #f', $R^2_{{adj}} = {model.rsquared_adj:.4f}$'
        )
        ci_str = f'{((1 - alpha_for_ci) * 100):.3g}% CI on fit'
        title = f'{title_prefix}{model_eq}\n{goodness_of_fit_str}\n{ci_str}'
        ax.set_title(title)

        assert ax.get_xlabel() == col_to_fit
        desc = scaling_method2desc[scaling_method_to_use]
        ax.set_xlabel(f'{col_to_fit}\n{desc}')


    copy_to_model_dirs = []

    dff_to_spiking_model_path = plot_dir / 'dff2spiking_fit.p'
    copy_to_model_dirs.append(dff_to_spiking_model_path)

    if separate_inh_model:
        dff_to_spiking_inh_model_path = plot_dir / 'dff2spiking_inh_fit.p'
        copy_to_model_dirs.append(dff_to_spiking_inh_model_path)

    dff_to_spiking_data_csv = plot_dir / 'dff2spiking_model_input.csv'
    copy_to_model_dirs.append(dff_to_spiking_data_csv)

    # TODO move all fitting + plotting up above (~where current seaborn plotting
    # is), so i can do for all scaling choices, like w/ current seaborn plots (still
    # just doing modelling w/ one scaling choice)

    # so we have a record of which scaling choice we made (modelling plots already show
    # many parameters and this one isn't going to vary across modelling outputs from a
    # given run)
    # TODO TODO save this and other non-plot outputs we need to load saved dF/F->spiking
    # fit outside of plot dirs, so i can swap between png and pdf w/o issue...
    dff_to_spiking_model_choices_csv = plot_dir / 'dff2spiking_model_choices.csv'
    copy_to_model_dirs.append(dff_to_spiking_data_csv)

    def _write_inputs_for_reproducibility(plot_root: Path, param_dict: Dict[str, Any]
        ) -> None:
        # plot_root should be the panel plot dir,
        # e.g. pebbled_6f/pdf/ijroi/mb_modeling/megamat
        assert plot_root.is_dir()

        output_dir = plot_root / param_dict['output_dir']
        assert output_dir.is_dir()

        used_model_cache = param_dict.get('used_model_cache', True)
        # should be ok to write these files regardless of `-c`
        # (check_outputs_unchanged) flag value, because we must have made it
        # thru fit_and_plot... above w/o it tripping anything
        if used_model_cache:
            return

        if al_util.verbose:
            print(f'copying model inputs to {output_dir.name}:')

        for to_copy in copy_to_model_dirs:
            # would need to check copy2 behavior in this case, if wanted to support
            assert not to_copy.is_symlink()

            dst = output_dir / to_copy.name

            if al_util.verbose:
                print(f'copying {to_copy}')

            # TODO or load/save, using diff fns for this depending on ext? not sure i
            # gain anything from that...
            #
            # should preserve as much file metadata as possible (e.g. modification
            # time). will overwrite `dst`, if it already exists.
            shutil.copy2(to_copy, dst)

        # TODO refactor to share date_format= part w/ consensus_df saving i copied this
        # from? / delete? not sure it's even relevant if date is in index...
        to_csv(unmodified_orn_dff_input_df, output_dir / 'full_orn_dff_input.csv',
            date_format=date_fmt_str
        )
        to_pickle(unmodified_orn_dff_input_df, output_dir / 'full_orn_dff_input.p')


    # TODO anything else i need to include in this?
    dff_to_spiking_model_choices = pd.Series({
        # TODO None survive round trip here? use 'none' / NaN instead?
        'scaling_method_to_use': scaling_method_to_use,
        'add_constant': add_constant,
        'separate_inh_model': separate_inh_model,
    })

    def read_dff_to_spiking_model_choices():
        bool_params = ('add_constant', 'separate_inh_model')

        ser = read_series_csv(dff_to_spiking_model_choices_csv,
            # TODO some way to infer add_constant dtype correctly (as bool)
            # (this didn't work)
            #dtype={'add_constant': bool}
        )

        for x in bool_params:
            if not type(ser[x]) is bool:
                x_lower = ser[x].lower()
                assert x_lower in ('true', 'false')
                ser[x] = x_lower == 'true'

        return ser

    if not use_saved_dff_to_spiking_model:
        to_csv(dff_to_spiking_model_choices, dff_to_spiking_model_choices_csv,
            header=False
        )
        # to check no more dtype issues (and just that we saved correctly)
        saved = read_dff_to_spiking_model_choices()
        assert saved.equals(dff_to_spiking_model_choices)
        del saved

        # TODO also add depth col (when available) here?
        cols_to_save = ['fly_id', 'odor', 'glomerulus', dff_col]

        if col_to_fit != dff_col:
            cols_to_save.append(col_to_fit)

        cols_to_save.append(spike_delta_col)

        assert all(c in merged_dff_and_hallem.columns for c in cols_to_save)
        dff_to_spiking_data = merged_dff_and_hallem[cols_to_save].copy()

        dff_to_spiking_data = dff_to_spiking_data.rename({
                col_to_fit: f'{col_to_fit} (X_train)',
                spike_delta_col: f'hallem_{spike_delta_col} (y_train)',
            }, axis='columns'
        )
        shape_before = dff_to_spiking_data.shape

        dff_to_spiking_data = dff_to_spiking_data.merge(fly_ids, left_on='fly_id',
            right_index=True
        )
        assert dff_to_spiking_data.shape == (shape_before[0], shape_before[1] + 2)
        assert num_null(dff_to_spiking_data) == 0

        # NOTE: ['odor', 'fly_id', 'glomerulus'] would be unique if not for those few
        # odors that are in two panels in one fly (e.g. 'ms @ -3' in diag and megamat).
        # we don't currently have 'panel' info in this df, so may need to keep in mind
        # if using saved data.
        # TODO include panel? (would need to merge in a way that preserves that. don't
        # think i currently do... shouldn't really matter)

        # TODO also save hallem receptor in a col here?
        to_csv(dff_to_spiking_data, dff_to_spiking_data_csv, index=False,
            date_format=date_fmt_str
        )

        model, inh_model = fit_dff2spiking_model(merged_dff_and_hallem)

        # TODO also save model.summary() to text file?
        cprint(f'saving dF/F -> spike delta model to {dff_to_spiking_model_path}',
            _cprint_color
        )
        save_model(model, dff_to_spiking_model_path)

        if separate_inh_model:
            cprint(
                f'saving separate inhibition model to {dff_to_spiking_inh_model_path}',
                _cprint_color
            )
            save_model(model, dff_to_spiking_inh_model_path)

        # TODO delete / put behind checks flag
        #deserialized_model = sm.load(dff_to_spiking_model_path)
        # TODO other comparison that would work after model.remove_data()?
        # care to remove_data? probably not
        #
        # ok, this is true at least...
        # TODO why is this failing now (seems to only be a line containing Time, and
        # only in the time part of that line. doesn't matter.)
        # from diffing these:
        # Path('model_summary.txt').write_text(str(model.summary()))
        # Path('deser_model_summary.txt').write_text(str(deserialized_model.summary()))
        # tom@atlas:~/src/al_analysis$ diff model_summary.txt deser_model_summary.txt
        # 7c7
        # < Time:                        15:46:35   Log-Likelihood:                         -17078.
        # ---
        # > Time:                        15:46:46   Log-Likelihood:                         -17078.
        #
        # TODO find a replacement check?
        #assert str(deserialized_model.summary()) == str(model.summary())
        #

        # TODO (reword to update / delete) use _model kwarg in predict to check
        # serialized->deser model is behaving same (below, where predict is called)
    else:
        # TODO TODO print some short summary of this data (panels, numbers of flies,
        # etc)
        cprint(f'using saved dF/F -> spiking model {dff_to_spiking_model_path}',
            _cprint_color
        )
        if separate_inh_model:
            cprint(
                f'using separate inhibition model from {dff_to_spiking_inh_model_path}',
                _cprint_color
            )

        cached_model_choices = read_dff_to_spiking_model_choices()
        if not cached_model_choices.equals(dff_to_spiking_model_choices):
            # TODO reword. -i dff2spiking w/ input data < all of it will not actually do
            # anything (cause model only ever saved if all data passed in)
            warn('current hardcoded model choices did not match those from saved model!'
                '\nre-run, adding `-i dff2spiking` to overwrite cached model (or just '
                'w/ all data as input... see reproducing.md) exiting!'
            )
            sys.exit()

        # TODO TODO save this and other non-plot outputs we need to load saved
        # dF/F->spiking fit outside of plot dirs, so i can swap between png and pdf w/o
        # issue...
        #
        # possible this alone has the input data, but save/loading that in parallel,
        # since i couldn't figure out how to access from here
        model = sm.load(dff_to_spiking_model_path)
        if separate_inh_model:
            inh_model = sm.load(dff_to_spiking_inh_model_path)
        else:
            inh_model = None

        # TODO load + summarize model input data
        # TODO +also load+summarize model choices

    # TODO still show if verbose=True or something?
    #print('dF/F -> spike delta model summary:')
    #print(model.summary())

    X0_test = pd.DataFrame({col_to_fit: 0.0}, index=[0])
    if add_constant:
        X0_test = sm.add_constant(X0_test)

    y0 = model.get_prediction(X0_test).predicted
    if add_constant:
        # may be close, but unlikely to equal 0 exactly
        assert y0 != 0.0
    else:
        assert y0 == 0.0

    # don't want to clutter str w/ the most typical values of these
    exclude_param_for_vals = {
        # always want to show scaling_method_to_use value
        #
        # TODO when these are true, maybe just include the str (w/o the '-True' suffix)?
        'add_constant': False,
        'separate_inh_model': False,
    }
    assert all(k in dff_to_spiking_model_choices.keys() for k in exclude_param_for_vals)
    params_for_suffix = {k: v for k, v in dff_to_spiking_model_choices.items()
        if not (k in exclude_param_for_vals and v == exclude_param_for_vals[k])
    }

    param_abbrevs = {
        'scaling_method_to_use': 'dff_scale',
        'add_constant': 'add_const',
        'separate_inh_model': 'separate_inh',
    }
    # TODO also thread (something like) this thru to be included in titles?
    dff2spiking_choices_str = '__'.join([
        f'{param_abbrevs[k] if k in param_abbrevs else k}-{v}'
        for k, v in params_for_suffix.items()
    ])

    pebbled_param_dir_prefix = f'{dff2spiking_choices_str}__'

    if not use_saved_dff_to_spiking_model:
        plot_fname = f'dff_vs_hallem__{dff2spiking_choices_str}'

        fig, _ = plt.subplots()
        # TODO factor into same fn that fits model?
        plot_dff2spiking_fit(merged_dff_and_hallem, model)
        # normalize_fname=False to prevent '__' from getting replaced w/ '_'
        # TODO TODO fix -c/-C failure here!
        # TODO add a to_csv(merged_dff_and_hallem, <some-path>) call first, to
        # sanity check input data not changing (pretty sure it's not)?
        # TODO need to change tolerance values in savefig (-c/-C) check of output
        # equivalence? or possible to make this completely deterministic (can i repro
        # this -c/-C failure on adjacent runs? maybe something actually did change?)?
        # sns.scatterplot and mpl.scatterplot (which former calls) both seem
        # deterministic though... or at least don't seem to have any seed kwargs / etc.
        # this was also the part of the plot that seemed (from the diff) like it had
        # changed...
        savefig(fig, plot_dir, plot_fname, normalize_fname=False)

        fig, _ = plt.subplots()
        # this one should plot fit over a 2d hist of data
        plot_dff2spiking_fit(merged_dff_and_hallem, model, scatter=False)
        savefig(fig, plot_dir, f'{plot_fname}_hist2d', normalize_fname=False)

        _seen_group_vals = set()
        def fit_and_plot_dff2spiking_model(*args, group_col=None, **kwargs):
            assert len(args) == 0
            assert 'label' not in kwargs
            # TODO OK to throw away color kwarg like this? my plotting fn uses
            # fly_palette internally...
            assert set(kwargs.keys()) == {'data', 'color'}

            df = kwargs['data']
            model, inh_model = fit_dff2spiking_model(df)

            group_vals = set(df[group_col].unique())
            assert len(group_vals) == 1
            group_val = group_vals.pop()
            group_tuple = (group_col, group_val)
            assert group_tuple not in _seen_group_vals, f'{group_tuple=} already seen'
            _seen_group_vals.add(group_tuple)

            assert group_col in ('glomerulus', 'depth_bin')
            if group_col == 'glomerulus':
                # TODO add receptors in parens after glom, for easy ref to hallem paper?
                if roi_depths is not None:
                    avg_depth_col = f'avg_{roi_depth_col}'
                    assert df[avg_depth_col].nunique() == 1
                    avg_roi_depth_um = df[avg_depth_col].unique()[0]

                    title_prefix = \
                        f'{group_val} (avg depth: {avg_roi_depth_um:.1f} $\\mu$m)\n'
                else:
                    title_prefix = f'{group_val}\n'

            elif group_col == 'depth_bin':
                title_prefix = f'{group_val} $\\mu$m\n'

            # pylint: disable-next=possibly-used-before-assignment
            plot_dff2spiking_fit(df, model, inh_model, title_prefix=title_prefix)


        if roi_depths is not None:
            avg_depth_per_glomerulus = merged_dff_and_hallem.groupby('glomerulus')[
                roi_depth_col].mean()
            avg_depth_per_glomerulus.name = f'avg_{roi_depth_col}'
            merged_dff_and_hallem = merged_dff_and_hallem.merge(
                avg_depth_per_glomerulus, left_on='glomerulus', right_index=True
            )

            merged_dff_and_hallem = merged_dff_and_hallem.sort_values(
                f'avg_{roi_depth_col}').reset_index(drop=True)

        else:
            merged_dff_and_hallem = merged_dff_and_hallem.sort_values('glomerulus'
                ).reset_index(drop=True)

        col = 'glomerulus'
        # TODO default behavior do this anyway? easier way?
        grid_len = int(np.ceil(np.sqrt(merged_dff_and_hallem[col].nunique())))

        # to remove warning 'The figure layout has changed to tight' otherwise generated
        # when each FacetGrid is contructed (b/c my mpl rcParams use constrained layout
        # by default).
        #
        # setting layout='tight' via FacetGrid subplot_kws didn't work to fix (produced
        # an error), nor could gridspec_kws, as those are ignored if col_wrap passed.
        with mpl.rc_context({'figure.constrained_layout.use': False}):
            g = sns.FacetGrid(data=merged_dff_and_hallem, col=col, col_wrap=grid_len)
            g.map_dataframe(fit_and_plot_dff2spiking_model, group_col=col)

            viz.fix_facetgrid_axis_labels(g)
            savefig(g, plot_dir, f'by-glom_{plot_fname}', normalize_fname=False)

        if roi_depths is not None:
            # TODO maybe also show list of glomeruli in each bin for plot below?
            # (would need to get wrapping to work in commented code above. too many to
            # list nicely in one line.)
            # TODO or least print these glomeruli (+ value_counts?)

            n_depth_bins_options = (2, 3, 4, 5)
            for n_depth_bins in n_depth_bins_options:
                # should be a series of length equal to merged_dff_and_hallem
                # (w/ CategoricalDtype)
                depth_bins = pd.cut(merged_dff_and_hallem[roi_depth_col], n_depth_bins)

                df = merged_dff_and_hallem.copy()

                df['depth_bin'] = depth_bins

                col = 'depth_bin'
                grid_len = int(np.ceil(np.sqrt(df[col].nunique())))

                with mpl.rc_context({'figure.constrained_layout.use': False}):
                    g = sns.FacetGrid(data=df, col=col, col_wrap=grid_len)
                    g.map_dataframe(fit_and_plot_dff2spiking_model, group_col=col)

                    viz.fix_facetgrid_axis_labels(g)
                    savefig(g, plot_dir, f'by-depth-bin-{n_depth_bins}_{plot_fname}',
                        normalize_fname=False
                    )

        # TODO try a depth specific model too (seems not worth, from depth binned plots)
        # (quite clear overall scale changes when i need to avoid strongest responding
        # plane b/c contamination. e.g. often VA4 has contamination p-cre response in
        # highest (strongest) plane, from one of the nearby/above glomeruli that
        # responds to that)
        #
        # TODO try directly estimating fn like:
        # A(B*depth*x)? how would be best?
        # since it's linear, couldn't i just do:
        # A*depth*x?
        # i guess i might like to try nonlinear fns of depth, or at least to make depth
        # scaling more interpretable, but idk...
        # TODO or maybe i want A(B*depth + x)?
        # either way, i want to make sure that 0 dff is always 0 est spike delta
        # (regardless of depth), so making me thing i dont want this additive model...
        # TODO try scipy optimization stuff?
        #
        # TODO also compare to just adding a param for depth in linear model
        # (but otherwise still using all data for fit)
        # TODO possible to have automated detection of outlier glomeruli? i.e. those
        # benefitting from different fits


    # TODO TODO histogram of est spike deltas this spits out
    # (to what extent is that already done in loop over panels below?)
    # TODO are only NaNs in the dff_col in here coming from setting-wrong-odors-NaN
    # above?
    # TODO can delete branches in predict only for plotting w/ this input (don't
    # actually care about the plot here)
    #
    # this predict(...) call is the one actually adding the estimated spike deltas,
    # computed from data to be modelled (which can be different from data originally
    # used to compute dF/F -> spike delta est fn).
    fly_mean_df = predict_spiking_from_dff(fly_mean_df, model, inh_model)

    # TODO also save fly_mean_df similar to how we save dff2spiking_model_input.csv
    # above (for other people to analyze arbitrary subsets of est spike deltas /
    # whatever) (maybe refactor above to share + use panel_prefix from below?)
    # TODO + do same under each panel dir, for each panels data?

    # TODO TODO test whether downstream code works fine w/o stopping here (at least
    # check equiv in megamat case. may want to hardcode a skip of the validation2 panel
    # by default anyway [w/ a flag])
    # TODO would checking megamat subset of fly_mean_df is same between two runs (w/ all
    # data vs just megamat flies) get us most/all of the way there?
    #
    # TODO delete hack (see corresponding hack above where this flag is defined)
    # (plus now rest of modeling code loops over panels anyway, no?)
    if not use_saved_dff_to_spiking_model:
        print('EXITING EARLY AFTER HAVING SAVED MODEL ON ALL DATA (analyze specific '
            'panels with additional al_analysis runs, restricting date range to only '
            'one panel)!'
        )
        sys.exit()
    #

    # TODO plot histogram of fly_mean_df[est_spike_delta_col] (maybe resting on the x
    # axis in the same kind of scatter plot of (x=dF/F, y=delta spike rate,
    # hue=fly_id)?)

    # TODO rename? it's a series here, not a df (tho that should change in next
    # re-assignment, the one where RHS is unstacked...)
    mean_est_df = fly_mean_df.reset_index().groupby(['panel', 'odor', 'glomerulus'],
        sort=False)[est_spike_delta_col].mean()

    # then odors will be columns and glomeruli will be rows, which is same as
    # orns.orns().T
    mean_est_df = mean_est_df.unstack(['panel', 'odor'])

    # TODO delete?
    # TODO why was this getting triggered when run w/ megamat data, but not w/
    # kiwi/control data? not sure it matters...
    # was gonna sort again here, but seems true already
    #assert mean_est_df.equals(sort_odors(mean_est_df))
    #
    mean_est_df = sort_odors(mean_est_df)

    # TODO factor all dF/F -> spike delta fitting (above, ending ~here) into one fn in
    # here at least, to make model_mb_responses more readable

    # TODO TODO possible to get spike rates for any of the other door data sources?
    # available in their R package?

    # TODO TODO how to do an ephaptic model? possible to optimize one using my
    # data as input (where we never have the channels separated)? if using hallem, for
    # what fraction of sensilla do we have all / most contained ORN types?
    # TODO which data to use for ephaptic effects / how?
    # TODO plot ephaptic model adjusted dF/F (subtracting from other ORNs in sensilla)
    # vs spike rate?

    # TODO TODO plot mean_est_df vs same subset of hallem, just for sanity checking
    # (as a matrix in each case)
    # TODO TODO and do w/ my ORN input (untransformed by dF/F -> spike delta model?),
    # also my ORN input subset to hallem stuff
    # (computing correlation in each case, with nothing going thru MB model first)

    # TODO drop all non-megamat odors prior to running through fit_mb_model?
    # (no, want to model other stuff now too...) (am i not currently doing this tho?)
    # (diagnostics prob gonna produce much lower KC sparsity)
    # (are any of the non-HALLEM odors (which is probably most non-megamat odors?)
    # actually influencing model in fit_mb_model tho?)

    # TODO are relative sparsities recapitulating what remy seems? even broadly?

    # TODO TODO try fitting on hallem, and then running on my data passed thru
    # dF/F model (fitting on all hallem might produce very different thresholds from
    # fitting on the subset of odors remy uses!!!)

    hallem_for_comparison = hallem_delta_wide.copy()
    assert hallem_for_comparison.index.str.contains(' @ -3').all()
    # so things line up in comparison_orns path (fit_mb_model hallem data has '@ -2'
    # for each conc)
    hallem_for_comparison.index = hallem_for_comparison.index.str.replace(
        ' @ -3', ' @ -2'
    )
    # TODO delete? actually needed by anything?
    hallem_for_comparison.index.name = 'odor1'

    # TODO move all hallem version of this above loop over panels (done?)
    tidy_hallem = hallem_for_comparison.T.stack()
    # just to rename the second level from 'odor1'->'odor', to be consistent w/
    # above
    tidy_hallem.index.names = ['glomerulus', 'odor']
    tidy_hallem.name = spike_delta_col
    tidy_hallem = tidy_hallem.reset_index()
    fig, ax = plt.subplots()
    sns.histplot(data=tidy_hallem, x=spike_delta_col, bins=n_bins, ax=ax)
    ax.set_title('all Hallem')
    # TODO TODO fix -c failure (just a tolerance thing?)
    savefig(fig, plot_dir, 'hist_hallem')

    # TODO move inside loop (doing for every panel, not just megamat)?
    tidy_hallem_megamat = tidy_hallem.loc[
        tidy_hallem.odor.apply(odor_is_megamat)
    ].copy()

    fig, ax = plt.subplots()
    sns.histplot(data=tidy_hallem_megamat, x=spike_delta_col, bins=n_bins, ax=ax)
    ax.set_title('Hallem megamat')
    savefig(fig, plot_dir, 'hist_hallem_megamat')

    # TODO also include actual equation for scaling in this (just one constant w/
    # current choices)? currently just includes choices that influence that, but since
    # we aren't including full data in there (though we are copying it to each model dir
    # now), we can't quickly tell which scaling factor was used
    # (could refactor part of plotting code that gets `model_eq` for title, and use that
    # str equation here too? might want a bit more precision on some params, so maybe
    # include them [e.g. `model.params`] as well?)
    #
    # will be saved alongside later params, inside each model output dir
    # (for reproducibility)
    extra_params = {
        f'dff2spiking_{k}': v for k, v in dff_to_spiking_model_choices.to_dict().items()
    }

    # slightly nicer number (less sig figs) that is almost exactly the same as the
    # sparsity computed on remy's data (from binarized outputs she gave me on
    # 2024-04-03)
    remy_sparsity = 0.0915

    checks = True
    if checks:
        # TODO compute + use separate remy sparsity from validation data (not sure i'm
        # actually going to continue doing any modelling for the validation data. don't
        # think any of it is making it in to paper)? have what i need for that already,
        # or need something new from her?
        #
        # 0.091491682899149
        remy_sparsity_exact = remy_megamat_sparsity()

        # remy_sparsity_exact - remy_sparsity: -8.317100850752102e-06
        assert abs(remy_sparsity_exact - remy_sparsity) <= 1e-5

    target_sparsities = (remy_sparsity,)

    # TODO hardcode list of panels to NOT run sensitivity analysis on (just
    # validation2?)?

    # TODO delete eventually
    assert mean_est_df.equals(sort_odors(mean_est_df))

    # TODO support list values (-> iterate over)? (as long as directories would have
    # diff names)
    #
    # which panel(s) to use to "tune" the model (i.e. set the two inhibitory
    # parameters), to achieve the target sparsity. if a panel is not included in keys
    # here, it will just be tuned on it's own data.
    panel2tuning_panels = {
        'kiwi': ('kiwi', 'control'),
        'control': ('kiwi', 'control'),

        # TODO any way to salvage this idea? check dF/F distributions between the two
        # first? maybe z-score first or something? currently getting all silent cells in
        # first model (control + hemibrain) run this way.
        # TODO TODO and how do the parameters compare across the panels again?
        # i thought they were in a similar range? different enough i guess?
        #
        # running w/ `./al_analysis.py -d pebbled -n 6f -t 2023-04-22` for this.
        # (certain_df only has the flies i expected, which is the 9 megamat +
        # 5 validation + 9 kiwi/control flies)
        #'kiwi': ('megamat',),
        #'control': ('megamat',),

        # TODO also try using 'megamat' tuning for 'validation', and see how that
        # affects things?

        # TODO delete? was to test pre-tuning code working as expected.
        # (used new script al_analysis/check_pretuned_vs_not.py to compare responses +
        # spike counts from each)
        #
        # TODO TODO how to keep this in as an automated check? or move to a separate
        # test script (model_test.py, or something simliar?)? currently i need to
        # manually compare the outputs across the old/new dirs
        # (add panel2tuning_panels as kwarg of model_mb_responses -> make 2 calls?)
        #'megamat': ('megamat',)
        #
    }
    assert all(type(x) is tuple for x in panel2tuning_panels.values())
    # sorting so dir (which will include tuning panels) will always be the same
    panel2tuning_panels = {k: tuple(sorted(v)) for k, v in panel2tuning_panels.items()}

    tuning_panel_delim = '-'

    new_panels = {tuning_panel_delim.join(x) for x in panel2tuning_panels.values()}
    existing_panels = set(mean_est_df.columns.get_level_values('panel'))
    if any(x in existing_panels for x in new_panels):
        warn(f'some of {new_panels=} are already in {existing_panels=}! should only '
            'see this warning if testing that we can reproduce model output by '
            'pre-tuning with the same panel we later use to run the model!'
        )
    del new_panels, existing_panels

    # TODO want to drop the panel column level? or want to use it inside calls to
    # fit_and_plot...? groupby kwarg for dropping, if i want former?
    for panel, panel_est_df in mean_est_df.groupby('panel', axis='columns', sort=False):

        if panel == diag_panel_str:
            continue

        # TODO delete eventually
        assert panel_est_df.equals(sort_odors(panel_est_df))
        #
        # TODO delete (assertion above passing. seems we can rely on mean_est_df being
        # sorted)
        #panel_est_df = sort_odors(panel_est_df)

        panel_plot_dir = plot_dir / panel
        makedirs(panel_plot_dir)

        # these will have one row per model run, with all relevant parameters (as well
        # as a few other variables/statistics computed within model runs, e.g. sparsity)
        model_param_csv = panel_plot_dir / 'tuned_params.csv'
        model_params = None

        raw_dff_panel_df = sort_odors(certain_df.loc[panel], panel=panel)

        mean_fly_dff_corr = mean_of_fly_corrs(raw_dff_panel_df)

        # just checking that mean_of_fly_corrs isn't screwing up odor order, since
        # raw_dff_panel_df odors are sorted (and easier to check against panel_est_df,
        # as that doesn't have the repeats in it like raw_dff_panel_does, but the order
        # of the odors in the two should be the same)
        assert mean_fly_dff_corr.columns.equals(mean_fly_dff_corr.index)
        # this doesn't check .name, which is good, b/c mean_fly_dff_corr has 'odor1',
        # not 'odor'
        assert mean_fly_dff_corr.columns.equals(
            panel_est_df.columns.get_level_values('odor')
        )

        # TODO (just for ticklabels in plots) for kiwi/control at least (but maybe for
        # everything?) hide the '@ 0' part of conc strs [maybe unless there is another
        # odor w/ a diff conc, but may not matter]

        # TODO restore response matrix plot versions of these (i.e. plot responses in
        # addition to just corrs) (would technically be duped w/ ijroi versions, for
        # convenient comparison to 'est_orn_spike_deltas*' versions? or symlink to the
        # ijroi one?
        plot_corr(mean_fly_dff_corr, panel_plot_dir, 'orn_dff_corr',
            xlabel=f'ORN {dff_latex}'
        )

        fly_dff_hallem_subset = raw_dff_panel_df.loc[:,
            raw_dff_panel_df.columns.get_level_values('roi').isin(
                hallem_delta_wide.columns
            )
        ]
        mean_fly_dff_hallem_corr = mean_of_fly_corrs(fly_dff_hallem_subset)
        plot_corr(mean_fly_dff_hallem_corr, panel_plot_dir,
            'orn_dff_hallem-subset_corr',
            xlabel=f'ORN {dff_latex}\nHallem glomeruli only'
        )
        plot_corr(mean_fly_dff_hallem_corr, panel_plot_dir,
            'orn_dff_hallem-subset_corr-dist',
            xlabel=f'ORN {dff_latex}\nHallem glomeruli only', as_corr_dist=True
        )

        # TODO delete / move to al_analysis (get_gh146_glomeruli was only used in
        # al_analysis.py other than here, so didn't refactor it to al_util)
        '''
        gh146_glomeruli = get_gh146_glomeruli()
        # NOTE: true for megamat at least, may not be true for validation2
        if panel == 'validation2':
            assert {'VA4'} == (
                gh146_glomeruli - set(raw_dff_panel_df.columns.get_level_values('roi'))
            )
        else:
            # TODO TODO TODO commit a file that defines set of gh146 glomeruli under
            # data/ (-> use that)
            #
            # TODO TODO TODO relax to not err on new data? presumably this is what was
            # failing?
            # TODO TODO just warn instead? only do anything if we seem to be running on
            # the final megamat data?
            print('update / delete gh146 glomeruli checking code')
            #assert 0 == len(
            #    gh146_glomeruli - set(raw_dff_panel_df.columns.get_level_values('roi'))
            #)

        fly_dff_gh146_subset = raw_dff_panel_df.loc[:,
            raw_dff_panel_df.columns.get_level_values('roi').isin(gh146_glomeruli)
        ]
        mean_fly_dff_gh146_corr = mean_of_fly_corrs(fly_dff_gh146_subset)
        plot_corr(mean_fly_dff_gh146_corr, panel_plot_dir,
            'orn_dff_gh146-subset_corr',
            xlabel=f'ORN {dff_latex}\nGH146 glomeruli only'
        )
        plot_corr(mean_fly_dff_gh146_corr, panel_plot_dir,
            'orn_dff_gh146-subset_corr-dist',
            xlabel=f'ORN {dff_latex}\nGH146 glomeruli only', as_corr_dist=True
        )
        '''

        # should i also be passing each *individual fly* data thru dF/F -> est spike
        # delta fn (-> recomputing)? should i be doing that w/ all of modeling?
        # (no, Betty and i agreed it wasn't worth it for now)

        # TODO no need for copy, right?
        # TODO maybe i don't need to drop panel here?
        #
        # also, why the double transpose here? est_df used apart from for this plot?
        # (b/c usage as comparison_orns below)
        #
        # NOTE: this should currently be saved as a pickle+CSV under each model output
        # directory, at orn_deltas.[csv|p] (done by fit_and_plot...)
        est_df = panel_est_df.droplevel('panel', axis='columns').T.copy()

        # TODO TODO also plot hemibrain filled version(s) of this
        est_corr = plot_responses_and_corr(est_df.T, panel_plot_dir,
            f'est_orn_spike_deltas_{dff2spiking_choices_str}',
            # TODO maybe borrow final part from scaling_method2desc (but current strs
            # there have more info than i want)
            xlabel=('est. ORN spike deltas\n'
                f'{dff_latex} scaling: {scaling_method_to_use}'
            ),
        )
        del est_corr

        # TODO TODO plot responses + corrs for (est_orn_spike_deltas + sfr) and
        # (hallem_spike_deltas + sfr) too. compare to values from dynamic ORNs and
        # deltas alone. (probably do in fit_mb_model internals plotting?)
        #
        # (actually care about adding sfr? does it actually change corrs? if so, to a
        # meaningful degree?)

        # TODO or just move before loop over panels?
        if panel == 'megamat':
            hallem_megamat = hallem_delta_wide.loc[
                # get_level_values('odor') should work whether panel_est_df has 'odor'
                # as one level of a MultiIndex, or as single level of a regular Index
                panel_est_df.columns.get_level_values('odor')
            ].sort_index(axis='columns')

            # TODO label cbar w/ spike delta units
            plot_responses_and_corr(hallem_megamat.T, panel_plot_dir,
                'hallem_spike_deltas', xlabel='Hallem OR spike deltas'
            )

            # TODO TODO also NaN-fill Hallem to hemibrain, and plot those responses (if
            # i haven't already somewhere else). no need to plot corrs there, as they
            # should be same as raw hallem.

            # TODO TODO only zero fill just as in fitting tho (how is it diff? at least
            # add comment about how it's diff...)? current method also drops stuff like
            # DA3, which is in hallem but not in my data...
            # TODO TODO leave that to one of the model_internals plots in that case?
            # maybe just delete this then?
            #
            # TODO does this change correlation (yes, moderately increased)?
            # TODO plot delta corr wrt above?
            # TODO print about what the reindex is dropping (if verbose?)?
            zerofilled_hallem = hallem_megamat.reindex(panel_est_df.index,
                axis='columns').fillna(0)
            plot_responses_and_corr(zerofilled_hallem.T, panel_plot_dir,
                'hallem_spike_deltas_filled', xlabel='Hallem OR spike deltas\n'
                '(zero-filled to my consensus glomeruli)'
            )

        # TODO TODO and same thing with my raw data honestly. not sure i have that.
        # here might not be the place though (top-level ijroi stuff?)
        # TODO TODO matrix plot actually making my est spike deltas as comparable
        # as possible to the relevant subset of the hallem data (+ relevant subset of my
        # data)
        # (not here, but at least once for master version of hallem and pebbled data,
        # maybe just in megamat context)

        # TODO just use one of the previous things that was already tidy? and already
        # had hallem data?
        tidy_est = panel_est_df.droplevel('panel', axis='columns').stack()
        tidy_est.name = est_spike_delta_col
        tidy_est = tidy_est.reset_index()

        fig, ax = plt.subplots()
        # TODO TODO why did the xticks seem to change (comparing old vs new version of
        # dff_scale-to-avg-max_hist_est-spike-delta_validation2.pdf, highlighted by -C)?
        # shape of rest seems the same. something meaningful? diff input subset or
        # something?
        sns.histplot(data=tidy_est, x=est_spike_delta_col, bins=n_bins, ax=ax)
        ax.set_title(f'pebbled {panel}')
        # TODO or save in panel dir? this consistent w/ saving of hallem megamat stuff
        # above tho...
        savefig(fig, plot_dir,
            f'{pebbled_param_dir_prefix}hist_est-spike-delta_{panel}'
        )
        del tidy_est

        pebbled_input_df = panel_est_df
        responses_to_suffix = ''

        # TODO check outputs against those run previous way (without
        # explicitly passing inputs, when using + tuning on hallem)
        #hallem_input_df = hallem_for_comparison.T.copy()

        comparison_orns = None
        comparison_kc_corrs = None

        # TODO delete (still want this? or maybe for other panels, e.g. kiwi?)
        if panel != 'megamat':
            print('GET COMPARISON_ORNS (+ COMPARISON_KCS) WORKING ON non-megamat DATA')
        #
        if panel == 'megamat':
            # TODO TODO don't i still want comparison_orns in validation case?
            # TODO TODO what about comparison_kcs in validation case? what betty
            # has said so far (re: validation modelling figures) is that we only want
            # sparsity + correlation, but not sure that'll remain true...
            comparison_orns = {
                'raw-dff': raw_dff_panel_df,

                # NOTE: this one does not have single fly data like raw_dff_panel_df
                # (it's just mean responses), so correlation computed not exactly
                # apples-to-apples with most others (but similar to how model output
                # corr computed, given model is run on mean data)
                'est-spike-delta': est_df,

                # TODO also a version zero-filling like fit_mb_model does internally
                # (happy enough w/ corr_diff plots i added in fit_mb_model?)
            }

            # TODO rename to comparison_kc_corrs or something? observed_mean_kc_corrs?
            #
            # this is a mean-of-fly-corrs (WAS for Remy's 4 final KC flies, but now
            # adapting to also load the older data too)
            comparison_kc_corrs = load_remy_megamat_mean_kc_corrs()

            # TODO replace these two lines w/ just sorting, if that works (would have to
            # add panel to both column and index, at least one manually...)
            # (name order already cluster order, in panel2name_order?)
            # (current strategy will probably no longer work w/ panel_est_df/est_df
            # having panel level...)
            assert set(est_df.index) == set(comparison_kc_corrs.index)
            comparison_kc_corrs = comparison_kc_corrs.loc[est_df.index, est_df.index
                ].copy()
            #

            # TODO also an as_corr_dist=True version of my mean ORN corrs (to finish off
            # fig 3 C)
            # TODO same for model corrs (corr.pdf under each model param dir)

            # TODO delete similar code in comparison_kc_corrs branch inside
            # fit_and_plot...
            #
            # TODO would probably need to redo diverging_cmap + vmin/vmax to work w/
            # correlation distance (this was from before i converted correlations to
            # correlation distances. could also move this before converting above...)
            # TODO or convert back to correlations, if currently as distances
            plot_corr(comparison_kc_corrs, panel_plot_dir, 'remy_kc_corr',
                xlabel='observed KCs'
            )
            plot_corr(comparison_kc_corrs, panel_plot_dir, 'remy_kc_corr-dist',
                xlabel='observed KCs', as_corr_dist=True
            )
            # TODO make corr diff plots wrt orn inputs
            # (probably just the raw dF/F, or maybe also the transformed stuff before
            # fitting?)
            # TODO compare uniform - hemibrain corr to experimental correlation change
            # from ORNs->KCs?
            # TODO load responses.[p|csv] from each dir -> compute corrs -> diff from
            # there (might just make a separate script for that...)?

            assert set(comparison_kc_corrs.index) == set(
                raw_dff_panel_df.index.get_level_values('odor1')
            )
            mean_orn_corrs = mean_of_fly_corrs(raw_dff_panel_df, square=False)
            mean_kc_corrs = corr_triangular(comparison_kc_corrs)

            assert mean_kc_corrs.index.equals(mean_orn_corrs.index)

            orn_col = 'mean_orn_corr'
            kc_col = 'mean_kc_corr'
            mean_orn_corrs.name = orn_col
            mean_kc_corrs.name = kc_col

            merged_corrs = pd.concat([mean_orn_corrs, mean_kc_corrs], axis='columns')

            # TODO TODO refactor to share w/ where i copied from
            fig, ax = plt.subplots()
            add_unity_line(ax)
            lineplot_kws = dict(
                ax=ax, data=merged_corrs, x=orn_col, y=kc_col, linestyle='',
                color='black'
            )
            marker_only_kws = dict(
                markers=True, marker='o', errorbar=None,

                # seems to default to white otherwise
                markeredgecolor='black',

                markerfacecolor='None',
                alpha=0.175,
            )
            # plot points
            sns.lineplot(**lineplot_kws, **marker_only_kws)

            metric_name = 'correlation'
            ax.set_xlabel(f'{metric_name} of raw ORN {dff_latex} (observed)')
            ax.set_ylabel(f'{metric_name} of KCs (observed)')

            metric_max = max(merged_corrs[kc_col].max(), merged_corrs[orn_col].max())
            metric_min = min(merged_corrs[kc_col].min(), merged_corrs[orn_col].min())

            plot_max = 1
            plot_min = -.5
            assert metric_max <= plot_max, f'{metric_max=} > {plot_max=}'
            assert metric_min >= plot_min, f'{metric_min=} < {plot_min=}'

            ax.set_xlim([plot_min, plot_max])
            ax.set_ylim([plot_min, plot_max])

            # should give us an Axes that is of square size in figure coordinates
            ax.set_box_aspect(1)

            spear_text, _, _, _, _ = bootstrapped_corr(merged_corrs, kc_col, orn_col,
                method='spearman',
                # TODO delete (for debugging)
                _plot_dir=panel_plot_dir
            )
            ax.set_title(spear_text)

            # TODO also include errorbars along both x and y here? (across flies whose
            # correlations went into mean corr)

            savefig(fig, panel_plot_dir, 'remy-kc_vs_orn-raw-dff_corrs')
            # (end part to refactor to share w/ copied code)


        model_kw_list = [
            # TODO TODO why is DA4m in in these but not in regular hallem input call in
            # separate list below? conform preprocessing to match (+ sort glomeruli in
            # all those internal plots?)

            # weight_divisor=10,5 (and probably 2) didn't improve things, in terms of
            # either obvious impression of correlation (vs real KC corrs) or spearman's
            # corr between them
            #
            # same as in main hemibrain call (w/o weight_divisor), but now increasing
            # wPNKC values in proportion to connectome weight, rather than just counting
            # each unique PN with weight >=4.
            dict(
                orn_deltas=pebbled_input_df,
                responses_to_suffix=responses_to_suffix,
                tune_on_hallem=False,
                pn2kc_connections='hemibrain',
                weight_divisor=20,

                # also including for new weight_divisor=20 entry, just so that i don't
                # have to resend remy new uniform/hemidraw outputs, or update those
                # plots. if i were to start over again, i would still leave this at
                # default True.
                _drop_glom_with_plus=False,

                # TODO like these steps (in this new context, w/ weight_divisor=20)?
                sensitivity_analysis=True,
                sens_analysis_kws=dict(
                    n_steps=3,
                    fixed_thr_param_lim_factor=0.5,
                    wAPLKC_param_lim_factor=5.0,
                    drop_nonpositive_fixed_thr=True,
                    drop_negative_wAPLKC=True,
                ),

                comparison_orns=comparison_orns,
                comparison_kc_corrs=comparison_kc_corrs,
            ),

            # would need to uncomment this (and comment other hemibrain entry) to
            # reproduce preprint/paper plots that predated weight_divisor code.
            # initial submission (>= March 2025) should use weight_divisor
            # hemibrain version.
            #dict(
            #    orn_deltas=pebbled_input_df,
            #    responses_to_suffix=responses_to_suffix,

            #    tune_on_hallem=False,
            #    pn2kc_connections='hemibrain',

            #    # required to restore the 7 extra no-connection KCs that would be
            #    # dropped with new default behavior here, which slightly but
            #    # mostly-inconsequentially changed old hemibrain model resposnses.
            #    # not planning to use for newer call using weight_divisor=20, whose
            #    # outputs should replace those from these call in the paper.
            #    # the cells are those w/ bodyid: [519206240, 861280857, 943118619,
            #    # 1172713521, 1203779116, 5812979314, 5813081175]
            #    _drop_glom_with_plus=False,

            #    sensitivity_analysis=True,
            #    sens_analysis_kws=dict(
            #        n_steps=3,
            #        fixed_thr_param_lim_factor=0.5,
            #        wAPLKC_param_lim_factor=5.0,
            #        drop_nonpositive_fixed_thr=True,
            #        drop_negative_wAPLKC=True,
            #    ),

            #    comparison_orns=comparison_orns,
            #    comparison_kc_corrs=comparison_kc_corrs,
            #),

            # TODO restore?
            ## TODO TODO this actually make sense to try (w/ tune_on_hallem=True
            ## and drop_receptors_not_in_hallem=False?) does my code even support
            ## currently?
            ##dict(orn_deltas=pebbled_input_df, tune_on_hallem=True,
            ##    pn2kc_connections='hemibrain'
            ##),

            #dict(orn_deltas=pebbled_input_df, tune_on_hallem=True,
            #    drop_receptors_not_in_hallem=True,
            #    pn2kc_connections='hemibrain'
            #),

            #dict(orn_deltas=pebbled_input_df, tune_on_hallem=False,
            #    drop_receptors_not_in_hallem=True,
            #    pn2kc_connections='hemibrain'
            #),

            # NOTE: uniform/hemibrain models currently use # of KCs from hemibrain
            # connectome (1837 if _drop_glom_with_plus=False [= old behavior], or 1830
            # otherwise). model would default to 2000 otherwise. fafb data more cells
            # (2482 in left, probably similar in right).
            dict(
                orn_deltas=pebbled_input_df,
                responses_to_suffix=responses_to_suffix,
                tune_on_hallem=False,
                pn2kc_connections='uniform', n_claws=7, n_seeds=n_seeds,
                # NOTE: also need _drop_glom_with_plus=False for this and hemidraw,
                # to reproduce previous outputs (probably just b/c hemibrain KC number
                # is reduced by 7 if this is True, so these models use less cells?)
                _drop_glom_with_plus=False,
                comparison_orns=comparison_orns,
                comparison_kc_corrs=comparison_kc_corrs,
            ),

            # TODO delete? also try n_claws=5,6 for hemidraw?
            # since mean # connections is 5.44 in connectome='hemibrain' case [and
            # probably similar for fafb inputs]). prob just b/c matt had settled on it,
            # we had been using n_claws=7 for most things.
            # dict(
            #     orn_deltas=pebbled_input_df,
            #     responses_to_suffix=responses_to_suffix,
            #     tune_on_hallem=False,
            #     pn2kc_connections='uniform', n_claws=6, n_seeds=n_seeds,
            #     comparison_orns=comparison_orns,
            #     comparison_kc_corrs=comparison_kc_corrs,
            # ),
            # dict(
            #     orn_deltas=pebbled_input_df,
            #     responses_to_suffix=responses_to_suffix,
            #     tune_on_hallem=False,
            #     pn2kc_connections='uniform', n_claws=5, n_seeds=n_seeds,
            #     comparison_orns=comparison_orns,
            #     comparison_kc_corrs=comparison_kc_corrs,
            # ),
            # TODO try hemidraw updated to use weight_divisor hemibrain wPNKC?
            #dict(
            #    orn_deltas=pebbled_input_df,
            #    responses_to_suffix=responses_to_suffix,
            #    tune_on_hallem=False,
            #    pn2kc_connections='hemidraw', n_claws=7, n_seeds=n_seeds,
            #    # NOTE: also need _drop_glom_with_plus=False for this and hemidraw,
            #    # to reproduce previous outputs (probably just b/c hemibrain KC number
            #    # is reduced by 7 if this is True, so these models use less cells?)
            #    _drop_glom_with_plus=False,
            #    comparison_orns=comparison_orns,
            #    comparison_kc_corrs=comparison_kc_corrs,
            #),

            # TODO TODO TODO version like hemidraw, but by pre-generating wPNKC, to have
            # similar distribution to below (but otherwise keeping draws of each input
            # indep). possible? (should be...). try just taking each cell in real
            # connectome, taking # of connections from that cell, then drawing from
            # whatever distribution up to that number of connections? (in contrast to
            # current hemidraw/uniform cases which both have a fixed number of
            # connections [called "claws" in modelling code, not synapses, but maybe
            # assumed to be somewhat interchangeable?] per model KC)

            # TODO TODO maybe try a version of model where we use connectome weights for
            # either APL->KC or KC->APL, and then we [try to] scale one/both of them
            # up/down to achieve target sparsity? (would have to modify model code
            # somewhat)

            ## TODO TODO this actually make sense to try (w/ tune_on_hallem=True and
            ## drop_receptors_not_in_hallem=False?) does my code even support
            ## currently?
            ##dict(orn_deltas=pebbled_input_df, tune_on_hallem=True,
            ##    pn2kc_connections='uniform', n_claws=7, n_seeds=n_seeds
            ##),

            # TODO delete?
            #dict(
            #    orn_deltas=pebbled_input_df,
            #    responses_to_suffix=responses_to_suffix,
            #    tune_on_hallem=False,
            #    pn2kc_connections='fafb-left',

            #    # didn't help, unlike in hemibrain case
            #    # NOTE: seems like somewhere around 12-13 will produce similar value for
            #    # avg # of synapses per model KC (as in hemibrain w/ weight_divisor=20:
            #    # ~7.36). despite matching this one value, spearman-of-pearsons [model
            #    # vs real KCs] is still just as bad, or even slightly worse, with
            #    # weight_divisor=12 (compared to both weight_divisor=None|20)
            #    weight_divisor=12,

            #    comparison_orns=comparison_orns,
            #    comparison_kc_corrs=comparison_kc_corrs,
            #),
            #dict(
            #    orn_deltas=pebbled_input_df,
            #    responses_to_suffix=responses_to_suffix,
            #    tune_on_hallem=False,
            #    pn2kc_connections='fafb-right',

            #    # didn't help. see comment for fafb-left above.
            #    #weight_divisor=12,

            #    comparison_orns=comparison_orns,
            #    comparison_kc_corrs=comparison_kc_corrs,
            #),
        ]
        if panel == 'megamat':
            # TODO TODO make sure that my derived wPNKC from pratyush's data
            # actually has KCs getting at least some input from all the PN types i have
            # in wPNKC PN labels (and that i'm not subsetting down to "halfmat"
            # unnecessarily, from earlier comparisons to matt's stuff, or whatever)
            # TODO TODO could probably just compare outputs of hallem model w/ and
            # w/o _use_matt_wPNKC...
            # TODO TODO can pass hallem data in explicitly if that makes it
            # easier to get things handled exactly the same

            # NOTE: model responses (including cache) should only include these odors.
            # could `-i model` if wanted to change and regen cache.
            sim_odors = sorted(hallem_for_comparison.index)
            # TODO make work again. need to make corr diff plot for all odors, w/
            # megamat ones pulled out. this seemed the easiest way...
            #sim_odors = None

            # parameter combinations to recreate preprint figures, using same Hallem
            # data as input (that Matt did when making those figures, before we had our
            # own ORN outputs)
            preprint_repro_model_kw_list = [
                dict(pn2kc_connections='hemibrain',

                    # TODO TODO delete/comment
                    # need to fix breakpoint hit in fit_mb_model (currently just
                    # commented it...)
                    _use_matt_wPNKC=True,

                    sim_odors=sim_odors,

                    comparison_orns=hallem_for_comparison,
                    comparison_kc_corrs=comparison_kc_corrs,

                    # TODO TODO don't require this passed in! (do unconditionally)
                    _strip_concs_comparison_kc_corrs=True,
                ),

                dict(pn2kc_connections='uniform', n_claws=7,

                    # TODO TODO TODO delete/comment
                    # need to fix breakpoint hit in fit_mb_model
                    _use_matt_wPNKC=True,

                    # TODO probably also _add_back_methanoic_acid_mistake=True?
                    # shouldn't matter...
                    #_add_back_methanoic_acid_mistake=True,

                    n_seeds=n_seeds,

                    sim_odors=sim_odors,

                    comparison_orns=hallem_for_comparison,
                    comparison_kc_corrs=comparison_kc_corrs,

                    # TODO TODO don't require this passed in! (do unconditionally)
                    #
                    # since outputs of model will have ' @ -2' when using Hallem input,
                    # but KC comparison data has ' @ -3'. this will strip conc from all
                    # odor strings, when comparing data from these variables.
                    # NOTE: comparison_orns path currently strips unconditionally...
                    _strip_concs_comparison_kc_corrs=True,
                ),
                dict(pn2kc_connections='hemidraw', n_claws=7,
                    _use_matt_wPNKC=True,
                    n_seeds=n_seeds,
                    sim_odors=sim_odors,
                    comparison_orns=hallem_for_comparison,
                    comparison_kc_corrs=comparison_kc_corrs,
                    _strip_concs_comparison_kc_corrs=True,
                ),
            ]
            if not skip_hallem_models:
                # all entries in preprint_repro_model_kw_list use hallem data, and all
                # entries in model_kw_list (before line below) should not use hallem
                # data
                model_kw_list = model_kw_list + preprint_repro_model_kw_list
            else:
                warn('skipping all models using Hallem data (rather than our measured '
                    'ORN data) as input (because `-s model-hallem` CLI option)'
                )

        # hack to skip long running models, if I want to test something on pebbled and
        # hallem cases w/o re-running many seeds before getting an answer on the test.
        if skip_models_with_seeds:
            old_len = len(model_kw_list)
            model_kw_list = [x for x in model_kw_list if 'n_seeds' not in x]

            n_skipped = old_len - len(model_kw_list)
            warn(f'currently skipping {n_skipped} models with seeds! (because '
                '`-s model-seeds` CLI option)'
            )

        for model_kws in model_kw_list:

            for target_sparsity in target_sparsities:
                _model_kws = dict(model_kws)
                _model_kws['target_sparsity'] = target_sparsity

                if panel == 'megamat':
                    _model_kws['repro_preprint_s1d'] = True

                do_sensitivity_analysis = False

                if model_kws.get('sensitivity_analysis', False):
                    do_sensitivity_analysis = True

                if skip_sensitivity_analysis or not do_sensitivity_analysis:
                    try:
                        # assumes the default is False. could also set False explicitly,
                        # but not passing that in explicitly for other model_kws
                        # iterated over.
                        del _model_kws['sensitivity_analysis']
                    except KeyError:
                        pass

                else:
                    # TODO TODO also skip anything other than 1 or 2 hemibrain calls for
                    # these panels? move calls to natmix_data/analysis.py itself?
                    if panel in ('kiwi', 'control'):
                        # these values were for trying to find better combinations for
                        # use on newer kiwi/control data (for analysis in
                        # natmix_data/analysis.py). not 100% happy with outputs yet.
                        natmix_sens_analysis_kws = dict(
                            n_steps=7,
                            fixed_thr_param_lim_factor=0.75,

                            # TODO just leave these last 3 at default (that's what these
                            # values are)?
                            wAPLKC_param_lim_factor=5.0,
                            drop_nonpositive_fixed_thr=True,
                            drop_negative_wAPLKC=True,
                        )
                        warn('using diff sens_analysis_kws for kiwi/control panels:\n' +
                            pformat(natmix_sens_analysis_kws)
                        )
                        _model_kws['sens_analysis_kws'] = natmix_sens_analysis_kws

                if ('orn_deltas' in model_kws and
                    model_kws['orn_deltas'] is pebbled_input_df):

                    param_dir_prefix = pebbled_param_dir_prefix
                else:
                    # these cases should be all and only the hallem input data cases,
                    # where the only parameter in this prefix (the dF/F scaling before
                    # fitting) is not relevant.
                    param_dir_prefix = ''

                # TODO is this loop working as expected? in run on kiwi+control
                # data, i feel like i've seen more progress bars than i expected...
                # (should be 2 * 3, no? i.e. {hemidraw, uniform} X {control-kiwi,
                # control, kiwi}?) none of the duplicate-save-within-run detection
                # seemed to trip tho...

                fixed_thr = None
                wAPLKC = None
                _extra_params = dict(extra_params)
                # checking for orn_deltas because we don't want to ever do this
                # pre-tuning for hallem data (where the ORN data isn't passed here, but
                # loaded inside fit_mb_model)
                if 'orn_deltas' in model_kws and panel in panel2tuning_panels:
                    tuning_panels = panel2tuning_panels[panel]
                    tuning_panels_str = tuning_panel_delim.join(tuning_panels)

                    tuning_panels_plot_dir = plot_dir / tuning_panels_str
                    makedirs(tuning_panels_plot_dir)

                    panel_mask = mean_est_df.columns.get_level_values('panel'
                        ).isin(tuning_panels)

                    if panel_mask.sum() == 0:
                        raise RuntimeError(f'no data from {tuning_panels=}!\n\nedit '
                            'panel2tuning_panels if you do not intended to tune '
                            f'{panel=} data on these panels.\n\nyou may also just need '
                            'to change script CLI args to include this data.'
                        )

                    tuning_df = mean_est_df.loc[:, panel_mask]

                    tuning_model_kws = {k: v for k, v in _model_kws.items()
                        if k not in (
                            'orn_deltas', 'comparison_kc_corrs', 'comparison_orns'
                        )
                    }

                    # NOTE: if i wanted to do this pre-tuning on hallem data (which is
                    # loaded in fit_mb_model if orn_deltas not passed here), i'd need to
                    # not pass this. no real need to use this on hallem data tho.
                    #
                    # TODO (delete) need to drop panel level on tuning_df first
                    # (doesn't seem so...)? (if so, prob also want to prefix panel
                    # to odor names, or otherwise ensure no dupes?)
                    tuning_model_kws['orn_deltas'] = tuning_df

                    param_dict = fit_and_plot_mb_model(tuning_panels_plot_dir,
                        param_dir_prefix=param_dir_prefix,
                        extra_params=extra_params,
                        # TODO maybe set False for 'megamat' (if also tuning on
                        # 'megamat'), so that we can compare those two directories more
                        # easily, and maybe leave that test code in?
                        _only_return_params=True,
                        **tuning_model_kws
                    )
                    _write_inputs_for_reproducibility(tuning_panels_plot_dir,
                        param_dict
                    )

                    fixed_thr = param_dict['fixed_thr']
                    wAPLKC = param_dict['wAPLKC']
                    assert fixed_thr is not None
                    assert wAPLKC is not None

                    try:
                        # should be a list if it has a len
                        len(fixed_thr)
                        # will only get here if model has multiple seeds
                        # (i.e. if fixed_thr is a list)
                        assert len(fixed_thr) == len(wAPLKC)

                        # NOTE: currently relying on the pre-tuning + actual modelling
                        # calls using the same sequences of seeds (which they do, b/c
                        # initial seed currently hardcoded, and i always increment
                        # following seeds by one from there), so that applying the
                        # sequence of inh params across the two makes sense

                    # to catch:
                    # TypeError: object of type 'int' has no len()
                    # TypeError: object of type 'float' has no len()
                    except TypeError:
                        pass

                    del _model_kws['target_sparsity']
                    _model_kws['title_prefix'] = f'tuning panels: {tuning_panels_str}\n'

                    _extra_params['tuning_panels'] = tuning_panels_str
                    _extra_params['tuning_output_dir'] = param_dict['output_dir']

                    param_dir_prefix = \
                        f'tuned-on_{tuning_panels_str}__{param_dir_prefix}'

                params_for_csv = fit_and_plot_mb_model(panel_plot_dir,
                    param_dir_prefix=param_dir_prefix, extra_params=_extra_params,
                    fixed_thr=fixed_thr, wAPLKC=wAPLKC, **_model_kws
                )
                _write_inputs_for_reproducibility(panel_plot_dir, params_for_csv)

                # should only be the case if first_seed_only=True inside fit_and_plot...
                # (just used to regen model internal plots, which are made on first seed
                # for multi-seed runs. no downstream plots/caches are updated by
                # fit_and_plot... in that case).
                if params_for_csv is None:
                    continue

                if skip_models_with_seeds:
                    warn(f'not writing to {model_param_csv} (b/c '
                        'skip_models_with_seeds=True)!'
                    )
                    continue

                # should only be added if wAPLKC/fixed_thr passed, which should not
                # be the case in any of these calls
                assert 'pearson' not in params_for_csv

                if model_params is None:
                    model_params = pd.Series(params_for_csv).to_frame().T
                else:
                    # works (adding NaN) in both cases where appended row has
                    # more/less columns than existing data.
                    model_params = model_params.append(params_for_csv,
                        ignore_index=True
                    )

                # just doing in loop so if i interrupt early i still get this. don't
                # think i mind always overwritting this from past runs.
                #
                # NOTE: can't use wrapper here b/c it asserts output wasn't already
                # saved this run.
                model_params.to_csv(model_param_csv, index=False)

        if skip_models_with_seeds:
            warn('not making across-model plots (S1C/2E) (b/c '
                'skip_models_with_seeds=True)!'
            )
            continue

        # TODO TODO how much of stuff below should i factor out of model_mb_responses
        # (into something al_analysis calls after model_mb_responses is done. would
        # probably want to return something from this fn)?

        if panel != 'megamat':
            # TODO (at least if verbose) warn we warn we are skipping rest?
            continue

        # NOTE: special casing handling of this plot. other plots dealing with errorbars
        # across seeds will NOT subset seeds to first 20 (using global
        # `n_first_seeds_for_errorbar = None` instead)
        fig2e_n_first_seeds = 20
        _, fig2e_seed_err_fname_suffix = _get_seed_err_text_and_fname_suffix(
            n_first_seeds=fig2e_n_first_seeds
        )

        remy_2e_corrs = load_remy_2e_corrs(panel_plot_dir)

        # don't actually care about output data here, but it will save extra a plot
        # showing we can recreate the preprint fig 2E when use_preprint_data=True
        load_remy_2e_corrs(panel_plot_dir, use_preprint_data=True)

        # should already be sorted by mean-pair-correlation in load_remy_2e_corrs,
        # with all entries of each pair grouped together
        remy_2e_pair_order = remy_2e_corrs.odor_pair_str.unique()

        remy_2e_odors = (
            set(remy_2e_corrs.abbrev_row) | set(remy_2e_corrs.abbrev_col)
        )

        remy_pairs = set(list(zip(
            remy_2e_corrs.abbrev_row, remy_2e_corrs.abbrev_col
        )))


        # TODO refactor to share w/ load fn? delete in one or the other?
        pal = sns.color_palette()
        # green: hemibrain, orange: uniform, blue: hemidraw, black: observed
        label2color = {
            # green (but not 'green' exactly)
            'hemibrain': pal[2],
            # orange (but not 'orange' exactly)
            'uniform': pal[0],
            # blue (but not 'blue' exactly)
            'hemidraw': pal[1],
        }
        #

        # TODO try to make error bars only shown outside hollow circles?

        assert not model_params.output_dir.duplicated().any()

        # (fails if first_seed_only=True in fit_and_plot..., but that's only for
        # manual regeneration of fit_mb_model's model_internals/ plots, and should
        # never stay True)
        assert len(model_kw_list) == len(model_params)
        pebbled_mask = np.array(
            [x.get('orn_deltas') is pebbled_input_df for x in model_kw_list]
        )

        pn2kc_order = [
            'hemidraw',
            'uniform',
            'hemibrain',
        ]
        def _sort_pn2kc(x):
            if x in pn2kc_order:
                return pn2kc_order.index(x)
            else:
                return float('inf')

        # TODO i assume these are all in hallem?
        # NOTE: none of these are in Remy's validation2 panel (so I don't have them
        # in any of my pebbled data, as they also aren't in megamat, which is the
        # only other panel of hers I collected)
        #
        # ones not in megamat 17:
        # - 1-penten-3-ol
        # - delta-decalactone
        # - ethyl cinnamate
        # - eugenol
        # - gamma-hexalactone
        # - methyl octanoate
        # - propyl acetate

        # intentionally not dropping any silent/bad cells here. always want all
        # cells included for these type of plots.
        remy_binary_responses = load_remy_megamat_kc_binary_responses()

        # TODO comment explaining what all is done in this loop (/ below)?
        for desc, mask in (('pebbled', pebbled_mask), ('hallem', ~ pebbled_mask)):

            if mask.sum() == 0:
                warn(f'no {desc} data in current model runs. skipping 2E/S1C/etc!')
                continue

            # one row per model run
            curr_model_params = model_params.loc[mask]

            curr_model_params = curr_model_params.sort_values('pn2kc_connections',
                kind='stable', key=lambda x: x.map(_sort_pn2kc)
            )

            try:
                # TODO just drop_duplicates to keep first row for each, and warn we are
                # doing that instead?
                #
                # since we'll use this for line labels (e.g. 'hemibrain', 'uniform')
                assert not curr_model_params.pn2kc_connections.duplicated().any()
            except AssertionError:
                warn(f'duplicate pn2kc_connections values across {desc} models! '
                    'skipping 2E/S1C/etc!\n\ncomment/remove model_kw_list values to '
                    'remove these duplicates, to generate skipped plots.'
                )
                continue

            # e.g. 'hemibrain' -> DataFrame (Series?) with hemibrain model correlations
            pn_kc_cxn2model_corrs = dict()

            # inside the loop, we also make another version that only shows the KC data
            # that also has model data
            remy_2e_facetgrid = _create_2e_plot_with_obs_kc_corrs(remy_2e_corrs,
                remy_2e_pair_order, fill_markers=False
            )

            s1c_fig, s1c_ax = plt.subplots()

            first_model_pairs = None
            remy_2e_modelsubset_facetgrid = None

            # TODO comment explaining what all is done in this loop (/ below)?
            for i, row in enumerate(curr_model_params.itertuples()):
                output_dirname = row.output_dir
                output_dir = panel_plot_dir / output_dirname
                responses_cache = output_dir / model_responses_cache_name
                responses = pd.read_pickle(responses_cache)

                label = row.pn2kc_connections
                assert type(label) is str and label != ''

                color = label2color[label]

                responses.columns = responses.columns.map(olf.parse_odor_name)
                assert not responses.columns.isna().any()
                assert not responses.columns.duplicated().any()

                # at least for now, doing this here so that i don't need to re-run
                # model after abbrev_hallem_odor_index change (currently commented).
                # would also need to figure out how to deal w/ 'moct' if i wanted to
                # remove this (was thinking of changing 'MethOct' -> 'moct' in
                # load_remy_2e...)
                # thought I needed to do before corr_triangular, in order to get same
                # order as remy has for all the pairs, but moving this here didn't fix
                # all of that issue.
                my2remy_odor_names = {
                    'eugenol': 'eug',
                    'ethyl cinnamate': 'ECin',
                    'propyl acetate': 'PropAc',
                    'g-hexalactone': 'g-6lac',
                    'd-decalactone': 'd-dlac',
                    'moct': 'MethOct',
                    # I already had an abbreviation for the 7th
                    # ('1-penten-3-ol' -> '1p3ol'), which is consistent w/ hers.
                }
                ordered_pairs = None
                # thought I needed to do before corr_triangular, in order to get same
                # order as remy has for all the pairs, but moving this here didn't fix
                # all of that issue. still doing before corr_triangular, so my odor
                # names will line up with Remy's when I now pass in new ordered_pairs
                # kwarg to corr_triangular, which I added to manually fix this issue.
                if desc == 'hallem':
                    odor_strs = responses.columns

                    for old, new in my2remy_odor_names.items():
                        assert (odor_strs == new).sum() == 0
                        assert (odor_strs == old).sum() == 1
                        # TODO delete
                        #assert odor_strs.str.contains(f'{new} @').sum() == 0
                        #assert odor_strs.str.contains(f'{old} @').sum() == 1

                        odor_strs = odor_strs.str.replace(old, new)

                        # TODO delete
                        #assert odor_strs.str.contains(f'{new} @').sum() == 1
                        assert (odor_strs == new).sum() == 1

                    responses.columns = odor_strs

                    # any pairs (a, b) seen here will be used over any (b, a)
                    # corr_triangular would otherwise use. OK if not all pairs
                    # represented here (e.g. like how Remy's pairs are not all of the
                    # possible Hallem pairs, but this will at least make sure the
                    # overlap is consistent)
                    ordered_pairs = remy_pairs

                responses_including_silent = responses.copy()

                # TODO or factor corr calc + dropping into one fn, and call that in the
                # 3 places that currently use this?
                if drop_silent_model_kcs:
                    responses = drop_silent_model_cells(responses)

                # TODO refactor to combine dropping -> correlation [->mean across seeds]
                if 'seed' in responses.index.names:
                    # TODO refactor to share w/ internals of mean_of_fly_corrs?
                    # (use new square=False kwarg?)
                    corrs = responses.groupby(level='seed').apply(
                        lambda x: corr_triangular(x.corr(), ordered_pairs=ordered_pairs)
                    )
                    assert len(corrs) == n_seeds
                else:
                    corrs = corr_triangular(responses.corr(),
                        ordered_pairs=ordered_pairs
                    )
                    # so shape/type is same as in seed case above.
                    # name shouldn't be important.
                    corrs = corrs.to_frame(name='correlation').T

                del ordered_pairs

                # TODO where are NaN coming from in here?
                # ipdb> corrs.isna().sum().sum()
                # 34773
                # ipdb> corrs.size
                # 599500
                # ipdb> corrs.isna().sum().sum() / corrs.size
                # 0.05800333611342786

                # TODO is this weird? just some seeds have odors w/o cells
                # responding to them?
                #
                # ipdb> responses.shape
                # (163000, 110)
                # ipdb> corrs.shape[1]
                # 5995
                # ipdb> (corrs == 1).sum()
                # odor1              odor2
                # -aPine @ -2        -bCar @ -2          0
                # ...
                # t2h @ -2           terpinolene @ -2    0
                #                    va @ -2             0
                # terpinolene @ -2   va @ -2             0
                # Length: 5995, dtype: int64
                # ipdb> (corrs == 1).sum().sum()
                # 63
                # ipdb> np.isclose(corrs, 1).sum().sum()
                # 63

                pairs = corrs.columns.to_frame(index=False)
                # choosing 2 means none of the pairs are from the diagonal of the
                # correlation matrix (no identity elements. no correlations of odors
                # with themselves.)
                assert not (pairs.odor1 == pairs.odor2).any()

                assert not pairs.duplicated().any()

                # TODO delete. now doing this on responses.columns above
                '''
                # removing the concentration part of each odor str, e.g.
                # 'a @ -3' -> 'a' (since Remy and I format that part slightly diff)
                pairs = pairs.applymap(olf.parse_odor_name)
                # if any odor is presented at >1 conc, this 1st assertion would trip
                assert not pairs.duplicated().any()
                assert not pairs.isna().any().any()
                '''

                # TODO delete. doing before corr_triangular now.
                #pairs = pairs.replace(my2remy_odor_names)

                corrs.columns = pd.MultiIndex.from_frame(pairs)

                model_odors = set(pairs.odor1) | set(pairs.odor2)

                model_pairs = set(list(zip(pairs.odor1, pairs.odor2)))

                # we only ever have one representation of a given pair, and it's
                # always the same one across remy_pairs and model_pairs
                assert not any(
                    (b, a) in model_pairs or (b, a) in remy_pairs
                    for a, b in remy_pairs | model_pairs
                )

                n_odors = responses.shape[1]
                assert corrs.shape[1] == n_choose_2(n_odors)

                if desc == 'hallem':
                    assert n_odors == 110
                    # NOTE: unlike in pebbled cases below, we do sometimes have some
                    # odors without cells responding to them in here, and thus some
                    # NaN correlations

                    assert len(remy_2e_odors - model_odors) == 0

                    # TODO any other assertions in here? maybe something to complement
                    # currently-failing one above? (re: (a,b) vs (b,a))
                    # or will renaming those few odors fix that?
                    #import ipdb; ipdb.set_trace()

                # true as long as we don't also want to use this to plot
                # megamat+validation2 data (or validation2 alone)
                # (currently this code all only runs for megamat panel)
                elif desc == 'pebbled':
                    assert n_odors == len(megamat_odor_names) == len(model_odors)

                    # might also not be true in cases other than megamat
                    assert not corrs.isna().any().any()

                    assert len(model_odors - remy_2e_odors) == 0

                    remy_2e_odors_not_in_model = remy_2e_odors - model_odors
                    if i == 0 and len(remy_2e_odors_not_in_model) > 0:
                        # we are already checking model_odors doesn't change across
                        # iterations of this inner loop, so it's OK to only warn on
                        # first iteration.
                        warn(f'Remy 2e odors not in current ({desc}) model outputs: '
                            f'{remy_2e_odors_not_in_model}'
                        )
                    #

                    # TODO also want something like this in desc='hallem' case?
                    #
                    # seems Remy and I are constructing our pairs in the same way
                    # (so that I don't need to re-construct one or the other to make
                    # sure we never have (a, b) in one and (b, a) in the other)
                    assert len(model_pairs - remy_pairs) == 0

                    # all the other pairs Remy has include at least one non-megamat
                    # odor
                    assert not any([
                        (a in megamat_odor_names) and (b in megamat_odor_names)
                        for a, b in remy_pairs - model_pairs
                    ])


                if i == 0:
                    assert first_model_pairs is None
                    first_model_pairs = model_pairs

                    if desc != 'hallem':
                        remy_2e_corrs_in_model_mask = remy_2e_corrs.apply(lambda x:
                            (x.abbrev_row, x.abbrev_col) in model_pairs, axis=1
                        )
                        # TODO reset_index(drop=True)? prob no real effect on plots...
                        remy_2e_corrs_in_model = remy_2e_corrs[
                            remy_2e_corrs_in_model_mask
                        ]

                        assert 0 == len(
                            # in pebbled+megamat case, the two sets should also be
                            # equal.  in hallem case, model_pairs will have many pairs
                            # not in what Remy gave me (but all of Remy's pairs should
                            # have both odors in Hallem).
                            set(list(zip(
                                remy_2e_corrs_in_model.abbrev_row,
                                remy_2e_corrs_in_model.abbrev_col
                            ))) - model_pairs
                        )

                        # unlike pair sets, elements here are str (e.g. 'a, b')
                        remy_2e_pair_order_in_model = np.array([
                            x for x in remy_2e_pair_order
                            if tuple(x.split(', ')) in model_pairs
                        ])
                        assert (
                            set(remy_2e_corrs_in_model.odor_pair_str) ==
                            set(remy_2e_pair_order_in_model)
                        )

                        # we also make a version of this where we show all KC pairs (and
                        # only model data when we can) before this loop.
                        remy_2e_modelsubset_facetgrid = \
                            _create_2e_plot_with_obs_kc_corrs(
                                remy_2e_corrs_in_model, remy_2e_pair_order_in_model,
                                fill_markers=False
                        )
                else:
                    assert first_model_pairs is not None
                    # checking each iteration of this loop would be plotting the same
                    # subset of data
                    assert first_model_pairs == model_pairs

                corrs.columns.names = ['abbrev_row', 'abbrev_col']

                corr_dists = 1 - corrs

                # ignore_index=False so index (one 'seed' level only) is preserved,
                # so error can be computed across seeds for plot
                corr_dists = corr_dists.melt(ignore_index=False,
                    value_name='correlation_distance').reset_index()

                assert label not in pn_kc_cxn2model_corrs
                # label is str describing pn2kc connections (e.g. 'hemibrain')
                pn_kc_cxn2model_corrs[label] = corrs

                corr_dists['odor_pair_str'] = (
                    corr_dists.abbrev_row + ', ' + corr_dists.abbrev_col
                )

                _2e_plot_model_corrs(remy_2e_facetgrid, corr_dists,
                    remy_2e_pair_order, color=color, label=label,
                    n_first_seeds=fig2e_n_first_seeds
                )

                if desc != 'hallem':
                    assert remy_2e_modelsubset_facetgrid is not None
                    _2e_plot_model_corrs(remy_2e_modelsubset_facetgrid, corr_dists,
                        remy_2e_pair_order_in_model, color=color, label=label,
                        n_first_seeds=fig2e_n_first_seeds
                    )

                # TODO why does the hemibrain line on this seem more like ~0.6 than
                # the ~0.5 in preprint? matter (remy wasn't concerned enough to
                # track down which outputs she originally made plot from)?
                # TODO also, why does tail seem different in pebbled plot? meaningful?
                if desc == 'hallem':
                    responses_including_silent = responses_including_silent.loc[:,
                        # TODO delete (or revert, if plot_n_odors_per_cell doesn't work
                        # w/ concs stripped from responses...)
                        #responses_including_silent.columns.map(odor_is_megamat)
                        #
                        # responses.columns now have concentrations stripped, so
                        # checking this way rather than .map(odor_is_megamat)
                        responses_including_silent.columns.isin(megamat_odor_names)
                    ]

                assert (
                    len(responses_including_silent.columns) == len(megamat_odor_names)
                )
                plot_n_odors_per_cell(responses_including_silent, s1c_ax, label=label,
                    color=color
                )


            _finish_remy_2e_plot(remy_2e_facetgrid, n_first_seeds=fig2e_n_first_seeds)

            if desc != 'hallem':
                # TODO delete
                assert all(
                    '__data_pebbled__' in x.name or x.name == 'megamat'
                    for x, _, _ in _spear_inputs2dfs.keys()
                )

                mc_key = (
                    Path('pebbled_6f/pdf/ijroi/mb_modeling/megamat'),
                    'mean_kc_corr',
                    'mean_orn_corr'
                )
                assert _spear_inputs2dfs[mc_key].equals(merged_corrs)
                del _spear_inputs2dfs[mc_key]

                assert all(
                    (x.endswith('_dist') and y.endswith('_dist')) or
                    not (x.endswith('_dist') or y.endswith('_dist'))
                    for _, x, y in _spear_inputs2dfs.keys()
                )

                len_before = len(_spear_inputs2dfs)
                # would raise error if search didn't work on one (hence del above).
                # replacing Path objects with the relevant str part of their name,
                # for easier accessing.
                _spear_inputs2dfs = {
                    (re.search('pn2kc_([^_]*)__', p.name).group(1), x, y): df
                    # TODO delete. why wasn't this working for uniform/hemidraw
                    # (included extra past '__')?
                    #(re.search('pn2kc_(.*)__', p.name).group(1), x, y): df
                    for (p, x, y), df in _spear_inputs2dfs.items()
                }
                # checking we didn't map any 2 keys before to 1 key now
                assert len(_spear_inputs2dfs) == len_before

                assert orn_col == 'mean_orn_corr'

                model_corrs = []
                prev_model_corr = None
                for (pn2kc, x, y), odf in _spear_inputs2dfs.items():

                    if odf.index.names != ['odor1','odor2']:
                        odf = odf.set_index(['odor1','odor2'], verify_integrity=True)

                    if y == 'orn_corr':
                        s1 = merged_corrs[orn_col]
                        model_corr = odf['model_corr'].copy()
                        model_corr.name = f'{pn2kc}_corr'
                        model_corrs.append(model_corr)
                    else:
                        assert y == 'observed_kc_corr_dist'
                        # converting to correlation DISTANCE, to match `y` here
                        s1 = 1 - merged_corrs[kc_col]

                        model_corr = 1 - odf['model_corr_dist']

                        # these y == 'observed_kc_corr_dist' entries should always
                        # follow a y == 'orn_corr' entry with the same pn2kc value
                        assert prev_model_corr is not None
                        assert pd_allclose(model_corr, prev_model_corr)

                    prev_model_corr = model_corr

                    s2 = odf[y]
                    assert pd_allclose(s1, s2)

                merged_corrs = pd.concat([merged_corrs] + model_corrs, axis='columns',
                    verify_integrity=True
                )

                index_no_concs = merged_corrs.index.map(
                    # takes 2-tuples of ['odor1','odor2'] strs and strips concs
                    lambda x: (x[0].split(' @ ')[0], x[1].split(' @ ')[0])
                )
                assert all(
                    x.columns.equals(index_no_concs)
                    for x in pn_kc_cxn2model_corrs.values()
                )

                model_corrs2 = []
                for pn2kc, corrs in pn_kc_cxn2model_corrs.items():
                    # each `corrs` should be of shape (1|n_seeds, n_odors_choose_2)
                    if len(corrs) > 1:
                        assert corrs.index.name == 'seed'

                    mean_corrs = corrs.mean()
                    mean_corrs.name = f'{pn2kc}_corr'
                    model_corrs2.append(mean_corrs)

                model_corrs2 = pd.concat(model_corrs2, axis='columns',
                    verify_integrity=True
                )
                assert model_corrs2.index.equals(index_no_concs)
                model_corrs2.index = merged_corrs.index

                model_corrs1 = merged_corrs.iloc[:, 2:]
                assert set(model_corrs2.columns) == set(model_corrs1.columns)
                model_corrs2 = model_corrs2.loc[:, model_corrs1.columns]

                # TODO replace all model_corrs1 code w/ model_corrs2? (-> delete _spear*
                # global / etc)? (assertion below passing, so they are equiv now)
                #
                # no NaN in either, else we would want equal_nan=True
                assert pd_allclose(model_corrs1, model_corrs2)

                # checking nothing looks like a correlation DISTANCE (range [0, 2])
                #
                # the .[min|max]() calls returns series w/ index the 2 real (ORN, KC)
                # corrs, and the 3 model corrs, w/ the min|max for each, so we are
                # checking that each corr column has expected range.
                assert (merged_corrs.min() < 0).all()
                assert (merged_corrs.max() < 1).all()

                col_pairs = list(itertools.combinations(merged_corrs.columns, 2))
                # TODO names matter (for invert*)? omit?
                index = pd.MultiIndex.from_tuples(col_pairs, names=['c1', 'c2'])

                # compare mean-ORN / mean-KC / model Pearson's correlations, using
                # Spearman's correlation
                for method in ('spearman', 'pearson'):
                    corr_of_pearsons = merged_corrs.corr(method=method)
                    xlabel = f"{method.title()}s-of-Pearsons"

                    output_name_without_ci = f'{method}_of_pearsons'

                    plot_corr(corr_of_pearsons, panel_plot_dir, output_name_without_ci,
                        overlay_values=True, xlabel=xlabel
                    )

                    for ci in (90, 95):
                        ci_str = f'{ci:.0f}'
                        ci_title_str = f'{ci_str}% CI'
                        # TODO do something other than average over 100 seeds?

                        output_name = f'{output_name_without_ci}_ci{ci_str}'

                        corrs = []
                        lower_cis = []
                        upper_cis = []
                        pvals = []
                        for x, y in col_pairs:
                            # TODO add one extra sigfig (in spear_text, the first
                            # returned arg from bootstrapped_corr) for the correlation
                            # and CI part (from 2 sigfigs -> 3)
                            _, corr, ci_lower, ci_upper, pval = bootstrapped_corr(
                                merged_corrs, x, y, method=method, ci=ci
                            )
                            corrs.append(corr)
                            lower_cis.append(ci_lower)
                            upper_cis.append(ci_upper)
                            pvals.append(pval)

                        corr_df = pd.DataFrame({
                                'corr': corrs, 'lower': lower_cis, 'upper': upper_cis,
                                'pval': pvals
                            }, index=index
                        )
                        to_csv(corr_df, panel_plot_dir / f'{output_name}.csv')

                        square_corr = invert_corr_triangular(corr_df['corr'], name=None)
                        assert pd_allclose(square_corr, corr_of_pearsons)

                        # are CI's symmetric (no) (i.e. can i get one measure of error
                        # for each matrix element, or will it make more sense to have 2
                        # extra matrix plots, one for lower CI and one for upper CI?)
                        square_lower = invert_corr_triangular(corr_df['lower'],
                            name=None
                        )
                        square_upper = invert_corr_triangular(corr_df['upper'],
                            name=None
                        )

                        plot_corr(square_lower, panel_plot_dir,
                            f'{method}_of_pearsons_lower_ci{ci_str}',
                            overlay_values=True,
                            xlabel=f'{xlabel}\nlower side of {ci_title_str}'
                        )
                        plot_corr(square_upper, panel_plot_dir,
                            f'{method}_of_pearsons_upper_ci{ci_str}',
                            overlay_values=True,
                            xlabel=f'{xlabel}\nupper side of {ci_title_str}'
                        )

                assert remy_2e_modelsubset_facetgrid is not None
                _finish_remy_2e_plot(remy_2e_modelsubset_facetgrid,
                    n_first_seeds=fig2e_n_first_seeds
                )

            # seed_errorbar is used internally by plot_n_odors_per_cell
            savefig(remy_2e_facetgrid, panel_plot_dir,
                f'2e_{desc}{fig2e_seed_err_fname_suffix}'
            )

            # model subset same in this case
            if desc != 'hallem':
                savefig(remy_2e_modelsubset_facetgrid, panel_plot_dir,
                    f'2e_{desc}_model-subset{fig2e_seed_err_fname_suffix}'
                )

            # TODO double check error bars are 95% ci. some reason matt's are so much
            # larger? previous remy data really much more noisy here?
            # (pretty sure current errorbars are right. not sure if old ones were, or
            # what spread was like there)
            plot_n_odors_per_cell(remy_binary_responses, s1c_ax, label='observed',
                color='black'
            )

            s1c_ax.legend()
            # errorbars are really small for model here, and can barely see CI's get
            # bigger increasing from 95 to 99
            s1c_ax.set_title(f'model run on {desc}\n{seed_err_text}')

            savefig(s1c_fig, panel_plot_dir,
                f's1c_n_odors_vs_cell_frac_comparison_{desc}'
            )


# TODO try to move all fns below to al_analysis wrapping model_mb_responses output?
# (or otherwise separate out a simplest-possible version of running model code, that
# doesn't involve all the plotting for paper w/ remy)
# (or at least move to al_util, to declutter this file?)

n_megamat_odors = 17
assert len(megamat_odor_names) == n_megamat_odors

# TODO put in docstring which files we are loading from
def _load_remy_megamat_kc_responses(drop_nonmegamat: bool = True, drop_pfo: bool = True
    ) -> pd.DataFrame:

    fly_response_root = remy_data_dir / 'megamat17' / 'per_fly'
    response_file_to_use = 'xrds_suite2p_respvec_mean_peak.nc'
    # Remy confirmed it's this one
    response_calc_to_use = 'Fc_zscore'

    olddata_fly_response_root = remy_data_dir / '2024-11-12'
    olddata_response_file_to_use = 'xrds_responses.nc'

    # TODO is it a problem that we are using peak_amp here and something zscored above?
    # is this actually zscored too? matter?
    #
    # other variables in these Datasets:
    # Data variables:
    #     peak_amp          (trials, cells) float64 ...
    #     peak_response     (trials, cells) float64 ...
    #     bin_response      (trials, cells) int64 ...
    #     baseline_std      (trials, cells) float64 ...
    #     baseline_med      (trials, cells) float64 ...
    #     peak_idx          (trials, cells) int64 ...
    olddata_response_calc_to_use = 'peak_amp'

    verbose = al_util.verbose
    if verbose:
        print()
        print('loading Remy megamat KC responses to compute (odor X odor) corrs:')

    _seen_date_fly_combos = set()
    mean_response_list = []

    for fly_dir in fly_response_root.glob('*/'):

        if not fly_dir.is_dir():
            continue

        # corresponding correlation .nc file in fly_dir / 'RDM_trials' should also be
        # equiv to one element of above `corrs`
        fly_response_dir = fly_dir / 'respvec'
        fly_response_file = fly_response_dir / response_file_to_use

        if verbose:
            print(fly_response_file)

        responses = xr.open_dataset(fly_response_file)

        date = pd.Timestamp(responses.attrs[remy_date_col])
        assert len(remy_fly_cols) == 2 and 'fly_num' == remy_fly_cols[1]
        # should already be an int, just weird numpy.int64 type, and not sure that
        # behaves same in sets (prob does).
        fly_num = int(responses.attrs['fly_num'])
        thorimage = responses.thorimage
        if verbose:
            # NOTE: responses.attrs[x] seems to be equiv to responses.x
            print('/'.join(
                [str(responses.attrs[x]) for x in remy_fly_cols] + [thorimage]
            ))

        # excluding thorimage, b/c also don't want 2 recordings from one fly making it
        # in, like happened w/ her precomputed corrs
        assert (date, fly_num) not in _seen_date_fly_combos
        _seen_date_fly_combos.add( (date, fly_num) )

        n_cells = responses.sizes['cells']
        if verbose:
            print(f'{n_cells=}')

        assert (responses.iscell == 1).all().item()
        assert len(responses.attrs['bad_trials']) == 0

        # TODO move to checks=True?
        all_xid_set = set(responses.xid0.values)
        # TODO factor out this assertion to hong2p.util (probably do something like this
        # in a lot of places. use in those places too.)
        assert all_xid_set == set(range(max(all_xid_set) + 1))

        # NOTE: isin(...) does not work here if input is a Python set()
        # (so keeping good_xid as a DataArray, or whatever type it is)
        good_xid = responses.attrs['good_xid']
        good_cells_mask = responses.xid0.isin(good_xid)

        good_xid_set = set(good_xid)
        # we have some xid0 values other than those in attrs['good_xid']
        # (so Remy did not pre-subset the data, and we should have all the cells)
        assert len(all_xid_set - good_xid_set) > 0
        #

        n_good_cells = good_cells_mask.sum().item()
        assert n_good_cells < n_cells

        n_bad_cells = (~ good_cells_mask).sum().item()
        if verbose:
            print(f'{n_bad_cells=}')

        assert (n_good_cells + n_bad_cells) == responses.sizes['cells']

        checks = True
        if checks:
            single_fly_nc_files = list(remy_binary_response_dir.glob((
                f'{format_date(date)}__fly{fly_num:>02}__*/'
                f'{remy_fly_binary_response_fname}'
            )))
            assert len(single_fly_nc_files) == 1
            fly_binary_response_file = single_fly_nc_files[0]

            binary_responses = load_remy_fly_binary_responses(fly_binary_response_file,
                reset_index=False
            )

            binary_response_xids = set(binary_responses.index.get_level_values('xid0'))
            # binary responses don't have a subset of the XID, they have all of them
            # (i.e. they haven't been subset to just the good_xid cells by Remy)
            assert binary_response_xids == all_xid_set
            # TODO use factored out version of this when i make it
            assert binary_response_xids == set(range(max(binary_response_xids) + 1))

            assert np.array_equal(
                binary_responses.index.to_frame(index=False)[['cells_level_0','xid0']],
                np.array([responses.cells, responses.xid0]).T
            )

            # seems pretty good:
            # responders? False
            # 305 good-XID-cells / 1634 cells (0.187)
            # responders? True
            # 2166 good-XID-cells / 2547 cells (0.850)
            def _print_frac_good_xid(gdf):
                responder_val_set = set(gdf.responder)
                assert len(responder_val_set) == 1
                responder_val = responder_val_set.pop()
                assert responder_val in (False, True)

                if responder_val:
                    print('among responders:')
                else:
                    print('among silent cells:')
                # TODO delete
                #print(f'responders? {responder_val}')

                # this isin DOES (pandas index LHS, not DataArray) work w/ python set
                # arg
                good_xid_mask = gdf.index.get_level_values('xid0').isin(good_xid_set)

                n_good_cells = good_xid_mask.sum()
                n_cells = len(good_xid_mask)
                good_cell_frac = n_good_cells / n_cells
                # TODO actually inspect these outputs -> decide i'm happy with them ->
                # only run this code if verbose (via settings checks flag above in this
                # fn)
                print(f'{n_good_cells} good-XID-cells / {n_cells} cells'
                    f' ({good_cell_frac:.3f})'
                )

            if verbose:
                binary_responses['responder'] = binary_responses.any(axis='columns')
                binary_responses.groupby('responder').apply(_print_frac_good_xid)
                print()

        # another way to do the same thing:
        # responses = responses.where(good_cells_mask, drop=True)
        responses = responses.sel(cells=good_cells_mask)
        assert responses.sizes['cells'] == n_good_cells

        # doing this doesn't seem to preserve attrs (some way to? or are they only for
        # Dataset not DataArray?). seems DataArray support .attrs, but may just need to
        # manually assign from DataSet?
        #
        # (odors X trials) X cells
        responses = responses[response_calc_to_use]

        # odors X cells
        mean_responses = responses.groupby('stim').mean(dim='trials')

        # the reset_index(drop=True) is to remove cell numbers (which currently have
        # missing cells, because of xid-based dropping above, which might be confusing)
        mean_response_df = mean_responses.to_pandas().T.reset_index(drop=True)
        mean_response_df.index.name = 'cell'

        # referring to flies this way should make it simpler to compare to corrs in CSVs
        # Remy gave me for making fig 2E (which have a 'datefly' column, that should be
        # formatted like this)
        datefly = f'{format_date(date)}/{fly_num}'
        mean_response_df = util.addlevel(mean_response_df, 'datefly', datefly)

        # just to conform to format in loop below. only one recording for each of these
        # flies.
        mean_response_df = util.addlevel(mean_response_df, 'thorimage', thorimage)

        mean_response_list.append(mean_response_df)

        if verbose:
            print()

    # TODO delete. just to try to get new concat of new + old data to be in a similar
    # format.
    new_mean_responses = pd.concat(mean_response_list, verify_integrity=True)
    #

    # TODO TODO refactor to share as much of body of loop w/ above (convert to one loop,
    # and just special case a few things based on parent dir?). currently copied from
    # loop above.
    #
    # for this old data, don't have the same set of cells across any of the multiple
    # recordings for any one fly. always gonna be a diff set of cells.
    for fly_dir in olddata_fly_response_root.glob('*/'):

        if not fly_dir.is_dir():
            continue

        # corresponding correlation .nc file in fly_dir / 'RDM_trials' should also be
        # equiv to one element of above `corrs`
        fly_response_dir = fly_dir / 'respvec'
        fly_response_file = fly_response_dir / olddata_response_file_to_use

        if verbose:
            print(fly_response_file)

        responses = xr.open_dataset(fly_response_file)

        date = pd.Timestamp(responses.attrs[remy_date_col])
        assert len(remy_fly_cols) == 2 and 'fly_num' == remy_fly_cols[1]
        # should already be an int, just weird numpy.int64 type, and not sure that
        # behaves same in sets (prob does).
        fly_num = int(responses.attrs['fly_num'])
        thorimage = responses.thorimage
        if verbose:
            # NOTE: responses.attrs[x] seems to be equiv to responses.x
            print('/'.join(
                [str(responses.attrs[x]) for x in remy_fly_cols] + [thorimage]
            ))

        n_cells = responses.sizes['cells']
        if verbose:
            print(f'{n_cells=}')

        # old data doesn't have the attributes iscell or bad_trials

        # from Remy's code snippet she sent on 2024-11-12 via slack
        good_cells_mask = responses['iscell_responder'] == 1

        n_good_cells = good_cells_mask.sum().item()
        assert n_good_cells < n_cells

        n_bad_cells = (~ good_cells_mask).sum().item()
        if verbose:
            print(f'{n_bad_cells=}')

        assert (n_good_cells + n_bad_cells) == responses.sizes['cells']

        # another way to do the same thing:
        # responses = responses.where(good_cells_mask, drop=True)
        responses = responses.sel(cells=good_cells_mask)
        assert responses.sizes['cells'] == n_good_cells

        responses = responses[olddata_response_calc_to_use]

        # odors X cells
        mean_responses = responses.groupby('stim').mean(dim='trials')

        # the reset_index(drop=True) is to remove cell numbers (which currently have
        # missing cells, because of dropping above, which might be confusing)
        mean_response_df = mean_responses.to_pandas().T.reset_index(drop=True)
        mean_response_df.index.name = 'cell'

        # referring to flies this way should make it simpler to compare to corrs in CSVs
        # Remy gave me for making fig 2E (which have a 'datefly' column, that should be
        # formatted like this)
        datefly = f'{format_date(date)}/{fly_num}'
        mean_response_df = util.addlevel(mean_response_df, 'datefly', datefly)

        # multiple recordings for most/all flies, presumably each w/ diff odors
        mean_response_df = util.addlevel(mean_response_df, 'thorimage', thorimage)

        mean_response_list.append(mean_response_df)
        if verbose:
            print()

    # TODO compare format to new_mean_responses (temp debug var)
    mean_responses = pd.concat(mean_response_list, verify_integrity=True)

    odors = [olf.parse_odor(x) for x in mean_responses.columns]

    names = np.array([x['name'] for x in odors])
    assert megamat_odor_names - set(names) == set(), 'missing some megamat odors'

    # cast_int_concs=True to convert '-3.0' to '-3', to be consistent w/ mine
    odor_strs = [olf.format_odor(x, cast_int_concs=True) for x in odors]
    mean_responses.columns = odor_strs

    # so al_util.mean_of_fly_corrs works
    mean_responses.columns.name = 'odor1'

    # when also loading old (prior to final 4 flies) megamat data:
    # (set(names) - megamat_odor_names)={'PropAc', '1p3ol', 'pfo', 'g-6lac', 'eug',
    # 'd-dlac', 'MethOct', 'ECin'}
    if drop_nonmegamat:
        megamat_mask = [x in megamat_odor_names for x in names]
        megamat_mean_responses = mean_responses.loc[:, megamat_mask]

        if verbose:
            nonmegamat_odors = mean_responses.columns.difference(
                megamat_mean_responses.columns
            )
            warn('dropping the following non-megamat odors:\n'
                f'{pformat(list(nonmegamat_odors))}'
            )

        mean_responses = megamat_mean_responses

    # elif because we will have already dropped pfo if drop_nonmegamat=True
    # (pfo is not considered part of megamat)
    elif drop_pfo:
        pfo_mask = names == 'pfo'
        assert pfo_mask.sum() > 0
        mean_responses = mean_responses.loc[:, ~pfo_mask]

    return mean_responses


# TODO rename all of these fns to remove '_megamat' (unless i actually drop down to just
# megamat, but i don't think i want that?)? or just do it anyway to shorten these names?
def _remy_megamat_flymean_kc_corrs(ordered_pairs=None, **kwargs) -> pd.DataFrame:
    mean_responses = _load_remy_megamat_kc_responses(**kwargs)

    # TODO move some functionality like this into al_util.mean_of_fly_corrs (to average within
    # fly across recordings first)?
    recording_corrs = mean_responses.groupby(level=['datefly', 'thorimage'], sort=False
        ).apply(lambda x: corr_triangular(x.corr(), ordered_pairs=ordered_pairs))

    fly_corrs = recording_corrs.groupby(level='datefly', sort=False).mean()

    checks = True
    if checks:
        old_megamat_root = remy_data_dir / '2024-11-12'
        # TODO also load + check against stim_rdms__iscell_good_xid0__correlation.xlsx?
        # (it should have been generated from this .nc, w/ remy providing a script to
        # generate this xlsx file from it, but still...)
        corr_file_for_anoop = (
            old_megamat_root / 'xrda_stim_rdm_concat__iscell_good_xid0__correlation.nc'
        )

        # adapted from the example script Remy emailed (2024-11-07) Anoop alongside this
        # data (demo_megamat_new_and_old_by_fly_17_kc_soma_nls.py, from OdorSpaceShare
        # repo)
        da_stim_rdm_concat = xr.load_dataarray(corr_file_for_anoop)

        # otherwise .to_index() call below doesn't give me what i want
        da_stim_rdm_concat = da_stim_rdm_concat.set_index({
            'acq': remy_fly_cols + ['thorimage_name']
        })

        #            date_imaged  fly_num              thorimage_name
        # 0   2022-10-10        1                    megamat0
        # 1   2022-10-10        2                    megamat0
        # 2   2022-10-11        1                    megamat0
        # 3   2022-11-10        1            megamat0__dsub03
        # 4   2018-10-21        1         _002+_003+_004+_005
        # 5   2019-03-06        3                        _002
        # 6   2019-03-06        4                        _003
        # 7   2019-03-07        2                  _003+_0057
        # 8   2019-04-26        4                     fn_0002
        # 9   2019-05-09        4             fn_0001+fn_0003
        # 10  2019-05-09        5             fn_0001+fn_0002
        # 11  2019-05-23        2                     fn_0001
        # 12  2019-05-24        1                     fn_0003
        # 13  2019-05-24        3                     fn_0001
        # 14  2019-05-24        4             fn_0001+fn_0002
        # 15  2019-07-19        2  movie001+movie002+movie003
        # 16  2019-09-12        1             fn_0002+fn_0003
        # 17  2019-09-12        2             fn_0001+fn_0002
        fly_metadata = da_stim_rdm_concat.coords['acq'].to_index().to_frame(index=False)
        # we already have combined data across recordings (for flies that have multiple.
        # see rows w/ '+' in thorimage_name in comment above)
        assert len(fly_metadata) == len(fly_metadata[remy_fly_cols].drop_duplicates())

        assert fly_corrs.index.name == 'datefly'
        recalced_flies = set(fly_corrs.index)

        datefly_strs = fly_metadata[remy_fly_cols].astype(str).agg('/'.join, axis=1)
        flies_for_anoop = set(datefly_strs)
        assert recalced_flies == flies_for_anoop

        da_stim_rdm_concat = da_stim_rdm_concat.assign_coords(
            {'datefly': ('acq', datefly_strs)}).set_index({'acq': 'datefly'}
        )

        fly_corrs_has_dropped_non_megamat = kwargs.get('drop_nonmegamat', True)

        if fly_corrs_has_dropped_non_megamat:
            # (to compare to the single fly corrs in the .nc file Remy gave Anoop in
            # November 2024, which also included old megamat data, in addition to the
            # final 4 flies we had been using)
            corrs_to_compare_to_anoop_data = fly_corrs
        else:
            megamat_pairs = fly_corrs.columns.to_frame().applymap(odor_is_megamat
                ).all(axis='columns')
            corrs_to_compare_to_anoop_data = fly_corrs.loc[:, megamat_pairs]

            n_megamat_only_pairs = n_choose_2(n_megamat_odors)
            assert len(corrs_to_compare_to_anoop_data.columns) == n_megamat_only_pairs


        for datefly in corrs_to_compare_to_anoop_data.index:
            fly_corr = corrs_to_compare_to_anoop_data.loc[datefly]
            fly_corr = fly_corr.dropna()

            # need to go from '1-5ol @ -3.0' format row/col index odors have here,
            # to '1-5ol @ -3' as in fly_corr index
            fly_da_stim_rdm = da_stim_rdm_concat.sel(acq=datefly).to_pandas()

            odor_strs = [
                # TODO refactor this parsing -> formatting, to a fn just for normalizing
                # repr of conc part of str (to int)?
                olf.format_odor(olf.parse_odor(x), cast_int_concs=True)
                for x in fly_da_stim_rdm.index
            ]
            assert fly_da_stim_rdm.columns.equals(fly_da_stim_rdm.index)
            fly_da_stim_rdm.index = odor_strs
            fly_da_stim_rdm.columns = odor_strs

            # TODO delete if i remove assertion in corr_triangular that index/columns
            # names have to start with 'odor'
            fly_da_stim_rdm.index.name = 'odor'
            fly_da_stim_rdm.columns.name = 'odor'

            anoop_fly_corr = corr_triangular(1 - fly_da_stim_rdm,
                # TODO TODO do we actually need this tho? or was it the other changes
                # that made a diff?
                ordered_pairs=fly_corr.index
            )

            # TODO work? (comment what intention / effect is?)
            anoop_fly_corr = anoop_fly_corr.dropna()

            # seems we don't need to pass particular pairs into corr_triangular
            assert pd_allclose(fly_corr, anoop_fly_corr, equal_nan=True)

    # TODO delete
    # TODO check above equiv to this, at least if we no longer load old data?
    # TODO check this works if there are multiple thorimage level values (i.e.
    # recordings) for one pair (e.g. 1-6ol, 2-but) for any fly. should average the corrs
    # first, then compute average across flies.
    #mean_corr = al_util.mean_of_fly_corrs(mean_responses.T, id_cols=['datefly'])

    return fly_corrs


# don't need ordered_pairs here b/c output of this fn should be square, so it no longer
# matters.
def load_remy_megamat_mean_kc_corrs(**kwargs) -> pd.DataFrame:
    """Returns mean of fly correlations, for Remy's 4 final megamat KC flies.

    Drops cells from bad clusters (as Remy does, using xarray attrs['good_xid'] that she
    sets to good clusters, excluding clusters of bad cells, which should mostly be
    silent cells) before computing correlations. The 3 trials for each odor are
    averaged together into a single odor X cell response matrix before computing each
    fly's correlation. Correlation is computed within each fly, and then the average is
    computed across these correlations. This should all be consistent with how Remy
    computes correlations.
    """
    fly_corrs = _remy_megamat_flymean_kc_corrs(**kwargs)
    mean_corr_ser = fly_corrs.mean()
    mean_corr = invert_corr_triangular(mean_corr_ser)
    return mean_corr


remy_2e_metric = 'correlation_distance'

# TODO TODO try a version of this w/ either hollow points or no points (to show small
# errorbars that would otherwise get subsumed into point)
_fig2e_shared_plot_kws = dict(
    x='odor_pair_str',
    y=remy_2e_metric,

    errorbar=seed_errorbar,
    seed=bootstrap_seed,
    err_kws=dict(linewidth=1.5),

    markersize=7,
    #markeredgewidth=0,
)

def _check_2e_metric_range(df) -> None:
    # TODO cases where i'd want to warn instead?
    """Raises AssertionError if data range seems inconsistent w/ `remy_2e_metric`.
    """
    # TODO TODO assert things seem consistent w/ being correlation distance (or at
    # least, not correlation)
    if remy_2e_metric == 'correlation_distance':
        metric = df[remy_2e_metric]
        # if it were < 0, would suggest it's a correlation, not a correlation DISTANCE
        assert metric.min() >= 0
        # do we actually have values over 1 always tho? can just remove this if need be
        assert metric.max() > 1
    else:
        # could also do similar for 'correlation', but only ever using this one
        raise NotImplementedError("checking range only supported for remy_2e_metric="
            "'correlation_distance'"
        )

# TODO add some kind of module level dict of fig ID -> pair_order, and use to check each
# fig is getting the same pair_order across these two calls? or use so that only first
# call even takes pair_order, but then assert model pairs are a subset in the
# subsequence call(s)?
#
# need @no_constrained_layout since otherwise FacetGrid creation would warn with
# Warning: The figure layout has changed to tight
# (since my MPL config has constrained layout as default)
@no_constrained_layout
def _create_2e_plot_with_obs_kc_corrs(df_obs: pd.DataFrame, pair_order: np.array, *,
    fill_markers=True) -> sns.FacetGrid:

    _check_2e_metric_range(df_obs)

    odor_pair_set = set(pair_order)
    assert odor_pair_set == set(df_obs.odor_pair_str.unique())
    assert len(odor_pair_set) == len(pair_order)

    # don't have any identity correlations (odors correlated with themselves)
    assert not (df_obs.abbrev_row == df_obs.abbrev_col).any()

    color = 'k'

    if fill_markers:
        marker_kws = dict(markeredgewidth=0)
    else:
        # TODO TODO why are lines on these points thinner than in model corr plot call
        # (below)? (was because linewidth)
        marker_kws = dict(markerfacecolor='white', markeredgecolor=color)

    # other types besides array might work for pair_order, but I've only been using
    # arrays (as in Remy's code I adapted from)
    g = sns.catplot(
        data=df_obs,

        # TODO work to omit if input has x='odor_pair_str' values in sorted order i
        # want (and would it matter if subsequent calls had same order, or would it be
        # aligned?)
        # TODO what about if x column is a pd.Categorical(..., ordered=True)
        # (and if there are sometimes cases where data isn't aligned correctly across
        # calls, does this change the situation?)
        order=pair_order,

        kind='point',

        # TODO so it's jittering? can i seed that? not that it really matters, except
        # for running w/ -c flag...  i'm assuming seed= doesn't also seed jitter?
        # (haven't had -c flag trip, so i'm assuming it's not actually jittering [maybe
        # not enough data that there is a need?] or same seed controls that)
        #
        # jitter=False,

        color=color,

        aspect=2.5,
        height=7,
        #linewidth=1,

        **_fig2e_shared_plot_kws,
        **marker_kws
    )

    # test output same whether input is 'correlation' or 'correlation_distance', as
    # expected.
    pair_metrics = []
    for _, gdf in df_obs.groupby('odor_pair_str'):
        pair_metrics.append(gdf[remy_2e_metric].to_numpy())

    # one way ANOVA (null is that groups have same population mean. groups can be diff
    # sizes)
    result = f_oneway(*pair_metrics)
    # from scipy docs:
    # result.statistic: "The computed F statistic of the test."
    # result.pvalue: "The associated p-value from the F distribution."

    g.ax.set_title(
        f'{len(odor_pair_set)} non-identity odor pairs\n'
        # .2E will show 2 places after decimal w/ exponent (scientific notation)
        f'(one way ANOVA) F-statistic: {result.statistic:.2f}, p={result.pvalue:.2E}'
    )

    return g


@no_constrained_layout
def _2e_plot_model_corrs(g: sns.FacetGrid, df: pd.DataFrame, pair_order: np.ndarray,
    n_first_seeds: Optional[int] = n_first_seeds_for_errorbar, **kwargs) -> None:

    _check_2e_metric_range(df)

    if n_first_seeds is not None and 'seed' in df.columns:
        df = select_first_n_seeds(df, n_first_seeds=n_first_seeds)

    # TODO some way to get hue/palette to work w/ markeredgecolor? i assume not
    if 'hue' not in kwargs:
        assert 'color' in kwargs
        # TODO like? factor to share w/ other seed_errorbar plots?
        marker_kws = dict(markerfacecolor='None', markeredgecolor=kwargs['color'])
    else:
        # TODO keep? remy had before, but obviously prevents markeredgecolor working in
        # above case. not sure i care about this in hue/palette case.
        marker_kws = dict(markeredgewidth=0)

    sns.pointplot(data=df, order=pair_order, linestyle='none', ax=g.ax,
        **_fig2e_shared_plot_kws, **kwargs, **marker_kws
    )


# TODO move this (and related) to mb_model.py?
@no_constrained_layout
def _finish_remy_2e_plot(g: sns.FacetGrid, *, n_first_seeds=n_first_seeds_for_errorbar
    ) -> None:

    g.set_axis_labels('odor pairs', remy_2e_metric)
    # 0.9 wasn't enough to have axes title and suptitle not overlap
    g.fig.subplots_adjust(bottom=0.2, top=0.85)

    # TODO use paper's 1.2 instead? or just leave unset? just set min to 0?
    # TODO TODO why does it seem to be showing as 1.2 w/ this at 1.4 anyway?
    g.ax.set_ylim(0, 1.4)

    seed_err_text, _ = _get_seed_err_text_and_fname_suffix(n_first_seeds=n_first_seeds)

    g.fig.suptitle(f'odor-odor {remy_2e_metric}\n{seed_err_text}')

    sns.despine(fig=g.fig, trim=True, offset=2)
    g.ax.xaxis.set_tick_params(rotation=90, labelsize=8)

    # TODO move legend to bottom left (in top right now)?


# TODO rename to ...corr_dists or something?
def load_remy_2e_corrs(plot_dir=None, *, use_preprint_data=False) -> pd.DataFrame:

    # just for some debug outputs (currently 1 CSV w/ flies listed for each odor pair,
    # and recreation of Remy's old 2E plot). nothing hugely important.
    if plot_dir is not None:
        output_root = plot_dir
    else:
        output_root = Path('.')

    # TODO move relevant data to my own path in this repo (to pin version, independent
    # of what remy pushes to this repo) (-> use those files below)
    repo_root = Path.home() / 'src/OdorSpaceShare'
    assert repo_root.is_dir()

    preprint_data_folder = repo_root / 'preprint/data/figure-02/02e'

    # TODO roughly compare old vs new data? or just make plots w/ both (after settling
    # on error repr...)
    if use_preprint_data:
        warn('using pre-print data for 2E (set use_preprint_data=False to use newer '
            'data)!'
        )
        data_folder = preprint_data_folder
        csv_name = 'df_obs_plot_trialavg.csv'
    else:
        # TODO TODO TODO which flies are in this but not in old megamat data i'm now
        # loading for a lot of things? any?
        data_folder = repo_root / 'manuscript/data/figure-02/02e'
        # df_obs.csv in the same folder was one of her earlier attempts to get me a
        # newer version of this data, but was not completely consistent w/ format of old
        # CSV (and also had 'correlation' col that was actually correlation distance).
        # df_obs.csv should not be used.
        csv_name = 'df_obs_for_tom.csv'

    assert data_folder.is_dir()

    csv_path = data_folder.joinpath(csv_name)
    if al_util.verbose:
        print(f'loading Remy correlations for 2E from {csv_path}')

    df_obs = pd.read_csv(csv_path)

    assert not df_obs.isna().any().any()

    df_obs[['abbrev_row','abbrev_col']] = df_obs['odor_pair_str'].str.split(pat=', ',
        expand=True
    )
    assert (
        len(df_obs[['abbrev_row','abbrev_col']].drop_duplicates()) ==
        len(df_obs.odor_pair_str.drop_duplicates())
    )

    # (currently renaming my model output odors to match remy's, during creation of my
    # 2E plots, so no need for now)
    # TODO rename 'MethOct' -> 'moct', to be consistent w/ mine

    # TODO refactor to share def of these 2 odor cols w/ elsewhere?
    #
    # within each fly, expect each pair to only be reported once
    assert not df_obs.duplicated(subset=['datefly','abbrev_row','abbrev_col']).any()

    identity = df_obs.abbrev_col == df_obs.abbrev_row
    assert (df_obs[identity].correlation == 1).all()

    # we don't want to include these on plots, and the repeated 1.0 values also
    # interfere with some of the checks on my sorting.
    df_obs = df_obs[~identity].reset_index(drop=True)
    assert not (df_obs.correlation == 1).any()

    # plot ordering of odor pairs (ascending observed correlations)
    mean_pair_corrs = df_obs.groupby('odor_pair_str').correlation.mean()
    df_obs['mean_pair_corr'] = df_obs.odor_pair_str.map(mean_pair_corrs)

    # will start with low correlations, and end w/ the high ones (currently identity 1.0
    # corrs), as in preprint order.
    #
    # NOTE: if we ever really care to *exactly* recreate preprint 2E, we may need to use
    # Remy's order from:
    # np.load(data_folder.joinpath('odor_pair_ord_trialavg.npy'), allow_pickle=True)
    #
    # (previously committed code to use this order, but deleted now that I have my own
    # replacement for all new versions [which also either almost/exactly matches
    # preprint fig too])
    df_obs = df_obs.sort_values('mean_pair_corr', kind='stable').reset_index(drop=True)

    # TODO factor this into some check fn? haven't i done something similar elsewhere?
    #
    # checking all rows with a given pairs are adjacent after above sorting.
    # should be True since we are now dropping identity rows before sorting.
    # allows us to more easily derive order from output, for plotting against my own
    # model runs.
    last_seen_index = None
    for _, gdf in df_obs.groupby('odor_pair_str', sort=False):
        if last_seen_index is not None:
            assert last_seen_index + 1 == gdf.index.min()
        else:
            assert gdf.index.min() == 0

        last_seen_index = gdf.index.max()
        assert set(gdf.index) == set(range(gdf.index.min(), gdf.index.max() + 1))

    # .unique() has output in order first-seen, which (given check above), should be
    # same as sorting all pairs by mean correlation
    pair_order = df_obs.odor_pair_str.unique()
    assert np.array_equal(pair_order, mean_pair_corrs.sort_values().index)

    # TODO does it make sense that there is such a diversity of N counts for specific
    # pairs?  inspect pairs / flies + talk to remy.
    #
    # ipdb> df_obs.odor_pair_str.value_counts()
    # 1-6ol, 1-6ol    22
    # 2-but, 2-but    21
    # 1-6ol, 2-but    21
    # benz, benz      20
    # 1-6ol, benz     20
    #                 ..
    # aa, benz         4
    # ep, eug          3
    # PropAc, va       3
    # eug, ECin        3
    # 2h, MethOct      3
    # Name: odor_pair_str, Length: 205, dtype: int64
    # ipdb> set(df_obs.odor_pair_str.value_counts())
    # {3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22}
    s1 = df_obs.groupby('odor_pair_str').size()
    assert s1.equals(df_obs.groupby('odor_pair_str').nunique().datefly)

    # TODO delete (replacing w/ just 'datefly')?
    assert (df_obs.datefly.map(lambda x: x[:2]) == '20').all()
    df_obs['datefly_abbrev'] = df_obs.datefly.map(lambda x: x[2:])
    #

    unique_datefly_per_pair = df_obs.groupby('odor_pair_str', sort=False
        ).datefly_abbrev.unique()

    assert s1.equals(unique_datefly_per_pair.str.len().sort_index())
    del s1

    # TODO delete?
    n_summary = pd.concat([
            df_obs.groupby('odor_pair_str', sort=False).size(),
            unique_datefly_per_pair
        ], axis='columns'
    )
    n_summary.columns = ['n', 'datefly']

    assert np.array_equal(n_summary.index, pair_order)

    n_summary.datefly = n_summary.datefly.map(lambda x: ', '.join(x))

    if use_preprint_data:
        suffix = '_OLD-PREPRINT-DATA'
    else:
        suffix = ''

    # TODO TODO inspect with remy
    to_csv(n_summary, output_root / f'remy_2e_n_per_pair{suffix}.csv')
    #

    df_obs['correlation_distance'] = 1 - df_obs.correlation
    assert df_obs['correlation_distance'].max() <= 2.0
    # excluding equality b/c we already dropped identity
    assert df_obs['correlation_distance'].min() > 0

    # TODO check whether order in plots is same w/ and w/o passing order explicitly to
    # sns calls below (now that we are sorting df_obs to put pairs in same order).
    # (unclear from docs how categorical order is inferred...)

    # TODO want to subset before returning? any actual need to?
    # affect plots (no, right?)?
    #
    # RY: "reorder columns"
    df_obs = df_obs.loc[:, [
        # not sure any of these 3 needed for (/affect) plots, but could be useful at
        # output for comparing to other correlations i load / compute from remy's data.
        'datefly',
        # TODO refactor to share def of these 2 cols?
        'abbrev_row',
        'abbrev_col',

        'odor_pair_str',

        # TODO delete
        #'correlation',

        'correlation_distance',
        # TODO how does this differ from odor_pair_str? del?
        # not referenced anywhere else in this file...
        #'odor_pair',
    ]]

    # only want to make this plot (to show we can recreate preprint figure), when data
    # we are loading is same as in preprint. currently i'm only ever using that data to
    # show we can recreate this plot.
    plot = use_preprint_data

    if plot:
        # TODO fix so i can pass new errorbar into plotting fns, so that i can force
        # that seed_errorbar value for reproducing this plot?
        if seed_errorbar != ('ci', 95):
            warn("set seed_errorbar=('ci', 95) if you want to reproduce preprint 2E. "
                "returning!"
            )
            return

        g = _create_2e_plot_with_obs_kc_corrs(df_obs, pair_order)

        # seems to already have abbrev_[row|col]
        df_mdl = pd.read_csv(preprint_data_folder.joinpath('df_mdl_plot_trialavg.csv'))

        # df_mdl also contains uniform_4 and hemidraw_4
        model_types_to_plot = ['uniform_7', 'hemidraw_7', 'hemimatrix']

        pal = sns.color_palette()
        palette = {
            'hemidraw_7': pal[0],
            'uniform_7': pal[1],
            'hemimatrix': pal[2],
        }

        # TODO refactor? (to also check observed KC corrs in _create_..., and to move
        # this into _2e_plot...?)
        identity = df_mdl.abbrev_col == df_mdl.abbrev_row
        assert (df_mdl[identity].correlation == 1).all()

        df_mdl = df_mdl[~identity].copy()
        assert not (df_mdl.correlation == 1).any()

        df_mdl['correlation_distance'] = 1 - df_mdl.correlation
        assert df_mdl['correlation_distance'].max() <= 2.0
        assert df_mdl['correlation_distance'].min() > 0
        #

        _2e_plot_model_corrs(g, df_mdl.query('model in @model_types_to_plot'),
            pair_order, hue='model', palette=palette
        )

        _finish_remy_2e_plot(g)

        # NOTE: no seed_errorbar part in filename here, as only saving this if it's same
        # as preprints ('ci', 95)
        savefig(g, output_root, '2e_preprint-repro_old_data')


    checks = True
    if checks and not use_preprint_data:
        # TODO refactor w/ place copied from in model_mb...?
        remy_pairs = set(list(zip(df_obs.abbrev_row, df_obs.abbrev_col)))

        # TODO does it actually matter? does df_obs have all the corrs i would compute
        # for old flies anyway? maybe just expand checks below to also check those
        # flies?
        #
        # TODO TODO can i switch things to using corrs from
        # _load_remy_megamat_kc_responses? cause otherwise would prob need to have Remy
        # regen this file, including older megamat data betty now wants us to include...
        #
        # TODO TODO use -c check to verify i 2e outputs not changed by switching this
        # fn? add option to -c to pass substrs of outputs to check (ignoring rest)?
        #
        # TODO update comment. no longer just final 4.
        # data from best 4 "final" flies, which are the only megamat odor correlations
        # used anywhere in the paper except for figure 2E.
        #
        # TODO delete
        #mean_responses = _load_remy_megamat_kc_responses(drop_nonmegamat=False)
        #
        flymean_corrs = _remy_megamat_flymean_kc_corrs(ordered_pairs=remy_pairs,
            drop_nonmegamat=False
        )

        final_megamat_datefly = set(flymean_corrs.index.get_level_values('datefly'))
        # TODO delete (or update to include final 4 + however many old megamat flies i'm
        # now supposed to include)
        assert n_final_megamat_kc_flies <= len(final_megamat_datefly)

        flymean_corrs.columns = pd.MultiIndex.from_frame(
            flymean_corrs.columns.to_frame(index=False).applymap(olf.parse_odor_name)
        )
        assert not flymean_corrs.columns.duplicated().any()
        # TODO delete
        #mean_responses.columns = mean_responses.columns.map(olf.parse_odor_name)
        #assert not mean_responses.columns.duplicated().any()

        # TODO relax to include other pairs? or just drop? i assume we still won't have
        # all the data in df_obs if we just don't drop from latest set of (the old)
        # flies i'm loading?
        #assert set(mean_responses.columns) == megamat_odor_names

        # TODO delete? (replace w/ flymean_corrs)
        #corrs = mean_responses.groupby(level='datefly').apply(
        #    lambda x: corr_triangular(x.corr(), ordered_pairs=remy_pairs)
        #)
        #assert not corrs.isna().any().any()
        #

        # TODO move this dropna into above fn? this even doing anything? why would a
        # column be all NaN (and is that the right interpretation of axis='columns'?)?
        flymean_corrs = flymean_corrs.dropna(how='all', axis='columns')
        corrs = flymean_corrs

        n_megamat_only_pairs = n_choose_2(n_megamat_odors)
        # TODO delete? already relaxed from == to >=
        assert len(corrs.columns) >= n_megamat_only_pairs

        for datefly in corrs.index:
            fly_df = df_obs[df_obs.datefly == datefly]

            remy_2e_csv_ser = fly_df[['abbrev_row', 'abbrev_col',
                'correlation_distance']].set_index(['abbrev_row', 'abbrev_col'])

            # just to convert from shape (n, 1) to (n,)
            remy_2e_csv_ser = remy_2e_csv_ser.iloc[:, 0]

            remy_2e_csv_ser.index.names = ['odor1', 'odor2']

            # convert from correlation distance to correlation (to match what we have in
            # corrs)
            remy_2e_csv_ser = 1 - remy_2e_csv_ser
            remy_2e_csv_ser.name = 'correlation'

            recalced_ser = corrs.loc[datefly]

            # since corrs is of shape (<n_flies>, <n_total_odor_pairs>), this will drop
            # the pairs down to those actually measured in this fly
            recalced_ser = recalced_ser.dropna()
            assert not remy_2e_csv_ser.isna().any()

            recalced_pair_set = set(recalced_ser.index)
            # neither index should have any duplicate pairs
            assert len(recalced_pair_set) == len(recalced_ser)
            assert len(recalced_pair_set) == len(remy_2e_csv_ser)

            assert recalced_pair_set == set(remy_2e_csv_ser.index)

            # above assertion justifies indexing one by the other, as it's just the
            # order that is different, not that either series has any different pairs
            assert pd_allclose(recalced_ser, remy_2e_csv_ser.loc[recalced_ser.index])

        df_megamat = df_obs[
            df_obs.abbrev_row.isin(megamat_odor_names) &
            df_obs.abbrev_col.isin(megamat_odor_names)
        ]

        # TODO (delete? satisfied?) are all of the old megamat flies that i'm now
        # supposed to use a subset of these? do the correlations match what i would
        # compute?
        #
        # ipdb> len(set(df_megamat.datefly) - final_megamat_datefly)
        # 18
        # ipdb> pp (set(df_megamat.datefly) - final_megamat_datefly)
        # {'2018-10-21/1',
        #  '2019-03-06/3',
        #  '2019-03-06/4',
        #  '2019-03-07/2',
        #  '2019-04-26/4',
        #  '2019-05-09/4',
        #  '2019-05-09/5',
        #  '2019-05-23/2',
        #  '2019-05-24/1',
        #  '2019-05-24/3',
        #  '2019-05-24/4',
        #  '2019-07-19/2',
        #  '2019-09-12/1',
        #  '2019-09-12/2',
        #  '2022-09-21/1',
        #  '2022-09-22/2',o
        #  '2022-09-26/1',
        #  '2022-09-26/3'}
        assert final_megamat_datefly - set(df_megamat.datefly) == set()
        df_megamat_nonfinal = df_megamat[
            ~df_megamat.datefly.isin(final_megamat_datefly)
        ]

        # TODO delete (/update) (no longer just using final 4 flies)
        #
        # only the 4 "final" flies have all 17 odors measured (-> all 136 non-identity
        # pairs)
        #
        # ipdb> [len(x) for _, x in df_megamat_nonfinal.groupby('datefly')]
        # [47, 10, 28, 30, 21, 38, 38, 36, 36, 36, 57, 79, 71, 71, 3, 3, 3, 3]
        #assert all(len(x) < n_megamat_only_pairs
        #    for _, x in df_megamat_nonfinal.groupby('datefly')
        #)

        assert remy_2e_metric == 'correlation_distance'
        mean_nonfinal_corrdist = df_megamat_nonfinal.groupby(['abbrev_row','abbrev_col']
            )[remy_2e_metric].mean()

        mean_nonfinal_corrdist.index.names = ['odor1', 'odor2']

        # TODO better check than this try/except
        try:
            square_nonfinal_corrdist = invert_corr_triangular(mean_nonfinal_corrdist,
                diag_value=0, _index=corrs.columns
            )

            square_nonfinal_corrs = 1 - square_nonfinal_corrdist

            # since sorting expects concentrations apparently...
            square_nonfinal_corrs.columns = square_nonfinal_corrs.columns + ' @ -3'
            square_nonfinal_corrs.index = square_nonfinal_corrs.index + ' @ -3'

            square_nonfinal_corrs = sort_odors(util.addlevel(
                    util.addlevel(square_nonfinal_corrs, 'panel', 'megamat').T,
                'panel', 'megamat'
                ), warn=False
            )

            square_nonfinal_corrs = square_nonfinal_corrs.droplevel('panel',
                axis='columns'
            ).droplevel('panel', axis='index')

            plot_corr(square_nonfinal_corrs, output_root,
                '2e_remy_nonfinal-flies-only_corr', xlabel='non-final flies only'
            )

            # TODO actually plot this / delete
            '''
            nonfinal_pair_n = df_megamat_nonfinal.groupby(['abbrev_row','abbrev_col']
                ).size()
            # TODO just rename these cols in dataframe before (so we don't have to do
            # this here and for mean)
            nonfinal_pair_n.index.names = ['odor1', 'odor2']
            nonfinal_pair_n = invert_corr_triangular(nonfinal_pair_n, diag_value=np.nan,
                _index=corrs.columns
            )
            '''
        # ...
        #   File "./al_analysis.py", line 1208, in invert_corr_triangular
        #     assert all(odor2[:-1] == odor1[1:])
        except AssertionError:
            # TODO elaborate on why?
            warn('could not plot 2e square matrix corr plots')
    #

    return df_obs


def main():
    model_output_dir1 = Path('data/sent_to_remy/2025-03-18/'
        'dff_scale-to-avg-max__data_pebbled__hallem-tune_False__pn2kc_hemibrain__'
        'weight-divisor_20__drop-plusgloms_False__target-sp_0.0915'
    ).resolve()

    # panel          megamat   ...
    # odor           2h @ -3   ...   benz @ -3    ms @ -3
    # glomerulus               ...
    # D            40.363954   ...   42.445274  41.550370
    # DA2          15.144943   ...   12.363544   3.856004
    # ...
    # VM7d        108.535394   ...   58.686294  20.230297
    # VM7v         59.896953   ...   13.250292   8.446418
    orn_deltas = pd.read_csv(model_output_dir1 / 'orn_deltas.csv', header=[0,1],
        index_col=0
    )
    assert orn_deltas.columns.names == ['panel', 'odor']
    assert orn_deltas.index.names == ['glomerulus']

    reproduce_input = True
    # TODO TODO TODO restore
    #reproduce_input = False
    kws = dict()
    if reproduce_input:
        kws = dict(
            # I would not normally recommend you hardcode any of these except perhaps
            # weight_divisor=20. The defaults target_sparsity=0.1 and
            # _drop_glom_with_plus=True should be fine.
            target_sparsity=0.0915, weight_divisor=20, _drop_glom_with_plus=False
        )

    plot_root = Path('mb_model_example').resolve()
    # TODO modify this fn so dirname includes all same params by default (rather than
    # just e.g. param_dir='data_pebbled'), as the ones i'm currently manually creating
    # by calls in model_mb_... (prob behaving diff b/c e.g.
    # pn2kc_connections='hemibrain' is explicitly passed there)
    param_dict = fit_and_plot_mb_model(plot_root, orn_deltas=orn_deltas,
        try_cache=False, **kws
    )

    output_dir = (plot_root / param_dict['output_dir']).resolve()
    assert output_dir.is_dir()
    assert output_dir.parent == plot_root

    #           2h @ -3  IaA @ -3  pa @ -3  ...  1-6ol @ -3  benz @ -3  ms @ -3
    # model_kc                              ...
    # 0             0.0       0.0      0.0  ...         0.0        0.0      0.0
    # 1             0.0       0.0      0.0  ...         0.0        0.0      0.0
    # ...           ...       ...      ...  ...         ...        ...      ...
    # 1835          1.0       1.0      2.0  ...         1.0        0.0      0.0
    # 1836          0.0       0.0      0.0  ...         0.0        0.0      0.0
    df = pd.read_csv(output_dir / 'spike_counts.csv', index_col='model_kc')

    if reproduce_input:
        idf = pd.read_csv(model_output_dir1 / 'spike_counts.csv', index_col='model_kc')
        assert idf.equals(df)


    # TODO TODO also include an example w/ kiwi/control data (as used by natmix_data)
    # (either committing data in al_analysis too, or moving this whole example to
    # another repo)
    import ipdb; ipdb.set_trace()


if __name__ == '__main__':
    main()

