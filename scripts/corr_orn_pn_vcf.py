#!/usr/bin/env python3

import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import permutation_test, pearsonr

from hong2p.olf import parse_odor_name
from hong2p.util import pd_allclose, shorten_path, addlevel
from hong2p.viz import matshow, dff_latex
import al_util
from al_util import (mean_of_fly_corrs, corr_triangular, invert_corr_triangular, warn,
    plot_corr, plot_responses, data_root, sort_odors, savefig, diverging_cmap_kwargs
)
from al_analysis import fill_to_hemibrain


megamat_abbrev2inchi = {
    '1-5ol': 'InChI=1S/C5H12O/c1-2-3-4-5-6/h6H,2-5H2,1H3',
    '1-6ol': 'InChI=1S/C6H14O/c1-2-3-4-5-6-7/h7H,2-6H2,1H3',
    '1-8ol': 'InChI=1S/C8H18O/c1-2-3-4-5-6-7-8-9/h9H,2-8H2,1H3',
    '2-but': 'InChI=1S/C4H8O/c1-3-4(2)5/h3H2,1-2H3',
    '2h': 'InChI=1S/C7H14O/c1-3-4-5-6-7(2)8/h3-6H2,1-2H3',
    '6al': 'InChI=1S/C6H12O/c1-2-3-4-5-6-7/h6H,2-5H2,1H3',
    'B-cit': 'InChI=1S/C10H20O/c1-9(2)5-4-6-10(3)7-8-11/h5,10-11H,4,6-8H2,1-3H3',
    'IaA': 'InChI=1S/C7H14O2/c1-6(2)4-5-9-7(3)8/h6H,4-5H2,1-3H3',
    'Lin': 'InChI=1S/C10H18O/c1-5-10(4,11)8-6-7-9(2)3/h5,7,11H,1,6,8H2,2-4H3',
    'aa': 'InChI=1S/C2H4O2/c1-2(3)4/h1H3,(H,3,4)',
    'benz': 'InChI=1S/C7H6O/c8-6-7-4-2-1-3-5-7/h1-6H',
    'eb': 'InChI=1S/C6H12O2/c1-3-5-6(7)8-4-2/h3-5H2,1-2H3',
    'ep': 'InChI=1S/C5H10O2/c1-3-5(6)7-4-2/h3-4H2,1-2H3',
    'ms': 'InChI=1S/C8H8O3/c1-11-8(10)6-4-2-3-5-7(6)9/h2-5,9H,1H3',
    'pa': 'InChI=1S/C7H14O2/c1-3-4-5-6-9-7(2)8/h3-6H2,1-2H3',
    't2h': 'InChI=1S/C6H10O/c1-2-3-4-5-6-7/h4-6H,2-3H2,1H3/b5-4+',
    'va': 'InChI=1S/C5H10O2/c1-2-3-4-5(6)7/h2-4H2,1H3,(H,6,7)',
}
assert len(set(megamat_abbrev2inchi.keys())) == len(set(megamat_abbrev2inchi.values()))
inchi2abbrev = {v: k for k, v in megamat_abbrev2inchi.items()}
megamat_inchi = set(megamat_abbrev2inchi.values())
# TODO also define order from ORN/PN data i'm loading, which already seem to have odors
# in the KC-cluster-order I want (that of most figs in paper)

fly_cols = ['date', 'fly_num']

def parse_name(odor_str):
    # by default parse_odor_name will raise ValueError if conc_delim (' @ ') missing
    return parse_odor_name(odor_str, require_conc=False)


def compute_corr(df):
    if all(x in df.columns.names for x in fly_cols):
        # TODO check computing mean first (across trials) produces same output of
        # mean_of_fly_corrs (move to unit test?)
        corr = mean_of_fly_corrs(df)
    else:
        assert not any(x in df.columns.names for x in fly_cols)
        # mainly for bouton-weighted-[ORN|PN]-mean input
        corr = df.T.corr()

    assert corr.index.equals(corr.columns)

    abbrevs_without_conc = corr.index.map(parse_name)
    assert not abbrevs_without_conc.duplicated().any()
    assert not abbrevs_without_conc.isna().any()

    corr.index = abbrevs_without_conc
    corr.columns = abbrevs_without_conc

    return corr


def gloms(df):
    return set(df.columns.get_level_values('roi').unique())


def plot_orn_vs_pn(mean_orn, mean_pn, plot_dir, *, title_suffix=''):
    mean_pn_like_orn = mean_pn.reindex(columns=mean_orn.columns)
    assert mean_pn_like_orn.loc[:, mean_pn.columns].equals(mean_pn)
    assert mean_pn_like_orn.dropna(how='all', axis='columns').equals(mean_pn)

    # TODO also try scaling each to match (at least, if not already close
    # enough), then plotting difference (eh, happy w/ current side-by-side plots)?

    orn_vs_pn_by_glom = pd.concat([mean_orn, mean_pn_like_orn], keys=['orn','pn'],
        axis='columns'
    )
    assert orn_vs_pn_by_glom.index.equals(mean_orn.index)
    assert (len(orn_vs_pn_by_glom.columns) ==
        len(mean_orn.columns) + len(mean_pn_like_orn.columns)
    )
    # otherwise names will be [None, 'roi'] after concat. not sure if easily possible to
    # use levels= kwarg to concat to do in one step.
    orn_vs_pn_by_glom.columns.names = ['space', 'roi']
    orn_vs_pn_by_glom = orn_vs_pn_by_glom.sort_index(axis='columns', level='roi')

    # TODO share this cbar_label w/ elsewhere below
    kws = dict(levels_from_labels=False, linecolor='k', cbar_label=f'mean {dff_latex}',
        **diverging_cmap_kwargs
    )

    fig, ax = plt.subplots()
    matshow(orn_vs_pn_by_glom.T, ax=ax, hline_level_fn=lambda x: x['roi'],
        # TODO why yticklabels=False not working? (modify matshow to also have it hide
        # autogenerated numbers, via set_[y|x]ticklabels([]), at least when also showing
        # group text?)
        #
        # default hgroup_label_offset=0.12
        hline_group_text=True, hgroup_label_offset=0.05, yticklabels=False,
        title=f'ORN (top) vs PN (bottom) (for each glomerulus){title_suffix}', **kws
    )
    ax.set_yticklabels([])
    ax.set_yticks([])
    savefig(fig, plot_dir, 'orn_vs_pn_by_glom', bbox_inches='tight')

    orn_vs_pn_by_odor = pd.concat([mean_orn, mean_pn_like_orn], keys=['orn','pn'])
    assert orn_vs_pn_by_odor.columns.equals(mean_orn.columns)
    assert len(orn_vs_pn_by_odor) == len(mean_orn) + len(mean_pn_like_orn)
    orn_vs_pn_by_odor.index.names = ['space', 'odor1']
    # sorting by odor (using same KC-cluster-order sort_odors will use for megamat data)
    # to interleave spaces.
    orn_vs_pn_by_odor = sort_odors(orn_vs_pn_by_odor, add_panel='megamat'
        ).droplevel('panel')
    assert orn_vs_pn_by_odor.loc['orn'].equals(mean_orn)
    assert orn_vs_pn_by_odor.loc['pn'].equals(mean_pn_like_orn)

    fig, ax = plt.subplots()
    matshow(orn_vs_pn_by_odor.T, ax=ax, vline_level_fn=lambda x: x['odor1'],
        # default vgroup_label_offset=0.08
        vline_group_text=True, vgroup_label_rotation=90, vgroup_label_offset=0.02,
        # TODO why xticklabels=False/None not working?
        # TODO maybe it never did hide the sparse int labels, but maybe i should make it
        # do so in viz.matshow (at least, if showing group labels too)??
        xticklabels=False,
        title=f'ORN (left) vs PN (right) (for each odor){title_suffix}', **kws
    )
    ax.set_xticklabels([])
    ax.set_xticks([])
    savefig(fig, plot_dir, 'orn_vs_pn_by_odor', bbox_inches='tight')


def test_pearson_of_pearsons_are_equal(a: pd.DataFrame, b: pd.DataFrame,
    a_name: str, b_name: str, plot_dir: Path):
    # TODO TODO ever want to pass differences wrt vcf corr ser as input? or need to pass
    # that separately, so we can compute diff for each resample (which each has their
    # own mean corr ser)?

    assert a_name != b_name

    sample_fly_corrs = []
    def _add_sample_fly_corrs(df: pd.DataFrame, label: str):
        # TODO assert we aren't averaging over any other variations in index metadata?
        #
        # should just be averaging over 'repeat' and maybe some levels that don't
        # vary in index metadata (i.e. is_pair=False).
        #
        # only have sort=False to compare to mean_of_fly_corrs later (which doesn't
        # change odor order). NOTE: need level= prefix to have sort=False work.
        trialmean_df = df.groupby(level='odor1', sort=False).mean()

        trialmean_df.index = trialmean_df.index.map(parse_name)

        corr_ser_list = []
        #for fly, fly_df in trialmean_df.groupby(fly_cols, axis='columns'):
        # TODO delete
        for i, (fly, fly_df) in enumerate(trialmean_df.groupby(fly_cols,
            axis='columns')):
        #
            if 'TEST' in label and i == 5:
                break

            # TODO need to select certain ROIs first on my data? probably already done?
            # (assert all certain)

            # TODO check all indices same now that we aren't sorting above?
            ser = corr_triangular(fly_df.T.corr())
            # TODO TODO preserve (/add back) fly metadata in row index (which currently
            # will just have label by end of loop)?
            ser = addlevel(ser, 'label', label)
            # will be a dataframe with one row and #-odor-pair columns after unstack().T
            corr_ser_list.append(ser.unstack(level=0).T)

        # can't verify_integrity=True unless we add/keep fly metadata in row indices
        fly_corrs = pd.concat(corr_ser_list)

        # TODO put behind checks=True?
        # TODO TODO TODO restore after deleting hack above subsetting in test case
        '''
        corr1 = invert_corr_triangular(fly_corrs.mean())
        corr2 = mean_of_fly_corrs(df)
        abbrev_order = corr2.index.map(parse_name)
        assert np.allclose(corr1.loc[abbrev_order, abbrev_order].values, corr2.values)
        '''

        sample_fly_corrs.append(fly_corrs)


    # TODO compute sample sizes of each from number of list elements appended so
    # far? have this fn return list to pass to .extend?
    _add_sample_fly_corrs(a, a_name)
    _add_sample_fly_corrs(b, b_name)

    assert len(sample_fly_corrs) == 2
    m1 = sample_fly_corrs[0].mean()
    m2 = sample_fly_corrs[1].mean()
    # corrcoef (with this input) gives a 2x2 matrix, w/ diagonal 1 and x[0,1] == x[1,0]
    assert np.isclose(pearsonr(m1, m2).statistic, np.corrcoef(m1, m2)[0, 1])

    def pearson_of_mean_pearsons(x, y):
        # TODO assert we are averaging over a # that could be # of flies instead of # of
        # odors? (i.e. that len is like that)
        mx = x.mean()
        my = y.mean()
        return np.corrcoef(mx, my)[0,1]

    #permutation_test(sample_fly_corrs, pearson_of_mean_pearsons, random_state=0)

    # TODO update scipy from 1.10.1 to 1.16.0 for rng= kwarg to permutation test?
    # (requires python 3.11...) what's min version that supports it?
    # may have just been called random_state before?
    # TODO any other way to seed? context manager? global setting of np seed?

    # TODO compare pearsonr to np.corrcoef (and then use latter)

    # TODO TODO TODO do same for remy's PN bouton data, and statistical test
    # that correlation matrices are different

    import ipdb; ipdb.set_trace()


def main():
    # TODO refactor to share w/ al_analysis.py (or at least natmix_data/analysis.py)?
    # both use this machinery (tho each adds their own additional args)
    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--check-outputs-unchanged', action='store_true',
        help='For CSVs/pickles/plots (saved with my to_[csv|pickle]/savefig wrappers), '
        'check new outputs against any existing outputs they would overwrite. If there '
        'is a descrepancy, exit with an error. Currently do not support certain plot '
        'formats (where metadata includes things like file creation time, so same '
        'strategy can not be used to check files for equality).'
    )
    args = parser.parse_args()
    al_util.check_outputs_unchanged = args.check_outputs_unchanged
    al_util.verbose = True
    #

    script_dir = Path(__file__).resolve().parent
    final_data_dir = data_root / 'sent_to_remy' / '2023-10-29'

    # there is also a CSV alongside that each of these pickles that they could be
    # checked against, but i'm confident their contents match
    orn = pd.read_pickle(final_data_dir / 'pebbled_ij_certain-roi_stats.p')
    pn = pd.read_pickle(final_data_dir / 'GH146_ij_certain-roi_stats.p')

    # TODO were non-"consensus" glomeruli already dropped? not that it would change any
    # choices here...
    pn_gloms_allpanel = gloms(pn)

    # since there is also panel='glomeruli_diagnostics' data for both
    orn = orn.loc['megamat']
    pn = pn.loc['megamat']

    pn_gloms = gloms(pn)
    assert pn_gloms_allpanel == pn_gloms

    orn_gloms = gloms(orn)
    assert pn_gloms - orn_gloms == set()

    # ipdb> orn_gloms - pn_gloms
    # {'DL2v', 'VC3', 'VM5v', 'VC4', 'DC4', 'DP1l', 'VM5d', 'VL1', 'DL2d'}
    # ipdb> len(orn_gloms - pn_gloms)
    # 9
    print(f'{orn_gloms - pn_gloms=}')
    print(f'{len(orn_gloms - pn_gloms)=}')
    assert len(orn_gloms - pn_gloms) > 0

    # TODO restore? maybe just after exploring possibility of recomputing trial response
    # statistic on new GH146 data
    '''
    preprint_data_dir = data_root / 'sent_to_remy' / '2023-01-06'
    # TODO refactor any preprocessing of this to share w/ processing of `pn`?
    old_pn = pd.read_pickle(preprint_data_dir / 'GH146_ij_roi_stats.p')
    old_pn = old_pn.loc['megamat']
    # TODO would need to drop the uncertain ones first (did remy have something with
    # them dropped? did she also drop things w/ '?' suffices?)
    old_pn_gloms = gloms(old_pn)
    import ipdb; ipdb.set_trace()
    '''
    #

    gh146_gloms = pn_gloms
    del pn_gloms

    ocorr = compute_corr(orn)

    # ocorr.index should already be in KC cluster order used in most of paper (2h, IaA,
    # ... , ms).  will use to put VCF stuff in that order too.
    #
    # now we are stripping conc suffixes above, where ocorr/pcorr defined
    abbrev_order = ocorr.index.copy()
    assert not abbrev_order.duplicated().any()
    assert not abbrev_order.isna().any()

    pcorr = compute_corr(pn)
    assert ocorr.index.equals(pcorr.index)

    assert not pd_allclose(ocorr, pcorr)
    # just to further check they are meaningfully different, and not just in a small
    # numerical way (LHS mean-abs diff is 0.13837)
    assert (pcorr - ocorr).abs().mean().mean() > 0.1

    orn_gh146_subset = orn.loc[:, orn.columns.get_level_values('roi').isin(gh146_gloms)]
    gcorr = compute_corr(orn_gh146_subset)
    assert ocorr.index.equals(gcorr.index)
    assert not pd_allclose(ocorr, gcorr)
    # ipdb> (gcorr - ocorr).abs().mean().mean()
    # 0.047482307854904475
    assert (gcorr - ocorr).abs().mean().mean() > 0.04

    plot_dir = script_dir

    # TODO fix how i don't get usual output-changed stuff when there's a line like:
    # Warning: Image sizes do not match expected size: (354, 401, 3) actual size (359, 412, 3)
    # (contents did not actually seem different)
    #
    # rest of output:
    # Traceback (most recent call last):
    #   File "./corr_orn_pn_vcf.py", line 364, in <module>
    #     main()
    #   File "./corr_orn_pn_vcf.py", line 147, in main
    #     plot_corr(ocorr, plot_dir, 'orn_corr', title='ORN')
    #   File "/home/tom/src/al_analysis/al_util.py", line 1558, in plot_corr
    #     savefig(fig, plot_dir, prefix, bbox_inches='tight', debug=verbose, **_save_kws)
    #   File "/home/tom/src/al_analysis/al_util.py", line 1068, in savefig
    #     overwrite = _output_change_prompt_or_err(err, fig_path)
    #   File "/home/tom/src/al_analysis/al_util.py", line 293, in _output_change_prompt_or_err
    #     raise err
    #   File "/home/tom/src/al_analysis/al_util.py", line 1064, in savefig
    #     _check_output_would_not_change(fig_path, save_fn, **kwargs)
    #   File "/home/tom/src/al_analysis/al_util.py", line 711, in _check_output_would_not_change
    #     raise RuntimeError(err_msg)
    # RuntimeError: /home/tom/src/al_analysis/scripts/orn_corr.pdf (Jul 07 16:28:02 2025) would have changed! (run without -c/-C to ignore)
    #
    # compare with what would have been new output, saved at: /tmp/tmp0gdo4ose.pdf
    plot_corr(ocorr, plot_dir, 'orn_corr', title='ORN')

    # TODO any NaN/inf in here? no. why getting errors when showing? note that these
    # errors do not halt execution
    assert not pcorr.isna().any().any()
    assert np.isfinite(pcorr).all().all()
    # ...
    # Traceback (most recent call last):
    #   File "/home/tom/src/al_analysis/venv/lib/python3.8/site-packages/matplotlib/cbook/__init__.py", line 314, in process
    #     func(*args, **kwargs)
    #   File "/home/tom/src/al_analysis/venv/lib/python3.8/site-packages/matplotlib/backend_bases.py", line 3150, in mouse_move
    #     self.set_message(self._mouse_event_to_message(event))
    #   File "/home/tom/src/al_analysis/venv/lib/python3.8/site-packages/matplotlib/backend_bases.py", line 3142, in _mouse_event_to_message
    #     data_str = a.format_cursor_data(data).rstrip()
    #   File "/home/tom/src/al_analysis/venv/lib/python3.8/site-packages/matplotlib/artist.py", line 1362, in format_cursor_data
    #     g_sig_digits = cbook._g_sig_digits(data, delta)
    #   File "/home/tom/src/al_analysis/venv/lib/python3.8/site-packages/matplotlib/cbook/__init__.py", line 2218, in _g_sig_digits
    #     - math.floor(math.log10(delta))) if math.isfinite(value) else 0
    # OverflowError: cannot convert float infinity to integer
    pcorr_out = plot_corr(pcorr, plot_dir, 'pn_corr', title='final PN (2023-10-29)')
    # just to check plot_corr isn't (incorrectly, at least) recomputing correlation
    # itself
    assert pcorr_out.equals(pcorr)
    del pcorr_out

    plot_corr(gcorr, plot_dir, 'orn_gh146-subset_corr', title='ORN, GH146-subset')


    def mean_responses(df):
        # will need to re-order odors after groupby->mean.
        # sort=False still screws it up.
        mean_df = df.groupby('odor1').mean().groupby('roi', axis='columns').mean()

        # this was just to get comparable to abbrev_order again (which does not have
        # ' @ -3' suffix)
        mean_df.index = mean_df.index.map(parse_name)

        # TODO just use order of odors in input, and then move this line out (-> move
        # this fn to top-level)? (would then still need to do this separately for 1 or a
        # couple cases...)
        return mean_df.loc[abbrev_order]


    mean_orn = mean_responses(orn)

    mean_pn = mean_responses(pn)
    # ipdb> mean_pn.max().max()
    # 1.9617704376569685
    # ipdb> mean_orn.max().max()
    # 1.628468070216096
    #
    # ipdb> mean_orn.min().min()
    # -0.1810252689404889
    # ipdb> mean_pn.min().min()
    # -0.1577443928503241

    # these cbar limits should match what i use in paper for these (the supplemental
    # figs)
    supp_response_kws = dict(vmin=-0.35, vmax=2.5)

    mean_orn_hemibrain = fill_to_hemibrain(mean_orn)
    plot_responses(mean_orn_hemibrain.T, plot_dir, 'orn_hemibrain-filled',
        **supp_response_kws
    )
    mean_pn_hemibrain = fill_to_hemibrain(mean_pn)
    plot_responses(mean_pn_hemibrain.T, plot_dir, 'pn_hemibrain-filled',
        **supp_response_kws
    )

    plot_orn_vs_pn(mean_orn, mean_pn, plot_dir)

    # TODO TODO try repro-ing old PN response stat data (maybe to prove i did use max
    # there, and maybe that's why i things looked diff)? (would have to uncheck exclude
    # box on the relevant experiments in my Google Sheet metadata)
    #
    # TODO TODO plot comparing responses of old and new PN responses, similar to plots
    # above comparing ORN vs PN data
    # TODO TODO + compare it's correlation against emb vcf to new PN vs emb vcf
    #import ipdb; ipdb.set_trace()

    vcf_emb_dir = data_root / 'from_remy' / '2025-07-07'
    vcf_emb_path = (vcf_emb_dir /
        'VCFQuantHMDS_N0452_S2190_StandardScaler_pearson_MP05__ND030__lsf__shepard.tsv'
    )

    # has columns 'row_inchi', 'col_inchi', and then 10 seeds for each embedding of
    # dimensions on [2, 10]
    vcf = pd.read_csv(vcf_emb_path, sep='\t')

    emb_col = 'VCFQuantHMDS_N0452_S2190_StandardScaler_pearson_MP05__ND030_D03_seed08'
    # TODO also look at 'data' raw data distances? (may need to use to mask missing
    # pairs? does remy? she does both ways and it doesn't really matter which. it's only
    # 8 pairs.)
    # TODO check how much weird corr w/ PN data changes across seeds?
    vcf = vcf[['row_inchi', 'col_inchi', emb_col, 'data']]

    # otherwise includes rows for all odor pairs for 400-some (452?) odors.
    # this gives us 136 rows (17 choose 2)
    vcf = vcf[vcf.row_inchi.isin(megamat_inchi) & vcf.col_inchi.isin(megamat_inchi)]

    vcf['row_abbrev'] = vcf.row_inchi.map(inchi2abbrev)
    vcf['col_abbrev'] = vcf.col_inchi.map(inchi2abbrev)

    vser_raw_corr_dist = vcf[['row_abbrev', 'col_abbrev', 'data']].set_index(
        ['row_abbrev', 'col_abbrev']).squeeze()

    vser_corr_dist = vcf[['row_abbrev', 'col_abbrev', emb_col]].set_index(
        ['row_abbrev', 'col_abbrev']).squeeze()

    # to be consistent w/ output of corr_triangular
    vser_raw_corr_dist.index.names = ['odor1', 'odor2']
    vser_corr_dist.index.names = ['odor1', 'odor2']

    # converting from correlation distance [0, 2] to correlation [-1, 1]
    vser = 1 - vser_corr_dist
    vser_raw = 1 - vser_raw_corr_dist

    def square_corr(corr_ser):
        corr = invert_corr_triangular(corr_ser)
        assert not corr.index.duplicated().any()
        assert corr.index.equals(corr.columns)
        assert set(corr.index) == set(abbrev_order)
        return corr.loc[abbrev_order, abbrev_order]


    def check_corr_ser(ser, allow_nan=False):
        # corr series shouldn't have identity values, so nothing should be 1
        assert 0 < ser.max() < 1
        # all input data here should have some pairs somewhat anti-correlated
        assert -1 < ser.min() < 0

        # this true always? for everything but vser_raw, yes.
        # for vser_raw, there are 8/136 pairs null there.
        if not allow_nan:
            assert not ser.isna().any()


    check_corr_ser(vser_raw, allow_nan=True)

    check_corr_ser(vser)
    vcorr = square_corr(vser)

    # should just be changing how pairs are ordered/represented in index, not any of the
    # values/shape
    vser = corr_triangular(vcorr)
    assert square_corr(vser).equals(vcorr)
    check_corr_ser(vser)
    vser.name = 'vcf_emb'

    # TODO delete? don't really plan to use this to mask vser, since remy said it didn't
    # change much, and it is only 8 pairs (which pairs tho? matter for PN data?)
    #
    # TODO these pairs matter for vcf-emb vs pn-dendrite?
    # ipdb> vser_raw.index[vser_raw.isna()]
    # MultiIndex([('2-but', 'B-cit'),
    #             ('2-but',    'pa'),
    #             (   '2h', 'B-cit'),
    #             (   '2h',    'pa'),
    #             ('B-cit',    'pa'),
    #             ('B-cit',    'va'),
    #             (   'ms',    'pa'),
    #             (   'pa',    'va')],
    #            names=['odor1', 'odor2'])
    # #
    # should just be changing how pairs are ordered/represented in index, not any of the
    # values/shape
    vser_raw = corr_triangular(square_corr(vser_raw))
    check_corr_ser(vser_raw, allow_nan=True)
    #

    oser = corr_triangular(ocorr)
    assert square_corr(oser).equals(ocorr)
    check_corr_ser(oser)
    oser.name = 'orn'

    gser = corr_triangular(gcorr)
    assert square_corr(gser).equals(gcorr)
    check_corr_ser(gser)
    gser.name = 'orn_gh146_subset'

    pser = corr_triangular(pcorr)
    assert square_corr(pser).equals(pcorr)
    check_corr_ser(pser)
    pser.name = 'pn_dendrite'

    # TODO can we justify rectifying? if i switch response stat from mean to max, should
    # also rectify in many cases? in many cases it doesn't tho!!!
    pn_rect = pn.copy()
    pn_rect[pn_rect < 0] = 0

    # to sanity check
    plot_responses(mean_responses(pn_rect).T, plot_dir, 'pn-rect',
        title='PN, rectified', **supp_response_kws
    )

    rcorr = compute_corr(pn_rect)
    plot_corr(rcorr, plot_dir, 'pn-rect_corr', title='PN, rectified')

    rser = corr_triangular(rcorr)
    rser.name = 'pn_dendrite_rect'

    plot_corr(vcorr, plot_dir, 'vcf-emb_corr')

    repro_remy_vcf_emb_lab_meeting_corr = False
    if repro_remy_vcf_emb_lab_meeting_corr:
        vcorr_dist = invert_corr_triangular(vser_corr_dist, diag_value=0
            ).loc[abbrev_order, abbrev_order]

        fig, ax = plt.subplots()
        # should repro remy's plot from 2025-07-07 lab meeting. note her plot had cbar
        # that went up to only ~1.3, but data seems to go up to  ~1.6 here... (am i
        # computing wrong?)
        #
        # this one matches hers visually more.
        matshow(vcorr_dist, ax=ax, cmap='Reds_r')
        ax.set_xlabel(f'no explicit vmax (input max: {vcorr_dist.max().max():.2f})')

        fig, ax = plt.subplots()
        matshow(vcorr_dist, ax=ax, cmap='Reds_r', vmin=0, vmax=1.3)
        ax.set_xlabel('with vmax=1.3')

        plt.show()

    assert pser.index.equals(oser.index)
    assert vser.index.equals(oser.index)

    def fmt_float(x):
        return f'{x:.3f}'

    def compute_corr_vs_vcf_emb(corr_ser_list):
        assert all(x.index.equals(corr_ser_list[0].index) for x in corr_ser_list)

        # NOTE: verify_integrity=True w/ axis='columns' just seems to be checking for
        # duplicates in the row indices, which should all be the same anyway, rather
        # than the Series.name(s) we are using the build this new column index...
        corrs = pd.concat(corr_ser_list + [vser], axis='columns', verify_integrity=True)
        assert not corrs.columns.duplicated().any()

        corrs_vs_vcf = corrs.corr().loc['vcf_emb'].drop('vcf_emb')

        # TODO TODO save a plot for these too?
        print('Pearson correlations vs embedded VCF:')
        print(corrs_vs_vcf.to_string(float_format=fmt_float))
        print()

        return corrs_vs_vcf

    # TODO TODO get model PN internals too (maybe also compute via olsen equation
    # if easier?) (-> also correlate them w/ vcf-emb)
    # (or recompute olsen ones. could use drosolf for that?)

    # TODO TODO commit this data
    bouton_freq_path = (data_root / 'sent_to_remy' / '2025-07-03' /
        'hemibrain_wd20_boutons-per-glomerulus.csv'
    )
    # important that bouton_freq is subset down to only glomeruli we have in mean_orn,
    # or else `n=bouton_freq.sum()` below will be too high. without subsetting to
    # mean_orn.columns, bouton_freq.index would have all hemibrain glomeruli, including
    # some we have so far not been able to identify.
    bouton_freq = pd.read_csv(bouton_freq_path, index_col=0).squeeze()[mean_orn.columns]
    # TODO change to `== 'roi'`? still need 'glomerulus' sometimes?
    assert bouton_freq.index.name in ('glomerulus', 'roi')
    assert bouton_freq.name == 'n_est_boutons_hemibrain_wd20'

    assert not mean_orn.columns.duplicated().any()
    # TODO TODO probably do for PN data, rather than ORN (or both)?
    # TODO share seed (random_state) with anything else?
    bouton_resampled_orn = mean_orn.sample(axis='columns', replace=True, random_state=1,
        # TODO TODO TODO diff N than this sum? something smaller?
        # TODO TODO try another to check it's not sensitive at least?
        n=bouton_freq.sum(), weights=bouton_freq
    )
    # TODO duped with anything else?
    assert bouton_resampled_orn.columns.name == 'roi'
    assert bouton_resampled_orn.index.name == 'odor1'
    #

    # index (odor) metadata stays the same (length 17 [= # megamat odors]). just
    # duplicating columns to expand glomeruli to match "bouton" frequency.
    assert bouton_resampled_orn.index.equals(mean_orn.index)
    assert bouton_resampled_orn.index.name == mean_orn.index.name == 'odor1'
    assert bouton_resampled_orn.columns.duplicated().any()

    bcorr = compute_corr(bouton_resampled_orn)
    plot_corr(bcorr, plot_dir, 'orn-bouton-resampled_corr',
        # TODO generate part of str regarding source of bouton data
        title='ORN, bouton resampled\n(to hemibrain wd20 est freqs)'
    )
    bser = corr_triangular(bcorr)
    bser.name = f'{oser.name}_bouton-resampled'


    bouton_resampled_orn = mean_orn.sample(axis='columns', replace=True, random_state=1,
        n=1000, weights=bouton_freq
    )
    bcorr1k = compute_corr(bouton_resampled_orn)
    plot_corr(bcorr1k, plot_dir, 'orn-bouton-resampled-n1000_corr',
        # TODO generate part of str regarding source of bouton data
        title='ORN, bouton resampled (N=1000)\n(to hemibrain wd20 est freqs)'
    )
    bser1k = corr_triangular(bcorr1k)
    bser1k.name = f'{oser.name}_bouton-resampled-n1000'


    bouton_resampled_pn = mean_pn.sample(axis='columns', replace=True, random_state=1,
        n=1000, weights=bouton_freq
    )
    bcorr1k = compute_corr(bouton_resampled_pn)
    plot_corr(bcorr1k, plot_dir, 'pn-bouton-resampled-n1000_corr',
        # TODO generate part of str regarding source of bouton data
        title='PN, bouton resampled (N=1000)\n(to hemibrain wd20 est freqs)'
    )
    fser1k = corr_triangular(bcorr1k)
    fser1k.name = f'{pser.name}_bouton-resampled-n1000'


    # TODO TODO save plot of bouton_freq
    # TODO at least print bouton_freq.sum()
    print(f'{bouton_freq.sum()=}')

    # TODO + plot freq computed from resampled-ORN cols, to sanity check it's working?
    #import ipdb; ipdb.set_trace()


    # TODO refactor bouton resampling to share across orn/pn inputs
    assert not mean_pn.columns.duplicated().any()
    bouton_resampled_pn = mean_pn.sample(axis='columns', replace=True, random_state=1,
        n=bouton_freq.sum(), weights=bouton_freq
    )
    assert bouton_resampled_pn.columns.name == 'roi'
    assert bouton_resampled_pn.index.name == 'odor1'

    # index (odor) metadata stays the same (length 17 [= # megamat odors]). just
    # duplicating columns to expand glomeruli to match "bouton" frequency.
    assert bouton_resampled_pn.index.equals(mean_pn.index)
    assert bouton_resampled_pn.index.name == mean_pn.index.name == 'odor1'
    assert bouton_resampled_pn.columns.duplicated().any()

    bcorr2 = compute_corr(bouton_resampled_pn)
    plot_corr(bcorr2, plot_dir, 'pn-bouton-resampled_corr',
        # TODO generate part of str regarding source of bouton data
        title='PN, bouton resampled\n(to hemibrain wd20 est freqs)'
    )
    # TODO TODO TODO how does this make corr w/ vcf higher, but lower for orn input?
    fser = corr_triangular(bcorr2)
    fser.name = f'{pser.name}_bouton-resampled'


    # orn/pn_dendrite values match what remy had in her slides (she sent a screenshot of
    # the relevant slide on 2025-07-07 at 3:48pm)
    #
    # Pearson correlations vs embedded VCF:
    # orn                0.105
    # orn_gh146_subset   0.090
    # pn_dendrite        0.446
    # pn_dendrite_rect   0.459
    corrs_vs_vcf = compute_corr_vs_vcf_emb(
        [oser, bser, bser1k, gser, pser, fser, fser1k, rser]
    )

    # her plotted values (only shown to 2 figs after decimal) are second args here.
    # using this atol given how much precision she plotted.
    assert np.isclose(corrs_vs_vcf.loc['orn'], 0.1, atol=0.005)
    assert np.isclose(corrs_vs_vcf.loc['pn_dendrite'], 0.45, atol=0.005)

    # TODO what is range of pearsons (for PN vs embedded VCF, across seeds)?
    # remy looking into? yes, she showed me, and i don't think they varied enough. min
    # might have been ~0.3?

    # TODO TODO TODO commit contents of these dirs
    pn_response_options_dir = data_root / 'internal' / 'GH146_6f'
    orn_response_options_dir = data_root / 'internal' / 'pebbled_6f'

    response_option_dirs = (
        sorted(pn_response_options_dir.glob('*/')) +
        sorted(orn_response_options_dir.glob('*/'))
    )

    # only checked for pn outputs. see comment about it below.
    expected_matching_response_dirname = 'n-volumes-for-response_2__stat_mean'

    corr_ser_list = []
    for response_stat_option_dir in response_option_dirs:
        print()
        print(shorten_path(response_stat_option_dir, n=4))
        data_path = response_stat_option_dir / 'ij_certain-roi_stats.p'
        if not data_path.is_file():
            warn(f'{data_path} did not exist! skipping this directory.')
            continue

        df = pd.read_pickle(data_path)

        # compare on everything (including diagnostics, rather than dropping them)?
        df = df.loc['megamat']

        driver_indicator_str = response_stat_option_dir.parent.name
        assert driver_indicator_str in ('GH146_6f', 'pebbled_6f')

        # TODO just rename this to title and delete separate def of that var below?
        title_suffix = f'\n{driver_indicator_str}\n{response_stat_option_dir.name}'

        # similar assertion for one of the orn inputs? couldn't do exactly as easily,
        # b/c my current outputs will always mismatch for just a few glomeruli on one
        # fly. see scripts/check_curr_csvs_match_paper.py for summary of those minor
        # differences.
        if (driver_indicator_str == 'GH146_6f' and
            response_stat_option_dir.name == expected_matching_response_dirname):

            assert pn.droplevel('odor2').equals(df)
            print(f'contents exactly matched PN data sent to Remy ({final_data_dir})!')
            title_suffix += '\n(what Remy has been using)'

        title = title_suffix

        trialmean = df.groupby('odor1').mean()
        trialmean.index = trialmean.index.map(parse_name)
        trialmean = trialmean.loc[abbrev_order]

        mean_df = trialmean.groupby('roi', axis='columns').mean()
        # TODO delete
        mean_df2 = mean_responses(df)
        assert mean_df.equals(mean_df2)
        #

        mean_ser = mean_df.stack()
        quantiles = mean_ser.quantile(q=[0, 0.02, 0.05, 0.25, 0.75, 0.95, 0.98, 1])
        quantiles.name = 'value'
        quantiles = quantiles.to_frame()
        quantiles.index.name = 'quantile'
        print()
        print(quantiles.T.to_string(float_format=fmt_float))

        thresh = 1.0
        gt_thresh = mean_ser > thresh
        frac_gt_thresh = gt_thresh.sum() / mean_ser.size
        print()
        print(f'frac >{thresh:.1f}: {frac_gt_thresh:.3f}')
        # NOTE: input to df.mask will null values where condition is True,
        # hence all the inverting
        print(f'mean response >threshold: {mean_ser.mask(~gt_thresh).mean():.3f}')

        # TODO use .sem() instead of .std()? (+ also change [/add] in section of
        # al_analysis.py that does the same).
        # https://stats.stackexchange.com/questions/32318
        stddev = trialmean.groupby('roi', axis='columns').std()
        std_ser = stddev.stack()
        assert std_ser.index.equals(mean_ser.index)
        print(f'mean stddev > threshold: {std_ser.mask(~gt_thresh).mean():.3f}')
        print(f'mean stddev <=threshold: {std_ser.mask(gt_thresh).mean():.3f}')

        mean_df_hemibrain = fill_to_hemibrain(mean_df)
        plot_responses(mean_df_hemibrain.T, response_stat_option_dir,
            'mean_hemibrain-filled', title=title, cbar_label=f'mean {dff_latex}',
            **supp_response_kws
        )

        # TODO just fill_to_hemibrain on trialmean instead of doing for mean_df + this?
        stddev_hemibrain = fill_to_hemibrain(stddev)
        plot_responses(stddev_hemibrain.T, response_stat_option_dir,
            'stddev_hemibrain-filled', title=title,
            cbar_label=f'stddev (across-flies) of trial-mean {dff_latex}',
            **supp_response_kws
        )

        if driver_indicator_str == 'GH146_6f':
            # TODO or rename, and still do w/ diff inputs? don't think i care
            plot_orn_vs_pn(mean_orn, mean_df, response_stat_option_dir,
                title_suffix=f'{title_suffix} (<- PN only, not ORN)'
            )

        dcorr = compute_corr(df)
        plot_corr(dcorr, response_stat_option_dir, 'corr', title=title)

        dser = corr_triangular(dcorr)
        assert dser.index.equals(vser.index)
        check_corr_ser(dser)

        dir_str = response_stat_option_dir.name
        if response_stat_option_dir.name == expected_matching_response_dirname:
            dir_str = f'{dir_str} (what Remy has been using)'

        if driver_indicator_str.startswith('GH146_'):
            celltype_str = 'PN'
        else:
            assert driver_indicator_str.startswith('pebbled_')
            celltype_str = 'ORN'

        dser.name = (celltype_str, dir_str)
        corr_ser_list.append(dser)

        if (celltype_str == 'PN' and
            response_stat_option_dir.name == 'my-typical-dff_best-plane-per-panel'):
            # TODO TODO TODO also do for original pn data given to remy (not recomputed
            # w/ best plane per recording)

            # NOTE: al_util.n_multichoose_k(n=5, k=5) = 126, so only 126 ways to draw 5
            # items with replacement from a population of size 5. not sure if this has
            # relevance on how many times it makes sense to do the resampling. when
            # combined with resampling both populations, probably a meaningfully larger
            # number

            # TODO TODO TODO replace `pn` input w/ single fly bouton data from remy,
            # in whatever format she gives it to me (either precomputed single-fly
            # distances or single-fly responses)
            print('REPLACE PN INPUT W/ REMY BOUTON FLY DATA')
            test_pearson_of_pearsons_are_equal(df, pn, 'PN dendrites',
                'TEST BOUTON DATA', response_stat_option_dir
            )

            # TODO TODO TODO compute correlation vs vcf-emb for each fly -> print

        print()


    # TODO TODO also append paper ORN outputs as one dir for this?
    # (include one subdir under pebbled_6f for that, and maybe also call out like i do
    # for dir w/ outputs matching paper PN outputs)
    # (.105 paper vs .106 dff_best-plane-per-panel, so pretty dang close)
    corrs_vs_vcf = compute_corr_vs_vcf_emb(corr_ser_list)

    # TODO update:
    # Pearson correlations vs embedded VCF:
    # n-volumes-for-response_2__response-stat-fn_amax__median-for-baseline_True   0.469
    # n-volumes-for-response_2__stat_max                                          0.468
    # n-volumes-for-response_2__stat_mean (what Remy has been using)              0.446
    # n-volumes-for-response_3__stat_max                                          0.480
    #
    # mainly as a sanity check that all dirs above have meaningfully different contents.
    # may need to revise this threshold if this assertion ever trips.
    #assert ((corrs_vs_vcf[0] - corrs_vs_vcf[1:]).abs() > 0.005).all()


if __name__ == '__main__':
    main()

